{
 "cells": [
  {
   "cell_type": "code",
   "id": "4c0763f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:23:40.073235Z",
     "start_time": "2025-05-29T06:23:40.050709Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "LAST_UPDATE_FILE = \"results/last_update.txt\"\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def validate_chat_history_format():\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            return\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "        wrapped = [{\"timestamp\": datetime.now().isoformat(), \"conversation\": data}]\n",
    "        with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(wrapped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    validate_chat_history_format()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b237f03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:23:40.264724Z",
     "start_time": "2025-05-29T06:23:40.086226Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "\n",
    "def save_history(conversation: list[dict]):\n",
    "    \"\"\"\n",
    "    conversation: a list of plain dicts, each with keys \"role\" and \"content\".\n",
    "    Appends it as a new entry under a timestamp, preserving any existing history.\n",
    "    \"\"\"\n",
    "    # 1) Load existing timeline\n",
    "    timeline: list = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f)\n",
    "                if not isinstance(timeline, list):\n",
    "                    timeline = []\n",
    "        except (json.JSONDecodeError, OSError) as e:\n",
    "            print(f\"⚠️  Could not read/parse {HISTORY_FILE}: {e!r}. Starting fresh.\")\n",
    "            timeline = []\n",
    "\n",
    "    # 2) Append this turn\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": conversation\n",
    "    }\n",
    "    timeline.append(entry)\n",
    "\n",
    "    # 3) Write atomically\n",
    "    tmp_file = HISTORY_FILE + \".tmp\"\n",
    "    try:\n",
    "        with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(timeline, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp_file, HISTORY_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed writing history to {HISTORY_FILE}: {e!r}\")\n",
    "        # Cleanup tmp if it remains\n",
    "        try:\n",
    "            if os.path.exists(tmp_file):\n",
    "                os.remove(tmp_file)\n",
    "        except OSError:\n",
    "            pass\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:23:54.813247Z",
     "start_time": "2025-05-29T06:23:40.836228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "5ab9b2cf56268dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:23:54.860512Z",
     "start_time": "2025-05-29T06:23:54.845808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "a942e88111c09cd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:24:02.147607Z",
     "start_time": "2025-05-29T06:23:54.888196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "#Mostly Basic Programming Problems"
   ],
   "id": "874e06872db67446",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:24:08.710544Z",
     "start_time": "2025-05-29T06:24:02.165601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "21879a8c139d5d58",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:24:08.741544Z",
     "start_time": "2025-05-29T06:24:08.726539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "596c5f2c6a86785b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:24:20.988840Z",
     "start_time": "2025-05-29T06:24:08.759538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Cell 2: fix_chat_history_format()\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    \"\"\"\n",
    "    Alias for validate_chat_history_format, intended to run\n",
    "    right before loading via datasets or similar.\n",
    "    \"\"\"\n",
    "    validate_chat_history_format()\n",
    "    print(\"Ran fix_chat_history_format()\")\n",
    "\n",
    "\n",
    "# Call this function before loading the dataset\n",
    "fix_chat_history_format()\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "f591cc7b42d6d292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e8dcb05aef44cddbb21c8ada5fbeccd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "425d23ce2d084848b1ab342fb492798d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7922\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:24:21.254507Z",
     "start_time": "2025-05-29T06:24:21.008360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "11bf1be1d3d533b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8187 chunks from 7922 documents.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:35.290864Z",
     "start_time": "2025-05-29T06:24:21.272443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "1fa4a1269b344159",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_2144\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:35.382630Z",
     "start_time": "2025-05-29T06:26:35.374618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "f5cc4747dd970db4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:35.489971Z",
     "start_time": "2025-05-29T06:26:35.432195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "#\n",
    "# custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are a calm and knowledgeable tutor. Your task is to assist students by carefully analyzing each problem and providing clear, thoughtful solutions. Use the provided context if it contains relevant information, and otherwise rely on your own reasoning.\n",
    "#\n",
    "# Always begin your answer with:\n",
    "# \"I am here to help with a solution for this problem:\"\n",
    "#\n",
    "# Instructions:\n",
    "# - Carefully read the entire question. Pay close attention to:\n",
    "#   - Numbers, units, or constraints mentioned\n",
    "#   - Special conditions or exceptions\n",
    "#   - What exactly is being asked\n",
    "# - First look in the Context.\n",
    "#   - If you find the answer there, use it and briefly mention the source.\n",
    "#   - If the Context does not contain the answer, compute or explain it yourself using logical steps.\n",
    "# - Clearly show your step-by-step reasoning.\n",
    "# - Conclude with a direct and final answer prefixed by: \"Answer:\"\n",
    "# - Do not make up facts outside your reasoning.\n",
    "# - Stay calm and explanatory, like a patient teacher.\n",
    "#\n",
    "# Context:\n",
    "# {context}\n",
    "#\n",
    "# Question:\n",
    "# {question}\n",
    "#\n",
    "# Answer:\n",
    "# \"\"\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a calm, patient, and highly knowledgeable tutor. Your job is to answer exactly one student question per invocation.\n",
    " *Under no circumstances*, do *not* show the context\n",
    " -Answer only *once* to the question\n",
    " - Do *not* reprint the context unless asked.\n",
    "- *No extra answers*\n",
    "- **Do not invent or hallucinate.** If you are not absolutely certain of a fact or it’s not in the context, respond:\n",
    " Questions may be:\n",
    "\n",
    "- **Math** (numeric, algebra, calculus)\n",
    "- **Informatics** (algorithms, data structures, programming)\n",
    "- **Pure Theory** (definitions, concepts)\n",
    "\n",
    "**0. Context Check**\n",
    "First, read the **Context** below.\n",
    "- If it contains the exact answer (a worked math solution, a definition, or code snippet), use that verbatim and note “(taken from context)”.\n",
    "- If not, proceed to the relevant step below.\n",
    "- Do *not* reprint the context unless asked.\n",
    "- *No extra answers*\n",
    "- **Do not invent or hallucinate.** If you are not absolutely certain of a fact or it’s not in the context, respond:\n",
    "\n",
    "\n",
    "**1. Math**\n",
    "- If the question is *only* a basic arithmetic expression (`+`, `-`, `*`, `/`, parentheses), compute it directly and return.\n",
    "- For all other math, pick the simplest method, show only necessary steps with clear reasoning.\n",
    "\n",
    "**2. Informatics**\n",
    "- Identify the concept or algorithm.\n",
    "- Explain concisely, using code examples only when they clarify.\n",
    "- Focus strictly on the question.\n",
    "\n",
    "**3. Pure Theory**\n",
    "- Provide a crisp authoritative definition.\n",
    "- If the context has it, use it verbatim with “(taken from context)”.\n",
    "- Otherwise, supply your own clear definition.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Respond in this format:\n",
    "\n",
    "---\n",
    "\n",
    "Problem Type: [Math or Informatics].\n",
    "\n",
    "[Your detailed explanation or step-by-step solution]\n",
    "\n",
    "Answer: <final result or conclusion>\n",
    "\n",
    "---\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ],
   "id": "f300ba52fd33a2f5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:39.238142Z",
     "start_time": "2025-05-29T06:26:35.523251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "# Chain to generate answers with a defined prompt\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=custom_prompt\n",
    ")\n",
    "\n",
    "\n",
    "# # ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "#     retriever=retriever,\n",
    "#     return_source_documents=True,\n",
    "# ) -> pre-fabricated\n",
    "\n",
    "# Plug this into a RetrievalQA system\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",  # or \"map_reduce\", \"refine\" if you prefer\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True,\n",
    ")#custom retrieval\n",
    "\n",
    "# # ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "# query = \"How would you implement binary search in Python?\"\n",
    "# result = qa_chain(query)\n",
    "# print(\"Answer:\\n\", result[\"result\"])\n",
    "# print(\"\\nSources:\")\n",
    "# for doc in result[\"source_documents\"]:\n",
    "#     print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "# Test the chain with the correct input format\n",
    "# test_query = {\"context\": \"\", \"query\": \"What is Python?\"}  # Ensure both keys are included\n",
    "# try:\n",
    "#     test_result = qa_chain(test_query)\n",
    "#     print(\"✅ QA chain test successful!\")\n",
    "#     print(f\"Result: {test_result['result'][:100]}...\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ QA chain test failed with error: {e}\")\n"
   ],
   "id": "8ec760ff28f20c6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_2144\\1712614391.py:30: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_2144\\1712614391.py:38: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:42.219215Z",
     "start_time": "2025-05-29T06:26:39.246150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "93c1c76dc68bc7b9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "17efb8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:42.251223Z",
     "start_time": "2025-05-29T06:26:42.237216Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "MODEL_ID = \"meta-llama-3.1-8b-instruct\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer lm-studio\"}\n",
    "\n",
    "def chat():\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful programming tutor.\"}]\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     save_history(messages)\n",
    "        #     print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        #     break\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        payload = {\"model\": MODEL_ID, \"messages\": messages, \"temperature\": 0.7}\n",
    "        response = requests.post(URL, headers=HEADERS, json=payload, timeout=1000)\n",
    "        answer = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        # print(answer)\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     serialiable = [m.dict() if isinstance(m, BaseModel)\n",
    "        #                    else m\n",
    "        #                    for m in messages]\n",
    "        #     save_history(serialiable)\n",
    "        print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "9c3739c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:56.700772Z",
     "start_time": "2025-05-29T06:26:42.269225Z"
    }
   },
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "fix_chat_history_format()\n",
    "raw_chat = load_dataset(\"json\", data_files={\"train\": HISTORY_FILE})\n",
    "\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        user = [m[\"content\"] for m in conv if m[\"role\"] == \"user\"]\n",
    "        asst = [m[\"content\"] for m in conv if m[\"role\"] == \"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(format_chat_batch, batched=True, remove_columns=[\"timestamp\",\"conversation\"])\n",
    "\n",
    "raw_intents = load_dataset(\"json\", data_files={\"train\": \"intents.json\"})\n",
    "def format_intents_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(format_intents_batch, batched=True, remove_columns=[\"intents\"])\n",
    "\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(format_gsm_batch, batched=True, remove_columns=gsm.column_names)\n",
    "\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4d0d1be11ea46718de337017b8d946b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7922\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "9ecf1b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:56.748776Z",
     "start_time": "2025-05-29T06:26:56.733847Z"
    }
   },
   "source": [
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def get_new_conversations() -> List[Dict]:\n",
    "    last_update = None\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            last_update = f.read().strip()\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return []\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        history = json.load(f)\n",
    "        if not isinstance(history, list):\n",
    "            history = [history]\n",
    "    if last_update:\n",
    "        return [c for c in history if c.get(\"timestamp\", \"\") > last_update]\n",
    "    return history\n",
    "\n",
    "def is_correction(conv: Dict) -> bool:\n",
    "    msgs = conv.get(\"conversation\", [])\n",
    "    for i in range(1, len(msgs)):\n",
    "        if msgs[i][\"role\"] == \"user\" and msgs[i-1][\"role\"] == \"assistant\":\n",
    "            txt = msgs[i][\"content\"].lower()\n",
    "            if any(kw in txt for kw in [\"wrong\",\"incorrect\",\"mistake\",\"no,\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def conversations_to_docs(convs: List[Dict]) -> List[Document]:\n",
    "    docs = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for idx, conv in enumerate(convs):\n",
    "        text = \"\"\n",
    "        for msg in conv[\"conversation\"]:\n",
    "            if msg[\"role\"] != \"system\":\n",
    "                prefix = \"Question: \" if msg[\"role\"]==\"user\" else \"Answer: \"\n",
    "                text += f\"{prefix}{msg['content']}\\n\\n\"\n",
    "        metadata = {\"source\": f\"conversation-{idx}\", \"timestamp\": conv.get(\"timestamp\",\"\")}\n",
    "        for chunk in splitter.split_text(text):\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def update_knowledge_base():\n",
    "    new_convs = get_new_conversations()\n",
    "    if not new_convs:\n",
    "        print(\"No new conversations found.\")\n",
    "        return\n",
    "    corrections = [c for c in new_convs if is_correction(c)]\n",
    "    print(f\"Found {len(new_convs)} convs, {len(corrections)} with corrections\")\n",
    "    docs = conversations_to_docs(new_convs)\n",
    "    if not docs:\n",
    "        print(\"No documents to add.\")\n",
    "        return\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    try:\n",
    "        vs = FAISS.load_local(INDEX_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "    except:\n",
    "        vs = FAISS.from_documents(docs, embedder)\n",
    "    vs.add_documents(docs)\n",
    "    vs.save_local(INDEX_PATH)\n",
    "    os.makedirs(os.path.dirname(LAST_UPDATE_FILE), exist_ok=True)\n",
    "    with open(LAST_UPDATE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "    print(\"Knowledge base updated\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a0854251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:56.763784Z",
     "start_time": "2025-05-29T06:26:56.756778Z"
    }
   },
   "source": [
    "# # ────────────────────────────────\n",
    "# # 1) FastAPI & Required Imports\n",
    "# # ────────────────────────────────\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Literal\n",
    "# import re\n",
    "# from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 2) Initialize Math Tool (Optional Shortcut for Arithmetic)\n",
    "# # ────────────────────────────────\n",
    "# # This tool allows your system to directly execute math expressions using Python\n",
    "# python_tool = PythonREPLTool()\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 3) FastAPI App Setup + CORS Config\n",
    "# # ────────────────────────────────\n",
    "# app = FastAPI()\n",
    "#\n",
    "# # This allows frontend (e.g., React at localhost:3000) to communicate with backend\n",
    "# app.add_middleware(\n",
    "#     CORSMiddleware,\n",
    "#     allow_origins=[\"http://localhost:3000\"],\n",
    "#     allow_credentials=True,\n",
    "#     allow_methods=[\"*\"],\n",
    "#     allow_headers=[\"*\"],\n",
    "# )\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 4) Data Models for Request and Response\n",
    "# # ────────────────────────────────\n",
    "#\n",
    "# # A single message in the conversation\n",
    "# class ChatMessage(BaseModel):\n",
    "#     role: Literal[\"user\", \"assistant\", \"system\"]  # Who sent the message\n",
    "#     content: str\n",
    "#\n",
    "# # The client sends a message + full history\n",
    "# class ChatRequest(BaseModel):\n",
    "#     message: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # The server returns the latest answer + updated history\n",
    "# class ChatResponse(BaseModel):\n",
    "#     answer: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 5) Utility: Detect simple arithmetic expressions like \"17 * 19\"\n",
    "# # ────────────────────────────────\n",
    "# ARITH_PATTERN = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')  # Accepts + - * / and parentheses\n",
    "# def is_arithmetic(q: str) -> bool:\n",
    "#     return bool(ARITH_PATTERN.fullmatch(q.strip()))\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 6) Main Endpoint: POST /rag_chat\n",
    "# # ────────────────────────────────\n",
    "# @app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "# async def rag_chat_endpoint(req: ChatRequest):\n",
    "#     # A) Start from user-submitted chat history, or empty\n",
    "#     messages = req.history or []\n",
    "#\n",
    "#     # B) Sanitize history:\n",
    "#     # If any past assistant messages contain incorrect arithmetic, remove them\n",
    "#     clean_hist: List[ChatMessage] = []\n",
    "#     for m in messages:\n",
    "#         if m.role == \"assistant\" and is_arithmetic(m.content):\n",
    "#             correct = python_tool.run(m.content)\n",
    "#             if m.content.strip() != correct.strip():\n",
    "#                 # Replace with a warning message\n",
    "#                 clean_hist.append(ChatMessage(\n",
    "#                     role=\"system\",\n",
    "#                     content=\"⚠️ Removed previous unsupported arithmetic answer\"\n",
    "#                 ))\n",
    "#                 continue\n",
    "#         clean_hist.append(m)\n",
    "#     messages = clean_hist  # Use the cleaned list going forward\n",
    "#\n",
    "#     # C) Append the new user message to the conversation\n",
    "#     if not any(m.role == \"user\" and m.content == req.message for m in messages):\n",
    "#         messages.append(ChatMessage(role=\"user\", content=req.message))\n",
    "#\n",
    "#     try:\n",
    "#         # D) Short-circuit: if the message is just math, compute it directly\n",
    "#         if is_arithmetic(req.message):\n",
    "#             answer = python_tool.run(req.message)\n",
    "#\n",
    "#             # Log answer into history and save\n",
    "#             messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#             save_history([m.dict() for m in messages])  # Your implementation\n",
    "#             return ChatResponse(answer=answer, history=messages)\n",
    "#\n",
    "#         # E) Fallback: normal LLM-powered RAG pipeline\n",
    "#         result = qa_chain({\"query\": req.message})  # This uses your RetrievalQA\n",
    "#         answer = result[\"result\"]\n",
    "#         docs = result[\"source_documents\"]  # These are the context documents\n",
    "#\n",
    "#         # F) Filter for toxicity (banned keywords)\n",
    "#         toxic = {\n",
    "#             \"kill\", \"hate\", \"stupid\", \"dumb\", \"racist\", \"sexist\",\n",
    "#             \"violence\", \"bomb\", \"terror\", \"die\", \"suicide\"\n",
    "#         }\n",
    "#         if any(word in answer.lower() for word in toxic):\n",
    "#             answer = \"⚠️ Response blocked due to potentially inappropriate content.\"\n",
    "#\n",
    "#         else:\n",
    "#             # G) Hallucination filter:\n",
    "#             # Check if answer actually shares tokens with source context\n",
    "#             context_text = \" \".join(doc.page_content.lower() for doc in docs)\n",
    "#             shared = [word for word in answer.lower().split() if word in context_text]\n",
    "#             if len(shared) < 5:  # Tune this threshold to your needs\n",
    "#                 answer = \"⚠️ I'm not confident this answer is grounded in the provided context.\"\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         # If anything breaks in the chain (e.g. LLM crashed), report it cleanly\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "#\n",
    "#     # H) Append assistant's response, persist, and return everything\n",
    "#     messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#     save_history([m.dict() for m in messages])  # Your implementation\n",
    "#     return ChatResponse(answer=answer, history=messages)\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:26:56.795305Z",
     "start_time": "2025-05-29T06:26:56.780302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Notebook cell: run the RAG FastAPI server in the background\n",
    "# import threading\n",
    "# import uvicorn\n",
    "#\n",
    "# def run_server():\n",
    "#     uvicorn.run(\n",
    "#         app,\n",
    "#         host=\"0.0.0.0\",\n",
    "#         port=8000,\n",
    "#         log_level=\"info\",\n",
    "#         reload=False,        # disable auto-reload in notebooks\n",
    "#     )\n",
    "#\n",
    "# # Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "# server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "# server_thread.start()\n",
    "#\n",
    "# print(\"🚀 RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "69cf251d74bd716e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:27:00.949682Z",
     "start_time": "2025-05-29T06:26:56.828316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 1: RAG‐only FastAPI app\n",
    "\n",
    "import os, json, re\n",
    "from datetime import datetime\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "# ── Chat‐history persistence (messages only) ──\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "def save_history(conv: List[Dict[str,str]]):\n",
    "    timeline = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f) or []\n",
    "        except:\n",
    "            timeline = []\n",
    "    timeline.append({\"timestamp\": datetime.now().isoformat(),\"conversation\":conv})\n",
    "    tmp = HISTORY_FILE + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(timeline, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, HISTORY_FILE)\n",
    "\n",
    "# ── Metrics & intent data loaders ──\n",
    "bleu_metric  = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "with open(\"intents.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    intents_data = json.load(f)[\"intents\"]\n",
    "\n",
    "python_tool = PythonREPLTool()\n",
    "ALL_DATASETS = [\"gsm8k\",\"mbpp\",\"intents\"]\n",
    "ARITH = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')\n",
    "def is_arithmetic(q: str) -> bool:\n",
    "    return bool(ARITH.fullmatch(q.strip()))\n",
    "\n",
    "def find_ground_truth(source: str, q: str) -> Optional[str]:\n",
    "    ql = q.lower()\n",
    "    try:\n",
    "        if source==\"gsm8k\":\n",
    "            ds = load_dataset(\"gsm8k\",\"main\",split=\"train\")\n",
    "            for it in ds:\n",
    "                if ql in it[\"question\"].lower():\n",
    "                    return it[\"answer\"]\n",
    "        elif source==\"mbpp\":\n",
    "            ds = load_dataset(\"google-research-datasets/mbpp\",\"sanitized\",split=\"train\")\n",
    "            for it in ds:\n",
    "                text = it.get(\"text\", \"\") or it.get(\"prompt\", \"\")\n",
    "                if ql in text.lower():\n",
    "                    return it.get(\"code\")\n",
    "        elif source==\"intents\":\n",
    "            for intent in intents_data:\n",
    "                for pat in intent.get(\"patterns\", []):\n",
    "                    if ql in pat.lower():\n",
    "                        responses = intent.get(\"responses\", [])\n",
    "                        return responses[0] if responses else None\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ GT lookup error:\", e)\n",
    "    return None\n",
    "\n",
    "# ── FastAPI wiring ──\n",
    "app = FastAPI()\n",
    "app.add_middleware(CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "    ground_truth_source: Optional[Literal[\"gsm8k\",\"mbpp\",\"intents\",\"all\"]] = None\n",
    "\n",
    "class Metric(BaseModel):\n",
    "    dataset: str\n",
    "    bleu: Optional[float]\n",
    "    rouge: Optional[Dict[str,float]]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "    metrics: Optional[List[Metric]] = None\n",
    "\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_endpoint(req: ChatRequest):\n",
    "    # 1) sanitize history\n",
    "    msgs: List[ChatMessage] = []\n",
    "    for m in req.history:\n",
    "        if m.role==\"assistant\" and is_arithmetic(m.content):\n",
    "            corr = python_tool.run(m.content)\n",
    "            if m.content.strip() != corr.strip():\n",
    "                msgs.append(ChatMessage(role=\"system\",content=\"⚠️ Removed bad arithmetic\"))\n",
    "                continue\n",
    "        msgs.append(m)\n",
    "\n",
    "    # 2) append new user turn\n",
    "    if not any(m.role==\"user\" and m.content==req.message for m in msgs):\n",
    "        msgs.append(ChatMessage(role=\"user\",content=req.message))\n",
    "\n",
    "    # 3) math‐only\n",
    "    if is_arithmetic(req.message):\n",
    "        ans = python_tool.run(req.message)\n",
    "        msgs.append(ChatMessage(role=\"assistant\",content=ans))\n",
    "        save_history([m.dict() for m in msgs])\n",
    "        return ChatResponse(answer=ans, history=msgs)\n",
    "\n",
    "    result = qa_chain({\"query\": req.message})\n",
    "    ans    = result[\"result\"]\n",
    "    docs   = result[\"source_documents\"]\n",
    "\n",
    "    # F) Toxicity filter\n",
    "    toxic = {\n",
    "        \"kill\", \"hate\", \"stupid\", \"dumb\", \"racist\", \"sexist\",\n",
    "        \"violence\", \"bomb\", \"terror\", \"die\", \"suicide\"\n",
    "    }\n",
    "    if any(word in ans.lower() for word in toxic):\n",
    "        # if any banned keyword appears, block the response\n",
    "        ans = \"⚠️ Response blocked due to potentially inappropriate content.\"\n",
    "    else:\n",
    "        # G) Hallucination filter\n",
    "        # Build a lowercase blob of all retrieved context\n",
    "        context_text = \" \".join(doc.page_content.lower() for doc in docs)\n",
    "        # Count how many answer tokens actually appear in the context\n",
    "        shared = [tok for tok in ans.lower().split() if tok in context_text]\n",
    "        # If fewer than 5 tokens matched, assume ungrounded/hallucinated\n",
    "        if len(shared) < 5:\n",
    "            ans = \"⚠️ I'm not confident this answer is grounded in the provided context.\"\n",
    "\n",
    "\n",
    "\n",
    "    # 5) metrics\n",
    "    mets = []\n",
    "    if req.ground_truth_source:\n",
    "        to_do = ALL_DATASETS if req.ground_truth_source==\"all\" else [req.ground_truth_source]  # type: ignore\n",
    "        for ds in to_do:\n",
    "            gt = find_ground_truth(ds, req.message)\n",
    "            b,r = None,None\n",
    "            if gt:\n",
    "                b = bleu_metric.compute(predictions=[ans],references=[[gt]])[\"bleu\"]\n",
    "                raw = rouge_metric.compute(predictions=[ans],references=[gt])\n",
    "                r = {k:(v if isinstance(v,(float,int)) else v[\"recall\"]) for k,v in raw.items()}\n",
    "            mets.append({\"dataset\":ds,\"bleu\":b,\"rouge\":r})\n",
    "    msgs.append(ChatMessage(role=\"assistant\",content=ans))\n",
    "    save_history([m.dict() for m in msgs])\n",
    "    return ChatResponse(answer=ans, history=msgs, metrics=mets)\n"
   ],
   "id": "a495d7ab8bc83de",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:27:00.964770Z",
     "start_time": "2025-05-29T06:27:00.957681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ────────────────────────────────\n",
    "# 7) TEST CALL (in-notebook) to see prints under this cell\n",
    "# ────────────────────────────────\n",
    "# client = TestClient(app)\n",
    "# response = client.post(\n",
    "#     \"/rag_chat\",\n",
    "#     json={\n",
    "#         \"message\": \"What is computer architecture?\",\n",
    "#         \"history\": []\n",
    "#     }\n",
    "# )\n",
    "# print(\"Response JSON:\", response.json())\n"
   ],
   "id": "dfa0f3c5e2276a25",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:27:01.011220Z",
     "start_time": "2025-05-29T06:27:00.997222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # ────────────────────────────────\n",
    "# # Cell: LLM-Judge with real-time feedback\n",
    "# # ────────────────────────────────\n",
    "# import os\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from fastapi.testclient import TestClient\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Literal, Dict, Any, Optional\n",
    "# from datasets import load_dataset\n",
    "# from evaluate import load as load_metric\n",
    "# from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "# from langchain import OpenAI, LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from dotenv import load_dotenv\n",
    "#\n",
    "# # 1) Ensure API key is set (or replace with your key string)\n",
    "# load_dotenv(dotenv_path=\"llm.env\")\n",
    "#\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not api_key:\n",
    "#     raise ValueError(\n",
    "#         \"Missing OPENAI_API_KEY environment variable. \"\n",
    "#         \"Please create a .env file with your key.\"\n",
    "#     )\n",
    "# # 2) Build the judge prompt & chain\n",
    "# judge_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are an expert evaluator. Given a Question and an Answer, produce:\n",
    "# Verdict: Correct or Incorrect\n",
    "# Score: a number between 0.0 and 1.0\n",
    "# Comments: concise feedback.\n",
    "#\n",
    "# Question:\n",
    "# {question}\n",
    "#\n",
    "# Answer:\n",
    "# {answer}\n",
    "# \"\"\")\n",
    "# judge_llm = OpenAI(temperature=0, openai_api_key=api_key)\n",
    "# judge_chain = LLMChain(llm=judge_llm, prompt=judge_prompt)\n",
    "#\n",
    "# # 3) (Re)define your FastAPI app and models as before\n",
    "# app = FastAPI()\n",
    "# app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "#\n",
    "# class ChatMessage(BaseModel):\n",
    "#     role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "#     content: str\n",
    "#\n",
    "# class ChatRequest(BaseModel):\n",
    "#     message: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# class ChatResponse(BaseModel):\n",
    "#     answer: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # (Include your is_arithmetic, find_ground_truth, metrics logic here…)\n",
    "#\n",
    "# @app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "# async def rag_chat_with_judge(req: ChatRequest):\n",
    "#     # Call your original rag_chat logic (inline or imported)\n",
    "#     # For brevity, assume rag_chat returns ChatResponse\n",
    "#\n",
    "#     resp: ChatResponse = await rag_chat_endpoint(req)\n",
    "#\n",
    "#     # 4) Run the judge immediately and print it\n",
    "#     evaluation = judge_chain.run(question=req.message, answer=resp.answer)\n",
    "#     print(\"\\n🔍 LLM Judge Evaluation:\\n\" + evaluation)\n",
    "#\n",
    "#     return resp\n",
    "#\n",
    "# # 5) Test in-notebook to see prints under this cell\n",
    "# client = TestClient(app)\n",
    "# response = client.post(\"/rag_chat\", json={\"message\":\"2+4\",\"history\":[]})\n",
    "# print(\"Response JSON:\", response.json())\n"
   ],
   "id": "917270ec2f08ad8c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:27:01.107859Z",
     "start_time": "2025-05-29T06:27:01.029221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2: LLM‐Judge FastAPI app\n",
    "\n",
    "import os, logging\n",
    "from typing import Literal, List\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from openai import RateLimitError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# logging to file\n",
    "logging.basicConfig(\n",
    "  level=logging.INFO,\n",
    "  handlers=[logging.StreamHandler(),logging.FileHandler(\"judge.log\")],\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "load_dotenv(dotenv_path=\"llm.env\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # With this:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "judge_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert evaluator. Your job is to assess both the **final answer** and the **quality of the reasoning** provided.\n",
    "Do **not** generate any new solutions—only judge what’s already here.\n",
    "\n",
    "Rules:\n",
    "- If the final result is numerically correct but the reasoning is flawed or missing key steps, the **Verdict** can still be “Correct” but the **Score** should be < 1.0 to reflect the poor reasoning.\n",
    "- If the result is wrong, the **Verdict** is “Incorrect” and the **Score** should be low (close to 0).\n",
    "- Give concise feedback that calls out errors or omissions in the reasoning.\n",
    "\n",
    "Output exactly three lines in this format:\n",
    "\n",
    "Verdict: <Correct or Incorrect>\n",
    "Score: <decimal between 0.0 and 1.0>\n",
    "Comments: <brief notes on both answer correctness and reasoning quality>\n",
    "\n",
    "---\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer to evaluate:\n",
    "{answer}\n",
    "\n",
    "Evaluation:\n",
    "Verdict:\n",
    "Score:\n",
    "Comments:\n",
    "\n",
    "\"\"\")\n",
    "judge_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    openai_api_key=api_key,\n",
    ")\n",
    "\n",
    "judge_chain = LLMChain(\n",
    "    llm=judge_llm,\n",
    "    prompt=judge_prompt\n",
    ")\n",
    "\n",
    "def run_judge(q: str, a: str) -> str:\n",
    "    try:\n",
    "        v = judge_chain.run(question=q,answer=a)\n",
    "        logger.info(\"Verdict:\\n%s\",v)\n",
    "        return v\n",
    "    except RateLimitError:\n",
    "        msg=\"Judge skipped – rate limit\"\n",
    "        logger.warning(msg)\n",
    "        return msg\n",
    "    except Exception as e:\n",
    "        logger.error(\"Judge error: %s\",e)\n",
    "        return f\"Judge error: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "class EvaluateRequest(BaseModel):\n",
    "    question: str\n",
    "    answer:   str\n",
    "\n",
    "class EvaluateResponse(BaseModel):\n",
    "    evaluation: str\n",
    "\n",
    "@app.post(\"/evaluate\",response_model=EvaluateResponse)\n",
    "async def evaluate(req: EvaluateRequest):\n",
    "    verdict = run_judge(req.question,req.answer)\n",
    "    return EvaluateResponse(evaluation=verdict)\n"
   ],
   "id": "e18409e709dae3d7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:27:01.184940Z",
     "start_time": "2025-05-29T06:27:01.139786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Notebook cell: run the RAG FastAPI server in the background\n",
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        reload=False,        # disable auto-reload in notebooks\n",
    "    )\n",
    "\n",
    "# Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"🚀 RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "99b012da35df65fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 RAG server is now running at http://localhost:8000\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T06:27:34.064075Z",
     "start_time": "2025-05-29T06:27:01.218731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3: TestClient for both endpoints\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "client_chat  = TestClient(app)\n",
    "client_eval  = TestClient(app)\n",
    "\n",
    "# 1) Chat + metrics\n",
    "resp = client_chat.post(\"/rag_chat\", json={\n",
    "  \"message\":\"What is 2^10?\",\n",
    "  \"history\":[],\n",
    "  \"ground_truth_source\":\"all\"\n",
    "})\n",
    "data=resp.json()\n",
    "print(\"Chat Response:\",data)\n",
    "\n",
    "# 2) Judge\n",
    "ver = client_eval.post(\"/evaluate\",json={\n",
    "  \"question\":\"What is 2^10?\",\n",
    "  \"answer\":data[\"answer\"],\n",
    "}).json()[\"evaluation\"]\n",
    "print(\"Judge Verdict:\\n\",ver)\n"
   ],
   "id": "59ec58c2b5c00c09",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_2144\\1454632787.py:117: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": req.message})\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_2144\\1454632787.py:154: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  save_history([m.dict() for m in msgs])\n",
      "INFO:httpx:HTTP Request: POST http://testserver/rag_chat \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_2144\\2231540046.py:68: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  v = judge_chain.run(question=q,answer=a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Response: {'answer': '---\\n\\nProblem Type: Math.\\n\\nThe result of 2^10 can be directly calculated using the power operator:\\n\\n2^9 * 2 = 1024\\nor \\n2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 = 1024\\n\\nAnswer: 1024.', 'history': [{'role': 'user', 'content': 'What is 2^10?'}, {'role': 'assistant', 'content': '---\\n\\nProblem Type: Math.\\n\\nThe result of 2^10 can be directly calculated using the power operator:\\n\\n2^9 * 2 = 1024\\nor \\n2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 = 1024\\n\\nAnswer: 1024.'}], 'metrics': [{'dataset': 'gsm8k', 'bleu': None, 'rouge': None}, {'dataset': 'mbpp', 'bleu': None, 'rouge': None}, {'dataset': 'intents', 'bleu': None, 'rouge': None}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Verdict:\n",
      "Verdict: Correct  \n",
      "Score: 0.9  \n",
      "Comments: The final answer of 1024 is correct, but the reasoning could be clearer. The explanation could benefit from explicitly stating that 2^10 equals 1024 directly, rather than breaking it down into 2^9 * 2, which may confuse some readers.\n",
      "INFO:httpx:HTTP Request: POST http://testserver/evaluate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge Verdict:\n",
      " Verdict: Correct  \n",
      "Score: 0.9  \n",
      "Comments: The final answer of 1024 is correct, but the reasoning could be clearer. The explanation could benefit from explicitly stating that 2^10 equals 1024 directly, rather than breaking it down into 2^9 * 2, which may confuse some readers.\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
