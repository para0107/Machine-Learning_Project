{
 "cells": [
  {
   "cell_type": "code",
   "id": "4c0763f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:49:13.367886Z",
     "start_time": "2025-05-27T20:49:13.344628Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "LAST_UPDATE_FILE = \"results/last_update.txt\"\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def validate_chat_history_format():\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            return\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "        wrapped = [{\"timestamp\": datetime.now().isoformat(), \"conversation\": data}]\n",
    "        with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(wrapped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    validate_chat_history_format()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b237f03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:49:13.616699Z",
     "start_time": "2025-05-27T20:49:13.381890Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "\n",
    "def save_history(conversation: list[dict]):\n",
    "    \"\"\"\n",
    "    conversation: a list of plain dicts, each with keys \"role\" and \"content\".\n",
    "    Appends it as a new entry under a timestamp, preserving any existing history.\n",
    "    \"\"\"\n",
    "    # 1) Load existing timeline\n",
    "    timeline: list = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f)\n",
    "                if not isinstance(timeline, list):\n",
    "                    timeline = []\n",
    "        except (json.JSONDecodeError, OSError) as e:\n",
    "            print(f\"âš ï¸  Could not read/parse {HISTORY_FILE}: {e!r}. Starting fresh.\")\n",
    "            timeline = []\n",
    "\n",
    "    # 2) Append this turn\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": conversation\n",
    "    }\n",
    "    timeline.append(entry)\n",
    "\n",
    "    # 3) Write atomically\n",
    "    tmp_file = HISTORY_FILE + \".tmp\"\n",
    "    try:\n",
    "        with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(timeline, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp_file, HISTORY_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed writing history to {HISTORY_FILE}: {e!r}\")\n",
    "        # Cleanup tmp if it remains\n",
    "        try:\n",
    "            if os.path.exists(tmp_file):\n",
    "                os.remove(tmp_file)\n",
    "        except OSError:\n",
    "            pass\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:49:27.380420Z",
     "start_time": "2025-05-27T20:49:14.466093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "5ab9b2cf56268dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:49:27.428418Z",
     "start_time": "2025-05-27T20:49:27.401420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "a942e88111c09cd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:49:36.295576Z",
     "start_time": "2025-05-27T20:49:27.458418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "874e06872db67446",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:50:00.240141Z",
     "start_time": "2025-05-27T20:49:36.311972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "21879a8c139d5d58",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:50:00.272134Z",
     "start_time": "2025-05-27T20:50:00.258135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â”€â”€ Paths & names â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# â”€â”€ Hugging Face models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# â”€â”€ Datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# â”€â”€ RAG / Retrieval params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# â”€â”€ LoRA fine-tuning (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# â”€â”€ Trainer hyperparameters (for fine-tuning generator) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "596c5f2c6a86785b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:50:10.381277Z",
     "start_time": "2025-05-27T20:50:00.290152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Cell 2: fix_chat_history_format()\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    \"\"\"\n",
    "    Alias for validate_chat_history_format, intended to run\n",
    "    right before loading via datasets or similar.\n",
    "    \"\"\"\n",
    "    validate_chat_history_format()\n",
    "    print(\"Ran fix_chat_history_format()\")\n",
    "\n",
    "\n",
    "# Call this function before loading the dataset\n",
    "fix_chat_history_format()\n",
    "\n",
    "# 1) Chatâ€history (no builtâ€in validation split here, only â€œtrainâ€):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP â€œsanitizedâ€ (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K â€œmainâ€ train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "f591cc7b42d6d292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bee03b93d9f94cbf935ea31eb4c21510"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6d1ed3726a54a7c890e2816741df047"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7874\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:50:10.661162Z",
     "start_time": "2025-05-27T20:50:10.414277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"â–¶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "11bf1be1d3d533b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Created 8129 chunks from 7874 documents.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:32.157692Z",
     "start_time": "2025-05-27T20:50:10.676640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"âœ” FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "1fa4a1269b344159",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_3012\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:32.237692Z",
     "start_time": "2025-05-27T20:52:32.223695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "f5cc4747dd970db4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:32.314745Z",
     "start_time": "2025-05-27T20:52:32.270997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "#\n",
    "# custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are a calm and knowledgeable tutor. Your task is to assist students by carefully analyzing each problem and providing clear, thoughtful solutions. Use the provided context if it contains relevant information, and otherwise rely on your own reasoning.\n",
    "#\n",
    "# Always begin your answer with:\n",
    "# \"I am here to help with a solution for this problem:\"\n",
    "#\n",
    "# Instructions:\n",
    "# - Carefully read the entire question. Pay close attention to:\n",
    "#   - Numbers, units, or constraints mentioned\n",
    "#   - Special conditions or exceptions\n",
    "#   - What exactly is being asked\n",
    "# - First look in the Context.\n",
    "#   - If you find the answer there, use it and briefly mention the source.\n",
    "#   - If the Context does not contain the answer, compute or explain it yourself using logical steps.\n",
    "# - Clearly show your step-by-step reasoning.\n",
    "# - Conclude with a direct and final answer prefixed by: \"Answer:\"\n",
    "# - Do not make up facts outside your reasoning.\n",
    "# - Stay calm and explanatory, like a patient teacher.\n",
    "#\n",
    "# Context:\n",
    "# {context}\n",
    "#\n",
    "# Question:\n",
    "# {question}\n",
    "#\n",
    "# Answer:\n",
    "# \"\"\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a calm and highly knowledgeable tutor. Your job is to solve either a **math** problem or an **informatics** (computer science) problem based on the student's question.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Identify whether the question is **Math**(numeric/aglebra/caluclus) or **Informatics**(algorithms, data structures, etc).\n",
    "2. If **Math**:\n",
    "    -Solve *only* math\n",
    "    -Do **not** mention Informatics at all\n",
    "   - Pick the simplest, most efficient method.\n",
    "   - **For exponentiation**, you may directly apply the power operator (e.g. 2^10 = 1024) or show *one* clear step using it; do **not** write out every repeated multiplication.\n",
    "   - Show **only** the steps that are strictly necessary for understanding.\n",
    "   - Clearly explain your reasoning at each step.\n",
    "   - End with a single, concise final line:\n",
    "     `Answer: <final result>`\n",
    "3. If **Informatics**:\n",
    "   - Explain definitions or concepts concisely.\n",
    "   - Use code examples only if they add clarity.\n",
    "   - Keep the explanation focused.\n",
    "4. **Do not** repeat or re-run any previous questions or examples.\n",
    "5. Do **not** show the context block unless it is explicitly asked for.\n",
    "\n",
    "\n",
    "Respond in this format:\n",
    "\n",
    "---\n",
    "\n",
    "Problem Type: [Math or Informatics].\n",
    "\n",
    "[Your detailed explanation or step-by-step solution]\n",
    "\n",
    "Answer: <final result or conclusion>\n",
    "\n",
    "---\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ],
   "id": "f300ba52fd33a2f5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:35.087192Z",
     "start_time": "2025-05-27T20:52:32.321741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# â”€â”€ 0) Grab your token from env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# â”€â”€ 1) Reload FAISS index â”€â”€\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# â”€â”€ 2) Connect to local LM Studio API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "# Chain to generate answers with a defined prompt\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=custom_prompt\n",
    ")\n",
    "\n",
    "\n",
    "# # â”€â”€ 3) Build & run RetrievalQA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "#     retriever=retriever,\n",
    "#     return_source_documents=True,\n",
    "# ) -> pre-fabricated\n",
    "\n",
    "# Plug this into a RetrievalQA system\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",  # or \"map_reduce\", \"refine\" if you prefer\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True,\n",
    ")#custom retrieval\n",
    "\n",
    "# # â”€â”€ 4) Test query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# query = \"How would you implement binary search in Python?\"\n",
    "# result = qa_chain(query)\n",
    "# print(\"Answer:\\n\", result[\"result\"])\n",
    "# print(\"\\nSources:\")\n",
    "# for doc in result[\"source_documents\"]:\n",
    "#     print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "# Test the chain with the correct input format\n",
    "# test_query = {\"context\": \"\", \"query\": \"What is Python?\"}  # Ensure both keys are included\n",
    "# try:\n",
    "#     test_result = qa_chain(test_query)\n",
    "#     print(\"âœ… QA chain test successful!\")\n",
    "#     print(f\"Result: {test_result['result'][:100]}...\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ QA chain test failed with error: {e}\")\n"
   ],
   "id": "8ec760ff28f20c6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_3012\\1712614391.py:30: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_3012\\1712614391.py:38: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:37.483696Z",
     "start_time": "2025-05-27T20:52:35.105117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â”€â”€ Cell: Load FAISS index & build retriever â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "93c1c76dc68bc7b9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "17efb8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:37.515617Z",
     "start_time": "2025-05-27T20:52:37.501625Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "MODEL_ID = \"meta-llama-3.1-8b-instruct\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer lm-studio\"}\n",
    "\n",
    "def chat():\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful programming tutor.\"}]\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     save_history(messages)\n",
    "        #     print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        #     break\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        payload = {\"model\": MODEL_ID, \"messages\": messages, \"temperature\": 0.7}\n",
    "        response = requests.post(URL, headers=HEADERS, json=payload, timeout=1000)\n",
    "        answer = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        # print(answer)\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     serialiable = [m.dict() if isinstance(m, BaseModel)\n",
    "        #                    else m\n",
    "        #                    for m in messages]\n",
    "        #     save_history(serialiable)\n",
    "        print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "9c3739c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:48.185376Z",
     "start_time": "2025-05-27T20:52:37.533636Z"
    }
   },
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "fix_chat_history_format()\n",
    "raw_chat = load_dataset(\"json\", data_files={\"train\": HISTORY_FILE})\n",
    "\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        user = [m[\"content\"] for m in conv if m[\"role\"] == \"user\"]\n",
    "        asst = [m[\"content\"] for m in conv if m[\"role\"] == \"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(format_chat_batch, batched=True, remove_columns=[\"timestamp\",\"conversation\"])\n",
    "\n",
    "raw_intents = load_dataset(\"json\", data_files={\"train\": \"intents.json\"})\n",
    "def format_intents_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(format_intents_batch, batched=True, remove_columns=[\"intents\"])\n",
    "\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(format_gsm_batch, batched=True, remove_columns=gsm.column_names)\n",
    "\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0531c0c006dc4d2091e1dd44c9254c5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7874\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "9ecf1b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:48.435738Z",
     "start_time": "2025-05-27T20:52:48.218374Z"
    }
   },
   "source": [
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def get_new_conversations() -> List[Dict]:\n",
    "    last_update = None\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            last_update = f.read().strip()\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return []\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        history = json.load(f)\n",
    "        if not isinstance(history, list):\n",
    "            history = [history]\n",
    "    if last_update:\n",
    "        return [c for c in history if c.get(\"timestamp\", \"\") > last_update]\n",
    "    return history\n",
    "\n",
    "def is_correction(conv: Dict) -> bool:\n",
    "    msgs = conv.get(\"conversation\", [])\n",
    "    for i in range(1, len(msgs)):\n",
    "        if msgs[i][\"role\"] == \"user\" and msgs[i-1][\"role\"] == \"assistant\":\n",
    "            txt = msgs[i][\"content\"].lower()\n",
    "            if any(kw in txt for kw in [\"wrong\",\"incorrect\",\"mistake\",\"no,\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def conversations_to_docs(convs: List[Dict]) -> List[Document]:\n",
    "    docs = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for idx, conv in enumerate(convs):\n",
    "        text = \"\"\n",
    "        for msg in conv[\"conversation\"]:\n",
    "            if msg[\"role\"] != \"system\":\n",
    "                prefix = \"Question: \" if msg[\"role\"]==\"user\" else \"Answer: \"\n",
    "                text += f\"{prefix}{msg['content']}\\n\\n\"\n",
    "        metadata = {\"source\": f\"conversation-{idx}\", \"timestamp\": conv.get(\"timestamp\",\"\")}\n",
    "        for chunk in splitter.split_text(text):\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def update_knowledge_base():\n",
    "    new_convs = get_new_conversations()\n",
    "    if not new_convs:\n",
    "        print(\"No new conversations found.\")\n",
    "        return\n",
    "    corrections = [c for c in new_convs if is_correction(c)]\n",
    "    print(f\"Found {len(new_convs)} convs, {len(corrections)} with corrections\")\n",
    "    docs = conversations_to_docs(new_convs)\n",
    "    if not docs:\n",
    "        print(\"No documents to add.\")\n",
    "        return\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    try:\n",
    "        vs = FAISS.load_local(INDEX_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "    except:\n",
    "        vs = FAISS.from_documents(docs, embedder)\n",
    "    vs.add_documents(docs)\n",
    "    vs.save_local(INDEX_PATH)\n",
    "    os.makedirs(os.path.dirname(LAST_UPDATE_FILE), exist_ok=True)\n",
    "    with open(LAST_UPDATE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "    print(\"Knowledge base updated\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a0854251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:48.466743Z",
     "start_time": "2025-05-27T20:52:48.453740Z"
    }
   },
   "source": [
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 1) FastAPI & Required Imports\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Literal\n",
    "# import re\n",
    "# from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "#\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 2) Initialize Math Tool (Optional Shortcut for Arithmetic)\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # This tool allows your system to directly execute math expressions using Python\n",
    "# python_tool = PythonREPLTool()\n",
    "#\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 3) FastAPI App Setup + CORS Config\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# app = FastAPI()\n",
    "#\n",
    "# # This allows frontend (e.g., React at localhost:3000) to communicate with backend\n",
    "# app.add_middleware(\n",
    "#     CORSMiddleware,\n",
    "#     allow_origins=[\"http://localhost:3000\"],\n",
    "#     allow_credentials=True,\n",
    "#     allow_methods=[\"*\"],\n",
    "#     allow_headers=[\"*\"],\n",
    "# )\n",
    "#\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 4) Data Models for Request and Response\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# # A single message in the conversation\n",
    "# class ChatMessage(BaseModel):\n",
    "#     role: Literal[\"user\", \"assistant\", \"system\"]  # Who sent the message\n",
    "#     content: str\n",
    "#\n",
    "# # The client sends a message + full history\n",
    "# class ChatRequest(BaseModel):\n",
    "#     message: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # The server returns the latest answer + updated history\n",
    "# class ChatResponse(BaseModel):\n",
    "#     answer: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 5) Utility: Detect simple arithmetic expressions like \"17 * 19\"\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ARITH_PATTERN = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')  # Accepts + - * / and parentheses\n",
    "# def is_arithmetic(q: str) -> bool:\n",
    "#     return bool(ARITH_PATTERN.fullmatch(q.strip()))\n",
    "#\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # 6) Main Endpoint: POST /rag_chat\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# @app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "# async def rag_chat_endpoint(req: ChatRequest):\n",
    "#     # A) Start from user-submitted chat history, or empty\n",
    "#     messages = req.history or []\n",
    "#\n",
    "#     # B) Sanitize history:\n",
    "#     # If any past assistant messages contain incorrect arithmetic, remove them\n",
    "#     clean_hist: List[ChatMessage] = []\n",
    "#     for m in messages:\n",
    "#         if m.role == \"assistant\" and is_arithmetic(m.content):\n",
    "#             correct = python_tool.run(m.content)\n",
    "#             if m.content.strip() != correct.strip():\n",
    "#                 # Replace with a warning message\n",
    "#                 clean_hist.append(ChatMessage(\n",
    "#                     role=\"system\",\n",
    "#                     content=\"âš ï¸ Removed previous unsupported arithmetic answer\"\n",
    "#                 ))\n",
    "#                 continue\n",
    "#         clean_hist.append(m)\n",
    "#     messages = clean_hist  # Use the cleaned list going forward\n",
    "#\n",
    "#     # C) Append the new user message to the conversation\n",
    "#     if not any(m.role == \"user\" and m.content == req.message for m in messages):\n",
    "#         messages.append(ChatMessage(role=\"user\", content=req.message))\n",
    "#\n",
    "#     try:\n",
    "#         # D) Short-circuit: if the message is just math, compute it directly\n",
    "#         if is_arithmetic(req.message):\n",
    "#             answer = python_tool.run(req.message)\n",
    "#\n",
    "#             # Log answer into history and save\n",
    "#             messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#             save_history([m.dict() for m in messages])  # Your implementation\n",
    "#             return ChatResponse(answer=answer, history=messages)\n",
    "#\n",
    "#         # E) Fallback: normal LLM-powered RAG pipeline\n",
    "#         result = qa_chain({\"query\": req.message})  # This uses your RetrievalQA\n",
    "#         answer = result[\"result\"]\n",
    "#         docs = result[\"source_documents\"]  # These are the context documents\n",
    "#\n",
    "#         # F) Filter for toxicity (banned keywords)\n",
    "#         toxic = {\n",
    "#             \"kill\", \"hate\", \"stupid\", \"dumb\", \"racist\", \"sexist\",\n",
    "#             \"violence\", \"bomb\", \"terror\", \"die\", \"suicide\"\n",
    "#         }\n",
    "#         if any(word in answer.lower() for word in toxic):\n",
    "#             answer = \"âš ï¸ Response blocked due to potentially inappropriate content.\"\n",
    "#\n",
    "#         else:\n",
    "#             # G) Hallucination filter:\n",
    "#             # Check if answer actually shares tokens with source context\n",
    "#             context_text = \" \".join(doc.page_content.lower() for doc in docs)\n",
    "#             shared = [word for word in answer.lower().split() if word in context_text]\n",
    "#             if len(shared) < 5:  # Tune this threshold to your needs\n",
    "#                 answer = \"âš ï¸ I'm not confident this answer is grounded in the provided context.\"\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         # If anything breaks in the chain (e.g. LLM crashed), report it cleanly\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "#\n",
    "#     # H) Append assistant's response, persist, and return everything\n",
    "#     messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#     save_history([m.dict() for m in messages])  # Your implementation\n",
    "#     return ChatResponse(answer=answer, history=messages)\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:48.497739Z",
     "start_time": "2025-05-27T20:52:48.484741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Notebook cell: run the RAG FastAPI server in the background\n",
    "# import threading\n",
    "# import uvicorn\n",
    "#\n",
    "# def run_server():\n",
    "#     uvicorn.run(\n",
    "#         app,\n",
    "#         host=\"0.0.0.0\",\n",
    "#         port=8000,\n",
    "#         log_level=\"info\",\n",
    "#         reload=False,        # disable auto-reload in notebooks\n",
    "#     )\n",
    "#\n",
    "# # Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "# server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "# server_thread.start()\n",
    "#\n",
    "# print(\"ğŸš€ RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "69cf251d74bd716e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:52.659456Z",
     "start_time": "2025-05-27T20:52:48.515739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) FastAPI & Required Imports\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from datasets import load_dataset\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1a) Chatâ€history persistence (messages only)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "def save_history(conversation: List[Dict[str, str]]):\n",
    "    timeline = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f) or []\n",
    "        except Exception:\n",
    "            timeline = []\n",
    "    timeline.append({\"timestamp\": datetime.now().isoformat(),\"conversation\": conversation})\n",
    "    tmp = HISTORY_FILE + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(timeline, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, HISTORY_FILE)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) Metrics loaders & intents\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "bleu_metric  = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    intents_data = json.load(f)[\"intents\"]\n",
    "\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) FastAPI + CORS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) Pydantic Models\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "    ground_truth_source: Optional[\n",
    "        Literal[\"gsm8k\",\"mbpp\",\"intents\",\"all\"]\n",
    "    ] = None\n",
    "\n",
    "class Metric(BaseModel):\n",
    "    dataset: str\n",
    "    bleu: Optional[float]\n",
    "    rouge: Optional[Dict[str, float]]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "    metrics: Optional[List[Metric]] = None\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) Utilities\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ARITH = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')\n",
    "def is_arithmetic(q: str) -> bool:\n",
    "    return bool(ARITH.fullmatch(q.strip()))\n",
    "\n",
    "def find_ground_truth(source: str, query: str) -> Optional[str]:\n",
    "    q = query.lower()\n",
    "    if source == \"gsm8k\":\n",
    "        ds = load_dataset(\"gsm8k\",\"main\",split=\"train\")\n",
    "        for it in ds:\n",
    "            if q in it[\"question\"].lower():\n",
    "                return it[\"answer\"]\n",
    "    elif source == \"mbpp\":\n",
    "        ds = load_dataset(\"google-research-datasets/mbpp\",\"sanitized\",split=\"train\")\n",
    "        for it in ds:\n",
    "            if q in it[\"text\"].lower():\n",
    "                return it[\"code\"]\n",
    "    elif source == \"intents\":\n",
    "        for intent in intents_data:\n",
    "            if any(q in p.lower() for p in intent[\"patterns\"]):\n",
    "                return intent[\"responses\"][0]\n",
    "    return None\n",
    "\n",
    "ALL_DATASETS = [\"gsm8k\",\"mbpp\",\"intents\"]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) Main Endpoint â€“ with fixed ROUGE flattening\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_endpoint(req: ChatRequest):\n",
    "    # A) Clean history of bad arithmetic\n",
    "    messages: List[ChatMessage] = []\n",
    "    for m in req.history or []:\n",
    "        if m.role==\"assistant\" and is_arithmetic(m.content):\n",
    "            corr = python_tool.run(m.content)\n",
    "            if m.content.strip()!=corr.strip():\n",
    "                messages.append(ChatMessage(\n",
    "                    role=\"system\",\n",
    "                    content=\"âš ï¸ Removed previous unsupported arithmetic answer\"\n",
    "                ))\n",
    "                continue\n",
    "        messages.append(m)\n",
    "\n",
    "    # B) Append new user turn\n",
    "    if not any(m.role==\"user\" and m.content==req.message for m in messages):\n",
    "        messages.append(ChatMessage(role=\"user\",content=req.message))\n",
    "\n",
    "    metrics_list: Optional[List[Dict[str,Any]]] = None\n",
    "\n",
    "    try:\n",
    "        # C) Mathâ€only shortcut with builtâ€in metric\n",
    "        if is_arithmetic(req.message):\n",
    "            answer = python_tool.run(req.message)\n",
    "            # Perfect metric for pure arithmetic\n",
    "            metrics_list = [{\n",
    "                \"dataset\":\"arithmetic\",\n",
    "                \"bleu\":1.0,\n",
    "                \"rouge\":{\"recall\":1.0}\n",
    "            }]\n",
    "            messages.append(ChatMessage(role=\"assistant\",content=answer))\n",
    "            save_history([m.dict() for m in messages])\n",
    "            return ChatResponse(answer=answer,history=messages,metrics=metrics_list)\n",
    "\n",
    "        # D) Retrievalâ€augmented answer\n",
    "        result = qa_chain({\"query\":req.message})\n",
    "        answer = result[\"result\"]\n",
    "\n",
    "        # E) Compute BLEU+ROUGE for each dataset if requested\n",
    "        if req.ground_truth_source in ALL_DATASETS+[\"all\"]:\n",
    "            to_do = ALL_DATASETS if req.ground_truth_source==\"all\" else [req.ground_truth_source]  # type: ignore\n",
    "            metrics_list = []\n",
    "            for ds in to_do:\n",
    "                b:Optional[float]=None\n",
    "                r:Optional[Dict[str,float]]=None\n",
    "                # Try to pull ground truth and compute\n",
    "                gt = find_ground_truth(ds, req.message)\n",
    "                if gt:\n",
    "                    try:\n",
    "                        b = bleu_metric.compute(\n",
    "                            predictions=[answer],\n",
    "                            references=[[gt]]\n",
    "                        )[\"bleu\"]\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ BLEU error {ds}:\",e)\n",
    "                    try:\n",
    "                        raw = rouge_metric.compute(\n",
    "                            predictions=[answer],\n",
    "                            references=[gt]\n",
    "                        )\n",
    "                        # flatten whether raw[v] is dict or float\n",
    "                        r = {}\n",
    "                        for name,val in raw.items():\n",
    "                            if isinstance(val, dict):\n",
    "                                r[name] = val.get(\"recall\",0.0)\n",
    "                            elif isinstance(val,(float,int)):\n",
    "                                r[name] = float(val)\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ ROUGE error {ds}:\",e)\n",
    "                metrics_list.append({\"dataset\":ds,\"bleu\":b,\"rouge\":r})\n",
    "\n",
    "        # F) Toxicity / hallucination filters\n",
    "        toxic = {\"kill\",\"hate\",\"stupid\",\"dumb\",\"racist\",\"sexist\",\"violence\",\"bomb\",\"terror\",\"die\",\"suicide\"}\n",
    "        if any(w in answer.lower() for w in toxic):\n",
    "            answer = \"âš ï¸ Response blocked due to potentially inappropriate content.\"\n",
    "        else:\n",
    "            ctx = \" \".join(doc.page_content.lower() for doc in result[\"source_documents\"])\n",
    "            shared = [w for w in answer.lower().split() if w in ctx]\n",
    "            if len(shared)<5:\n",
    "                answer = \"âš ï¸ I'm not confident this answer is grounded in context.\"\n",
    "\n",
    "    except Exception as exc:\n",
    "        raise HTTPException(status_code=500,detail=str(exc))\n",
    "\n",
    "    # G) Finalize and persist only messages\n",
    "    messages.append(ChatMessage(role=\"assistant\",content=answer))\n",
    "    save_history([m.dict() for m in messages])\n",
    "\n",
    "    return ChatResponse(\n",
    "        answer=answer,\n",
    "        history=messages,\n",
    "        metrics=metrics_list\n",
    "    )\n"
   ],
   "id": "a495d7ab8bc83de",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T20:52:52.722479Z",
     "start_time": "2025-05-27T20:52:52.691468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Notebook cell: run the RAG FastAPI server in the background\n",
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        reload=False,        # disable auto-reload in notebooks\n",
    "    )\n",
    "\n",
    "# Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"ğŸš€ RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "99b012da35df65fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ RAG server is now running at http://localhost:8000\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
