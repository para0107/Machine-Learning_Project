{
 "cells": [
  {
   "cell_type": "code",
   "id": "4c0763f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:04.599397Z",
     "start_time": "2025-05-25T15:09:04.576397Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "LAST_UPDATE_FILE = \"results/last_update.txt\"\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def validate_chat_history_format():\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            return\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "        wrapped = [{\"timestamp\": datetime.now().isoformat(), \"conversation\": data}]\n",
    "        with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(wrapped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    validate_chat_history_format()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b237f03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:04.738421Z",
     "start_time": "2025-05-25T15:09:04.632732Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "\n",
    "def save_history(conversation: list[dict]):\n",
    "    \"\"\"\n",
    "    conversation: a list of plain dicts, each with keys \"role\" and \"content\".\n",
    "    Appends it as a new entry under a timestamp, preserving any existing history.\n",
    "    \"\"\"\n",
    "    # 1) Load existing timeline\n",
    "    timeline: list = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f)\n",
    "                if not isinstance(timeline, list):\n",
    "                    timeline = []\n",
    "        except (json.JSONDecodeError, OSError) as e:\n",
    "            print(f\"⚠️  Could not read/parse {HISTORY_FILE}: {e!r}. Starting fresh.\")\n",
    "            timeline = []\n",
    "\n",
    "    # 2) Append this turn\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": conversation\n",
    "    }\n",
    "    timeline.append(entry)\n",
    "\n",
    "    # 3) Write atomically\n",
    "    tmp_file = HISTORY_FILE + \".tmp\"\n",
    "    try:\n",
    "        with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(timeline, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp_file, HISTORY_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed writing history to {HISTORY_FILE}: {e!r}\")\n",
    "        # Cleanup tmp if it remains\n",
    "        try:\n",
    "            if os.path.exists(tmp_file):\n",
    "                os.remove(tmp_file)\n",
    "        except OSError:\n",
    "            pass\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:15.180692Z",
     "start_time": "2025-05-25T15:09:05.006563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "5ab9b2cf56268dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:15.227390Z",
     "start_time": "2025-05-25T15:09:15.203881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "a942e88111c09cd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:21.896441Z",
     "start_time": "2025-05-25T15:09:15.248480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "874e06872db67446",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:29.386774Z",
     "start_time": "2025-05-25T15:09:21.912817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "21879a8c139d5d58",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:29.418372Z",
     "start_time": "2025-05-25T15:09:29.403919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "596c5f2c6a86785b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:41.909757Z",
     "start_time": "2025-05-25T15:09:29.436385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Cell 2: fix_chat_history_format()\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    \"\"\"\n",
    "    Alias for validate_chat_history_format, intended to run\n",
    "    right before loading via datasets or similar.\n",
    "    \"\"\"\n",
    "    validate_chat_history_format()\n",
    "    print(\"Ran fix_chat_history_format()\")\n",
    "\n",
    "\n",
    "# Call this function before loading the dataset\n",
    "fix_chat_history_format()\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "f591cc7b42d6d292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "743dec3f5bf4431f8be065590a4f4b74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e81804f9744b4546bd19c2b805b2d87b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7885\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:09:42.188791Z",
     "start_time": "2025-05-25T15:09:41.939405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "11bf1be1d3d533b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8135 chunks from 7885 documents.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:03.583339Z",
     "start_time": "2025-05-25T15:09:42.205783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "1fa4a1269b344159",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_23860\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:03.644302Z",
     "start_time": "2025-05-25T15:12:03.639302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "f5cc4747dd970db4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:06.089983Z",
     "start_time": "2025-05-25T15:12:03.693827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "93c1c76dc68bc7b9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:09.033292Z",
     "start_time": "2025-05-25T15:12:06.097848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# # ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "# query = \"How would you implement binary search in Python?\"\n",
    "# result = qa_chain(query)\n",
    "# print(\"Answer:\\n\", result[\"result\"])\n",
    "# print(\"\\nSources:\")\n",
    "# for doc in result[\"source_documents\"]:\n",
    "#     print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "\n"
   ],
   "id": "8ec760ff28f20c6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_23860\\2054238658.py:29: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "17efb8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:09.064292Z",
     "start_time": "2025-05-25T15:12:09.049285Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "MODEL_ID = \"meta-llama-3.1-8b-instruct\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer lm-studio\"}\n",
    "\n",
    "def chat():\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful programming tutor.\"}]\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     save_history(messages)\n",
    "        #     print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        #     break\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        payload = {\"model\": MODEL_ID, \"messages\": messages, \"temperature\": 0.7}\n",
    "        response = requests.post(URL, headers=HEADERS, json=payload, timeout=1000)\n",
    "        answer = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        # print(answer)\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     serialiable = [m.dict() if isinstance(m, BaseModel)\n",
    "        #                    else m\n",
    "        #                    for m in messages]\n",
    "        #     save_history(serialiable)\n",
    "        print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "9c3739c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:20.536042Z",
     "start_time": "2025-05-25T15:12:09.082291Z"
    }
   },
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "fix_chat_history_format()\n",
    "raw_chat = load_dataset(\"json\", data_files={\"train\": HISTORY_FILE})\n",
    "\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        user = [m[\"content\"] for m in conv if m[\"role\"] == \"user\"]\n",
    "        asst = [m[\"content\"] for m in conv if m[\"role\"] == \"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(format_chat_batch, batched=True, remove_columns=[\"timestamp\",\"conversation\"])\n",
    "\n",
    "raw_intents = load_dataset(\"json\", data_files={\"train\": \"intents.json\"})\n",
    "def format_intents_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(format_intents_batch, batched=True, remove_columns=[\"intents\"])\n",
    "\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(format_gsm_batch, batched=True, remove_columns=gsm.column_names)\n",
    "\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e17e55c6ac204467802c050bce17fbd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7885\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "9ecf1b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:20.582593Z",
     "start_time": "2025-05-25T15:12:20.566039Z"
    }
   },
   "source": [
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def get_new_conversations() -> List[Dict]:\n",
    "    last_update = None\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            last_update = f.read().strip()\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return []\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        history = json.load(f)\n",
    "        if not isinstance(history, list):\n",
    "            history = [history]\n",
    "    if last_update:\n",
    "        return [c for c in history if c.get(\"timestamp\", \"\") > last_update]\n",
    "    return history\n",
    "\n",
    "def is_correction(conv: Dict) -> bool:\n",
    "    msgs = conv.get(\"conversation\", [])\n",
    "    for i in range(1, len(msgs)):\n",
    "        if msgs[i][\"role\"] == \"user\" and msgs[i-1][\"role\"] == \"assistant\":\n",
    "            txt = msgs[i][\"content\"].lower()\n",
    "            if any(kw in txt for kw in [\"wrong\",\"incorrect\",\"mistake\",\"no,\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def conversations_to_docs(convs: List[Dict]) -> List[Document]:\n",
    "    docs = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for idx, conv in enumerate(convs):\n",
    "        text = \"\"\n",
    "        for msg in conv[\"conversation\"]:\n",
    "            if msg[\"role\"] != \"system\":\n",
    "                prefix = \"Question: \" if msg[\"role\"]==\"user\" else \"Answer: \"\n",
    "                text += f\"{prefix}{msg['content']}\\n\\n\"\n",
    "        metadata = {\"source\": f\"conversation-{idx}\", \"timestamp\": conv.get(\"timestamp\",\"\")}\n",
    "        for chunk in splitter.split_text(text):\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def update_knowledge_base():\n",
    "    new_convs = get_new_conversations()\n",
    "    if not new_convs:\n",
    "        print(\"No new conversations found.\")\n",
    "        return\n",
    "    corrections = [c for c in new_convs if is_correction(c)]\n",
    "    print(f\"Found {len(new_convs)} convs, {len(corrections)} with corrections\")\n",
    "    docs = conversations_to_docs(new_convs)\n",
    "    if not docs:\n",
    "        print(\"No documents to add.\")\n",
    "        return\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    try:\n",
    "        vs = FAISS.load_local(INDEX_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "    except:\n",
    "        vs = FAISS.from_documents(docs, embedder)\n",
    "    vs.add_documents(docs)\n",
    "    vs.save_local(INDEX_PATH)\n",
    "    os.makedirs(os.path.dirname(LAST_UPDATE_FILE), exist_ok=True)\n",
    "    with open(LAST_UPDATE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "    print(\"Knowledge base updated\")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "a0854251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:20.991669Z",
     "start_time": "2025-05-25T15:12:20.599722Z"
    }
   },
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Literal\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],\n",
    "    allow_credentials=True,\n",
    "     allow_methods=[\"*\"],\n",
    "                   allow_headers=[\"*\"]\n",
    ")\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_endpoint(req: ChatRequest):\n",
    "    messages = req.history if req.history else []\n",
    "\n",
    "    user_message_exists = any(\n",
    "        msg.role == \"user\" and msg.content == req.message\n",
    "        for msg in messages\n",
    "    )\n",
    "\n",
    "    # Only add the user message if it doesn't already exist\n",
    "    if not user_message_exists:\n",
    "        messages.append(ChatMessage(role=\"user\", content=req.message))\n",
    "\n",
    "    # Call your chain\n",
    "    try:\n",
    "        result = qa_chain({\"query\": req.message})\n",
    "        answer = result[\"result\"]\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    # Append assistant reply\n",
    "    messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "\n",
    "    # Convert to dict format for saving\n",
    "    messages_dict = [\n",
    "        msg.dict() if hasattr(msg, \"dict\") else msg\n",
    "        for msg in messages\n",
    "    ]\n",
    "\n",
    "    # Persist\n",
    "    save_history(messages_dict)\n",
    "\n",
    "    # Return response\n",
    "    return ChatResponse(answer=answer, history=messages)\n",
    "#\n",
    "# def start_rag_server():\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T15:12:21.023690Z",
     "start_time": "2025-05-25T15:12:21.009735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# notebook cell\n",
    "import threading, uvicorn\n",
    "\n",
    "def run_server():\n",
    "    config = uvicorn.Config(\n",
    "        app=app,                        # your FastAPI app\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        reload=False,                   # no file‐watcher in notebooks\n",
    "        # install_signal_handlers=False,  # avoids “signal only works in main thread”\n",
    "    )\n",
    "    server = uvicorn.Server(config)\n",
    "    server.run()                       # blocks inside this thread\n",
    "\n",
    "thread = threading.Thread(target=run_server, daemon=True)\n",
    "thread.start()\n",
    "print(\"🚀 RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "69cf251d74bd716e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 RAG server is now running at http://localhost:8000\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
