{
 "cells": [
  {
   "cell_type": "code",
   "id": "4c0763f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:01:04.517446Z",
     "start_time": "2025-05-28T09:01:04.500904Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "LAST_UPDATE_FILE = \"results/last_update.txt\"\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def validate_chat_history_format():\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            return\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "        wrapped = [{\"timestamp\": datetime.now().isoformat(), \"conversation\": data}]\n",
    "        with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(wrapped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    validate_chat_history_format()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b237f03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:01:04.611283Z",
     "start_time": "2025-05-28T09:01:04.529484Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "\n",
    "def save_history(conversation: list[dict]):\n",
    "    \"\"\"\n",
    "    conversation: a list of plain dicts, each with keys \"role\" and \"content\".\n",
    "    Appends it as a new entry under a timestamp, preserving any existing history.\n",
    "    \"\"\"\n",
    "    # 1) Load existing timeline\n",
    "    timeline: list = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f)\n",
    "                if not isinstance(timeline, list):\n",
    "                    timeline = []\n",
    "        except (json.JSONDecodeError, OSError) as e:\n",
    "            print(f\"⚠️  Could not read/parse {HISTORY_FILE}: {e!r}. Starting fresh.\")\n",
    "            timeline = []\n",
    "\n",
    "    # 2) Append this turn\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": conversation\n",
    "    }\n",
    "    timeline.append(entry)\n",
    "\n",
    "    # 3) Write atomically\n",
    "    tmp_file = HISTORY_FILE + \".tmp\"\n",
    "    try:\n",
    "        with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(timeline, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp_file, HISTORY_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed writing history to {HISTORY_FILE}: {e!r}\")\n",
    "        # Cleanup tmp if it remains\n",
    "        try:\n",
    "            if os.path.exists(tmp_file):\n",
    "                os.remove(tmp_file)\n",
    "        except OSError:\n",
    "            pass\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-28T09:01:04.833661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "5ab9b2cf56268dd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "a942e88111c09cd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "874e06872db67446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "21879a8c139d5d58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "596c5f2c6a86785b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Cell 2: fix_chat_history_format()\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    \"\"\"\n",
    "    Alias for validate_chat_history_format, intended to run\n",
    "    right before loading via datasets or similar.\n",
    "    \"\"\"\n",
    "    validate_chat_history_format()\n",
    "    print(\"Ran fix_chat_history_format()\")\n",
    "\n",
    "\n",
    "# Call this function before loading the dataset\n",
    "fix_chat_history_format()\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "f591cc7b42d6d292",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "11bf1be1d3d533b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "1fa4a1269b344159",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "f5cc4747dd970db4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "#\n",
    "# custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are a calm and knowledgeable tutor. Your task is to assist students by carefully analyzing each problem and providing clear, thoughtful solutions. Use the provided context if it contains relevant information, and otherwise rely on your own reasoning.\n",
    "#\n",
    "# Always begin your answer with:\n",
    "# \"I am here to help with a solution for this problem:\"\n",
    "#\n",
    "# Instructions:\n",
    "# - Carefully read the entire question. Pay close attention to:\n",
    "#   - Numbers, units, or constraints mentioned\n",
    "#   - Special conditions or exceptions\n",
    "#   - What exactly is being asked\n",
    "# - First look in the Context.\n",
    "#   - If you find the answer there, use it and briefly mention the source.\n",
    "#   - If the Context does not contain the answer, compute or explain it yourself using logical steps.\n",
    "# - Clearly show your step-by-step reasoning.\n",
    "# - Conclude with a direct and final answer prefixed by: \"Answer:\"\n",
    "# - Do not make up facts outside your reasoning.\n",
    "# - Stay calm and explanatory, like a patient teacher.\n",
    "#\n",
    "# Context:\n",
    "# {context}\n",
    "#\n",
    "# Question:\n",
    "# {question}\n",
    "#\n",
    "# Answer:\n",
    "# \"\"\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a calm, patient, and highly knowledgeable tutor. Your job is to answer exactly one student question per invocation. Questions may be:\n",
    "\n",
    "- **Math** (numeric, algebra, calculus)\n",
    "- **Informatics** (algorithms, data structures, programming)\n",
    "- **Pure Theory** (definitions, concepts)\n",
    "\n",
    "**0. Context Check**\n",
    "First, read the **Context** below.\n",
    "- If it contains the exact answer (a worked math solution, a definition, or code snippet), use that verbatim and note “(taken from context)”.\n",
    "- If not, proceed to the relevant step below.\n",
    "- Do *not* reprint the context unless asked.\n",
    "- *No extra answers*\n",
    "- **Do not invent or hallucinate.** If you are not absolutely certain of a fact or it’s not in the context, respond:\n",
    "\n",
    "\n",
    "**1. Math**\n",
    "- If the question is *only* a basic arithmetic expression (`+`, `-`, `*`, `/`, parentheses), compute it directly and return.\n",
    "- For all other math, pick the simplest method, show only necessary steps with clear reasoning.\n",
    "\n",
    "**2. Informatics**\n",
    "- Identify the concept or algorithm.\n",
    "- Explain concisely, using code examples only when they clarify.\n",
    "- Focus strictly on the question.\n",
    "\n",
    "**3. Pure Theory**\n",
    "- Provide a crisp authoritative definition.\n",
    "- If the context has it, use it verbatim with “(taken from context)”.\n",
    "- Otherwise, supply your own clear definition.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Respond in this format:\n",
    "\n",
    "---\n",
    "\n",
    "Problem Type: [Math or Informatics].\n",
    "\n",
    "[Your detailed explanation or step-by-step solution]\n",
    "\n",
    "Answer: <final result or conclusion>\n",
    "\n",
    "---\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ],
   "id": "f300ba52fd33a2f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "# Chain to generate answers with a defined prompt\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=custom_prompt\n",
    ")\n",
    "\n",
    "\n",
    "# # ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "#     retriever=retriever,\n",
    "#     return_source_documents=True,\n",
    "# ) -> pre-fabricated\n",
    "\n",
    "# Plug this into a RetrievalQA system\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",  # or \"map_reduce\", \"refine\" if you prefer\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True,\n",
    ")#custom retrieval\n",
    "\n",
    "# # ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "# query = \"How would you implement binary search in Python?\"\n",
    "# result = qa_chain(query)\n",
    "# print(\"Answer:\\n\", result[\"result\"])\n",
    "# print(\"\\nSources:\")\n",
    "# for doc in result[\"source_documents\"]:\n",
    "#     print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "# Test the chain with the correct input format\n",
    "# test_query = {\"context\": \"\", \"query\": \"What is Python?\"}  # Ensure both keys are included\n",
    "# try:\n",
    "#     test_result = qa_chain(test_query)\n",
    "#     print(\"✅ QA chain test successful!\")\n",
    "#     print(f\"Result: {test_result['result'][:100]}...\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ QA chain test failed with error: {e}\")\n"
   ],
   "id": "8ec760ff28f20c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "93c1c76dc68bc7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17efb8a7",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "MODEL_ID = \"meta-llama-3.1-8b-instruct\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer lm-studio\"}\n",
    "\n",
    "def chat():\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful programming tutor.\"}]\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     save_history(messages)\n",
    "        #     print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        #     break\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        payload = {\"model\": MODEL_ID, \"messages\": messages, \"temperature\": 0.7}\n",
    "        response = requests.post(URL, headers=HEADERS, json=payload, timeout=1000)\n",
    "        answer = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        # print(answer)\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     serialiable = [m.dict() if isinstance(m, BaseModel)\n",
    "        #                    else m\n",
    "        #                    for m in messages]\n",
    "        #     save_history(serialiable)\n",
    "        print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c3739c7",
   "metadata": {},
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "fix_chat_history_format()\n",
    "raw_chat = load_dataset(\"json\", data_files={\"train\": HISTORY_FILE})\n",
    "\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        user = [m[\"content\"] for m in conv if m[\"role\"] == \"user\"]\n",
    "        asst = [m[\"content\"] for m in conv if m[\"role\"] == \"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(format_chat_batch, batched=True, remove_columns=[\"timestamp\",\"conversation\"])\n",
    "\n",
    "raw_intents = load_dataset(\"json\", data_files={\"train\": \"intents.json\"})\n",
    "def format_intents_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(format_intents_batch, batched=True, remove_columns=[\"intents\"])\n",
    "\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(format_gsm_batch, batched=True, remove_columns=gsm.column_names)\n",
    "\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ecf1b4a",
   "metadata": {},
   "source": [
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def get_new_conversations() -> List[Dict]:\n",
    "    last_update = None\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            last_update = f.read().strip()\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return []\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        history = json.load(f)\n",
    "        if not isinstance(history, list):\n",
    "            history = [history]\n",
    "    if last_update:\n",
    "        return [c for c in history if c.get(\"timestamp\", \"\") > last_update]\n",
    "    return history\n",
    "\n",
    "def is_correction(conv: Dict) -> bool:\n",
    "    msgs = conv.get(\"conversation\", [])\n",
    "    for i in range(1, len(msgs)):\n",
    "        if msgs[i][\"role\"] == \"user\" and msgs[i-1][\"role\"] == \"assistant\":\n",
    "            txt = msgs[i][\"content\"].lower()\n",
    "            if any(kw in txt for kw in [\"wrong\",\"incorrect\",\"mistake\",\"no,\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def conversations_to_docs(convs: List[Dict]) -> List[Document]:\n",
    "    docs = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for idx, conv in enumerate(convs):\n",
    "        text = \"\"\n",
    "        for msg in conv[\"conversation\"]:\n",
    "            if msg[\"role\"] != \"system\":\n",
    "                prefix = \"Question: \" if msg[\"role\"]==\"user\" else \"Answer: \"\n",
    "                text += f\"{prefix}{msg['content']}\\n\\n\"\n",
    "        metadata = {\"source\": f\"conversation-{idx}\", \"timestamp\": conv.get(\"timestamp\",\"\")}\n",
    "        for chunk in splitter.split_text(text):\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def update_knowledge_base():\n",
    "    new_convs = get_new_conversations()\n",
    "    if not new_convs:\n",
    "        print(\"No new conversations found.\")\n",
    "        return\n",
    "    corrections = [c for c in new_convs if is_correction(c)]\n",
    "    print(f\"Found {len(new_convs)} convs, {len(corrections)} with corrections\")\n",
    "    docs = conversations_to_docs(new_convs)\n",
    "    if not docs:\n",
    "        print(\"No documents to add.\")\n",
    "        return\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    try:\n",
    "        vs = FAISS.load_local(INDEX_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "    except:\n",
    "        vs = FAISS.from_documents(docs, embedder)\n",
    "    vs.add_documents(docs)\n",
    "    vs.save_local(INDEX_PATH)\n",
    "    os.makedirs(os.path.dirname(LAST_UPDATE_FILE), exist_ok=True)\n",
    "    with open(LAST_UPDATE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "    print(\"Knowledge base updated\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0854251",
   "metadata": {},
   "source": [
    "# # ────────────────────────────────\n",
    "# # 1) FastAPI & Required Imports\n",
    "# # ────────────────────────────────\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Literal\n",
    "# import re\n",
    "# from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 2) Initialize Math Tool (Optional Shortcut for Arithmetic)\n",
    "# # ────────────────────────────────\n",
    "# # This tool allows your system to directly execute math expressions using Python\n",
    "# python_tool = PythonREPLTool()\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 3) FastAPI App Setup + CORS Config\n",
    "# # ────────────────────────────────\n",
    "# app = FastAPI()\n",
    "#\n",
    "# # This allows frontend (e.g., React at localhost:3000) to communicate with backend\n",
    "# app.add_middleware(\n",
    "#     CORSMiddleware,\n",
    "#     allow_origins=[\"http://localhost:3000\"],\n",
    "#     allow_credentials=True,\n",
    "#     allow_methods=[\"*\"],\n",
    "#     allow_headers=[\"*\"],\n",
    "# )\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 4) Data Models for Request and Response\n",
    "# # ────────────────────────────────\n",
    "#\n",
    "# # A single message in the conversation\n",
    "# class ChatMessage(BaseModel):\n",
    "#     role: Literal[\"user\", \"assistant\", \"system\"]  # Who sent the message\n",
    "#     content: str\n",
    "#\n",
    "# # The client sends a message + full history\n",
    "# class ChatRequest(BaseModel):\n",
    "#     message: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # The server returns the latest answer + updated history\n",
    "# class ChatResponse(BaseModel):\n",
    "#     answer: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 5) Utility: Detect simple arithmetic expressions like \"17 * 19\"\n",
    "# # ────────────────────────────────\n",
    "# ARITH_PATTERN = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')  # Accepts + - * / and parentheses\n",
    "# def is_arithmetic(q: str) -> bool:\n",
    "#     return bool(ARITH_PATTERN.fullmatch(q.strip()))\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 6) Main Endpoint: POST /rag_chat\n",
    "# # ────────────────────────────────\n",
    "# @app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "# async def rag_chat_endpoint(req: ChatRequest):\n",
    "#     # A) Start from user-submitted chat history, or empty\n",
    "#     messages = req.history or []\n",
    "#\n",
    "#     # B) Sanitize history:\n",
    "#     # If any past assistant messages contain incorrect arithmetic, remove them\n",
    "#     clean_hist: List[ChatMessage] = []\n",
    "#     for m in messages:\n",
    "#         if m.role == \"assistant\" and is_arithmetic(m.content):\n",
    "#             correct = python_tool.run(m.content)\n",
    "#             if m.content.strip() != correct.strip():\n",
    "#                 # Replace with a warning message\n",
    "#                 clean_hist.append(ChatMessage(\n",
    "#                     role=\"system\",\n",
    "#                     content=\"⚠️ Removed previous unsupported arithmetic answer\"\n",
    "#                 ))\n",
    "#                 continue\n",
    "#         clean_hist.append(m)\n",
    "#     messages = clean_hist  # Use the cleaned list going forward\n",
    "#\n",
    "#     # C) Append the new user message to the conversation\n",
    "#     if not any(m.role == \"user\" and m.content == req.message for m in messages):\n",
    "#         messages.append(ChatMessage(role=\"user\", content=req.message))\n",
    "#\n",
    "#     try:\n",
    "#         # D) Short-circuit: if the message is just math, compute it directly\n",
    "#         if is_arithmetic(req.message):\n",
    "#             answer = python_tool.run(req.message)\n",
    "#\n",
    "#             # Log answer into history and save\n",
    "#             messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#             save_history([m.dict() for m in messages])  # Your implementation\n",
    "#             return ChatResponse(answer=answer, history=messages)\n",
    "#\n",
    "#         # E) Fallback: normal LLM-powered RAG pipeline\n",
    "#         result = qa_chain({\"query\": req.message})  # This uses your RetrievalQA\n",
    "#         answer = result[\"result\"]\n",
    "#         docs = result[\"source_documents\"]  # These are the context documents\n",
    "#\n",
    "#         # F) Filter for toxicity (banned keywords)\n",
    "#         toxic = {\n",
    "#             \"kill\", \"hate\", \"stupid\", \"dumb\", \"racist\", \"sexist\",\n",
    "#             \"violence\", \"bomb\", \"terror\", \"die\", \"suicide\"\n",
    "#         }\n",
    "#         if any(word in answer.lower() for word in toxic):\n",
    "#             answer = \"⚠️ Response blocked due to potentially inappropriate content.\"\n",
    "#\n",
    "#         else:\n",
    "#             # G) Hallucination filter:\n",
    "#             # Check if answer actually shares tokens with source context\n",
    "#             context_text = \" \".join(doc.page_content.lower() for doc in docs)\n",
    "#             shared = [word for word in answer.lower().split() if word in context_text]\n",
    "#             if len(shared) < 5:  # Tune this threshold to your needs\n",
    "#                 answer = \"⚠️ I'm not confident this answer is grounded in the provided context.\"\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         # If anything breaks in the chain (e.g. LLM crashed), report it cleanly\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "#\n",
    "#     # H) Append assistant's response, persist, and return everything\n",
    "#     messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#     save_history([m.dict() for m in messages])  # Your implementation\n",
    "#     return ChatResponse(answer=answer, history=messages)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Notebook cell: run the RAG FastAPI server in the background\n",
    "# import threading\n",
    "# import uvicorn\n",
    "#\n",
    "# def run_server():\n",
    "#     uvicorn.run(\n",
    "#         app,\n",
    "#         host=\"0.0.0.0\",\n",
    "#         port=8000,\n",
    "#         log_level=\"info\",\n",
    "#         reload=False,        # disable auto-reload in notebooks\n",
    "#     )\n",
    "#\n",
    "# # Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "# server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "# server_thread.start()\n",
    "#\n",
    "# print(\"🚀 RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "69cf251d74bd716e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ────────────────────────────────\n",
    "# 1) FastAPI & Required Imports\n",
    "# ────────────────────────────────\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 1a) Chat‐history persistence (messages only)\n",
    "# ────────────────────────────────\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "def save_history(conversation: List[Dict[str, str]]):\n",
    "    timeline = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f) or []\n",
    "        except Exception:\n",
    "            timeline = []\n",
    "    timeline.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": conversation\n",
    "    })\n",
    "    tmp = HISTORY_FILE + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(timeline, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, HISTORY_FILE)\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 2) Metrics loaders & intents\n",
    "# ────────────────────────────────\n",
    "bleu_metric  = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    intents_data = json.load(f)[\"intents\"]\n",
    "\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 3) FastAPI + CORS\n",
    "# ────────────────────────────────\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 4) Pydantic Models\n",
    "# ────────────────────────────────\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 5) Utilities\n",
    "# ────────────────────────────────\n",
    "ARITH = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')\n",
    "def is_arithmetic(q: str) -> bool:\n",
    "    return bool(ARITH.fullmatch(q.strip()))\n",
    "\n",
    "ALL_DATASETS = [\"gsm8k\",\"mbpp\",\"intents\"]\n",
    "\n",
    "def find_ground_truth(source: str, query: str) -> Optional[str]:\n",
    "    \"\"\"Safely look up the ground truth for `query` in the given dataset.\"\"\"\n",
    "    q = query.lower()\n",
    "    try:\n",
    "        if source == \"gsm8k\":\n",
    "            ds = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "            for it in ds:\n",
    "                question = it.get(\"question\", \"\")\n",
    "                if q in question.lower():\n",
    "                    return it.get(\"answer\")\n",
    "        elif source == \"mbpp\":\n",
    "            ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\", split=\"train\")\n",
    "            for it in ds:\n",
    "                text = it.get(\"text\", \"\")\n",
    "                if q in text.lower():\n",
    "                    return it.get(\"code\")\n",
    "        elif source == \"intents\":\n",
    "            for intent in intents_data:\n",
    "                for p in intent.get(\"patterns\", []):\n",
    "                    if q in p.lower():\n",
    "                        return intent.get(\"responses\", [None])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Ground-truth lookup error for {source}:\", e)\n",
    "    return None\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 6) Main Endpoint – compute & print only non-null metrics\n",
    "# ────────────────────────────────\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_endpoint(req: ChatRequest):\n",
    "    messages: List[ChatMessage] = []\n",
    "\n",
    "    # A) Clean history of bad arithmetic\n",
    "    for m in req.history:\n",
    "        if m.role == \"assistant\" and is_arithmetic(m.content):\n",
    "            corr = python_tool.run(m.content)\n",
    "            if m.content.strip() != corr.strip():\n",
    "                messages.append(ChatMessage(\n",
    "                    role=\"system\",\n",
    "                    content=\"⚠️ Removed previous unsupported arithmetic answer\"\n",
    "                ))\n",
    "                continue\n",
    "        messages.append(m)\n",
    "\n",
    "    # B) Append new user turn\n",
    "    if not any(m.role == \"user\" and m.content == req.message for m in messages):\n",
    "        messages.append(ChatMessage(role=\"user\", content=req.message))\n",
    "\n",
    "    try:\n",
    "        # C) Math-only shortcut\n",
    "        if is_arithmetic(req.message):\n",
    "            answer = python_tool.run(req.message)\n",
    "            metrics_list = [{\n",
    "                \"dataset\": \"arithmetic\",\n",
    "                \"bleu\": 1.0,\n",
    "                \"rouge\": {\"recall\": 1.0}\n",
    "            }]\n",
    "            # print only non-null metrics\n",
    "            for m in metrics_list:\n",
    "                print(f\"bleu - {m['dataset']} = {m['bleu']}\")\n",
    "                for name, val in m[\"rouge\"].items():\n",
    "                    print(f\"{name} - {m['dataset']} = {val}\")\n",
    "\n",
    "            messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "            save_history([m.dict() for m in messages])\n",
    "            return ChatResponse(answer=answer, history=messages)\n",
    "\n",
    "        # D) Retrieval-augmented answer\n",
    "        result = qa_chain({\"query\": req.message})\n",
    "        answer = result[\"result\"]\n",
    "\n",
    "        # E) Compute BLEU+ROUGE for all datasets\n",
    "        metrics_list: List[Dict[str,Any]] = []\n",
    "        for ds in ALL_DATASETS:\n",
    "            b: Optional[float] = None\n",
    "            r: Optional[Dict[str,float]] = None\n",
    "            gt = find_ground_truth(ds, req.message)\n",
    "            if gt:\n",
    "                try:\n",
    "                    b = bleu_metric.compute(\n",
    "                        predictions=[answer],\n",
    "                        references=[[gt]]\n",
    "                    )[\"bleu\"]\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ BLEU error {ds}:\", e)\n",
    "                try:\n",
    "                    raw = rouge_metric.compute(\n",
    "                        predictions=[answer],\n",
    "                        references=[gt]\n",
    "                    )\n",
    "                    r = {}\n",
    "                    for nm, val in raw.items():\n",
    "                        if isinstance(val, dict):\n",
    "                            r[nm] = val.get(\"recall\", 0.0)\n",
    "                        elif isinstance(val, (float, int)):\n",
    "                            r[nm] = float(val)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ ROUGE error {ds}:\", e)\n",
    "            metrics_list.append({\"dataset\": ds, \"bleu\": b, \"rouge\": r})\n",
    "\n",
    "        # print only non-null metrics\n",
    "        for m in metrics_list:\n",
    "            if m[\"bleu\"] is not None:\n",
    "                print(f\"bleu - {m['dataset']} = {m['bleu']}\")\n",
    "            if m[\"rouge\"] is not None:\n",
    "                for name, val in m[\"rouge\"].items():\n",
    "                    if val is not None:\n",
    "                        print(f\"{name} - {m['dataset']} = {val}\")\n",
    "\n",
    "        # F) Toxicity / hallucination filters\n",
    "        toxic = {\"kill\",\"hate\",\"stupid\",\"dumb\",\"racist\",\"sexist\",\"violence\",\"bomb\",\"terror\",\"die\",\"suicide\"}\n",
    "        if any(w in answer.lower() for w in toxic):\n",
    "            answer = \"⚠️ Response blocked due to potentially inappropriate content.\"\n",
    "        else:\n",
    "            ctx = \" \".join(doc.page_content.lower() for doc in result[\"source_documents\"])\n",
    "            shared = [w for w in answer.lower().split() if w in ctx]\n",
    "            if len(shared) < 5:\n",
    "                answer = \"⚠️ I'm not confident this answer is grounded in context.\"\n",
    "\n",
    "    except Exception as exc:\n",
    "        raise HTTPException(status_code=500, detail=str(exc))\n",
    "\n",
    "    # G) Finalize and persist\n",
    "    messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "    save_history([m.dict() for m in messages])\n",
    "    return ChatResponse(answer=answer, history=messages)\n",
    "\n"
   ],
   "id": "a495d7ab8bc83de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ────────────────────────────────\n",
    "# 7) TEST CALL (in-notebook) to see prints under this cell\n",
    "# ────────────────────────────────\n",
    "# client = TestClient(app)\n",
    "# response = client.post(\n",
    "#     \"/rag_chat\",\n",
    "#     json={\n",
    "#         \"message\": \"What is computer architecture?\",\n",
    "#         \"history\": []\n",
    "#     }\n",
    "# )\n",
    "# print(\"Response JSON:\", response.json())\n"
   ],
   "id": "dfa0f3c5e2276a25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # ────────────────────────────────\n",
    "# # Cell: LLM-Judge with real-time feedback\n",
    "# # ────────────────────────────────\n",
    "# import os\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from fastapi.testclient import TestClient\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Literal, Dict, Any, Optional\n",
    "# from datasets import load_dataset\n",
    "# from evaluate import load as load_metric\n",
    "# from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "# from langchain import OpenAI, LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from dotenv import load_dotenv\n",
    "#\n",
    "# # 1) Ensure API key is set (or replace with your key string)\n",
    "# load_dotenv(dotenv_path=\"llm.env\")\n",
    "#\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not api_key:\n",
    "#     raise ValueError(\n",
    "#         \"Missing OPENAI_API_KEY environment variable. \"\n",
    "#         \"Please create a .env file with your key.\"\n",
    "#     )\n",
    "# # 2) Build the judge prompt & chain\n",
    "# judge_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are an expert evaluator. Given a Question and an Answer, produce:\n",
    "# Verdict: Correct or Incorrect\n",
    "# Score: a number between 0.0 and 1.0\n",
    "# Comments: concise feedback.\n",
    "#\n",
    "# Question:\n",
    "# {question}\n",
    "#\n",
    "# Answer:\n",
    "# {answer}\n",
    "# \"\"\")\n",
    "# judge_llm = OpenAI(temperature=0, openai_api_key=api_key)\n",
    "# judge_chain = LLMChain(llm=judge_llm, prompt=judge_prompt)\n",
    "#\n",
    "# # 3) (Re)define your FastAPI app and models as before\n",
    "# app = FastAPI()\n",
    "# app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "#\n",
    "# class ChatMessage(BaseModel):\n",
    "#     role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "#     content: str\n",
    "#\n",
    "# class ChatRequest(BaseModel):\n",
    "#     message: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# class ChatResponse(BaseModel):\n",
    "#     answer: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # (Include your is_arithmetic, find_ground_truth, metrics logic here…)\n",
    "#\n",
    "# @app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "# async def rag_chat_with_judge(req: ChatRequest):\n",
    "#     # Call your original rag_chat logic (inline or imported)\n",
    "#     # For brevity, assume rag_chat returns ChatResponse\n",
    "#\n",
    "#     resp: ChatResponse = await rag_chat_endpoint(req)\n",
    "#\n",
    "#     # 4) Run the judge immediately and print it\n",
    "#     evaluation = judge_chain.run(question=req.message, answer=resp.answer)\n",
    "#     print(\"\\n🔍 LLM Judge Evaluation:\\n\" + evaluation)\n",
    "#\n",
    "#     return resp\n",
    "#\n",
    "# # 5) Test in-notebook to see prints under this cell\n",
    "# client = TestClient(app)\n",
    "# response = client.post(\"/rag_chat\", json={\"message\":\"2+4\",\"history\":[]})\n",
    "# print(\"Response JSON:\", response.json())\n"
   ],
   "id": "917270ec2f08ad8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Notebook cell: run the RAG FastAPI server in the background\n",
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        reload=False,        # disable auto-reload in notebooks\n",
    "    )\n",
    "\n",
    "# Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"🚀 RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "99b012da35df65fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 1: app definition + logging setup\n",
    "import os, json, re, logging\n",
    "from datetime import datetime\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from openai import RateLimitError\n",
    "\n",
    "# —───────────────\n",
    "# 0) Logging config\n",
    "# —───────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),                     # console\n",
    "        logging.FileHandler(\"judge_evaluations.log\") # file\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# —───────────────\n",
    "# 1) LLM‐Judge chain\n",
    "# —───────────────\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"sk-YOUR_KEY_HERE\")\n",
    "judge_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert evaluator. Given a Question and an Answer, produce:\n",
    "Verdict: Correct or Incorrect\n",
    "Score: a number between 0.0 and 1.0\n",
    "Comments: concise feedback.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\")\n",
    "judge_llm   = OpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "judge_chain = LLMChain(llm=judge_llm, prompt=judge_prompt)\n",
    "\n",
    "def run_judge(question: str, answer: str):\n",
    "    try:\n",
    "        verdict = judge_chain.run(question=question, answer=answer)\n",
    "        logger.info(\"Judge verdict:\\n%s\", verdict)\n",
    "        return verdict\n",
    "    except RateLimitError as e:\n",
    "        msg = \"Judge skipped – rate limit\"\n",
    "        logger.warning(msg + \": %s\", e)\n",
    "        return msg\n",
    "    except Exception as e:\n",
    "        msg = f\"Judge error: {e}\"\n",
    "        logger.error(msg)\n",
    "        return msg\n",
    "\n",
    "# —───────────────\n",
    "# 2) RAG endpoint (import or inline your rag_chat_endpoint)\n",
    "# —───────────────\n",
    "# … your existing rag_chat_endpoint code goes here …\n",
    "# It must return an instance of ChatResponse.\n",
    "# For brevity assume:\n",
    "\n",
    "\n",
    "# —───────────────\n",
    "# 3) FastAPI wiring\n",
    "# —───────────────\n",
    "app = FastAPI()\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer:  str\n",
    "    history: List[ChatMessage]\n",
    "    metrics: Optional[List[Dict[str,Any]]] = None\n",
    "    evaluation: Optional[str] = None\n",
    "\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_with_judge(req: ChatRequest, bg: BackgroundTasks):\n",
    "    # 1) normal RAG → ChatResponse\n",
    "    resp: ChatResponse = await rag_chat_endpoint(req)\n",
    "\n",
    "    # 2) schedule judge in background\n",
    "    bg.add_task(run_judge, req.message, resp.answer)\n",
    "\n",
    "    # 3) immediately return response (no evaluation field yet)\n",
    "    return resp\n"
   ],
   "id": "e18409e709dae3d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cell 2A: Get metrics from the API, then synchronously call run_judge\n",
    "from fastapi.testclient import TestClient\n",
    "import json\n",
    "\n",
    "# assume `app` and `run_judge` are already in scope from Cell 1\n",
    "client = TestClient(app)\n",
    "\n",
    "# 1) Fire the request *with* ground_truth_source so metrics are computed\n",
    "payload = {\n",
    "    \"message\": \"What is 2^10?\",\n",
    "    \"history\": [],\n",
    "    \"ground_truth_source\": \"all\"\n",
    "}\n",
    "resp = client.post(\"/rag_chat\", json=payload)\n",
    "data = resp.json()\n",
    "\n",
    "# 2) Print out the metrics\n",
    "print(\"=== Metrics ===\")\n",
    "if data.get(\"metrics\"):\n",
    "    for m in data[\"metrics\"]:\n",
    "        print(f\"{m['dataset']:>10} →  BLEU={m['bleu']}, ROUGE={m['rouge']}\")\n",
    "else:\n",
    "    print(\"No metrics returned\")\n",
    "\n",
    "# 3) Now synchronously run the judge on that same question/answer\n",
    "print(\"\\n=== LLM-Judge Evaluation ===\")\n",
    "verdict = run_judge(payload[\"message\"], data[\"answer\"])\n",
    "print(verdict)\n"
   ],
   "id": "d960f6f2fa2077ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:57:54,741 INFO HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_21676\\546098977.py:210: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  save_history([m.dict() for m in messages])\n",
      "2025-05-28 11:58:12,838 INFO HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-05-28 11:58:12,841 INFO Retrying request to /completions in 0.387882 seconds\n",
      "2025-05-28 11:58:13,436 INFO HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-05-28 11:58:13,438 INFO Retrying request to /completions in 0.815001 seconds\n",
      "2025-05-28 11:58:14,451 INFO HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-05-28 11:58:14,453 WARNING Judge skipped – rate limit: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2025-05-28 11:58:14,455 INFO HTTP Request: POST http://testserver/rag_chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      " No metrics returned.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:58:22.104973Z",
     "start_time": "2025-05-28T08:58:22.042757Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x96 in position 681: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo judge_evaluations.log found.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m----> 8\u001B[0m     lines \u001B[38;5;241m=\u001B[39m \u001B[43mlogpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msplitlines()\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# Filter for the lines starting with our verdict marker\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     verdict_lines \u001B[38;5;241m=\u001B[39m [l \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lines \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJudge verdict:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m l]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pathlib.py:1135\u001B[0m, in \u001B[0;36mPath.read_text\u001B[1;34m(self, encoding, errors)\u001B[0m\n\u001B[0;32m   1133\u001B[0m encoding \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mtext_encoding(encoding)\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopen(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m-> 1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:322\u001B[0m, in \u001B[0;36mBufferedIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;66;03m# decode input (taking the buffer into account)\u001B[39;00m\n\u001B[0;32m    321\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[1;32m--> 322\u001B[0m     (result, consumed) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;66;03m# keep undecoded input until the next call\u001B[39;00m\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m=\u001B[39m data[consumed:]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0x96 in position 681: invalid start byte"
     ]
    }
   ],
   "execution_count": 30,
   "source": [
    "# Cell 2B: Tail judge_evaluations.log\n",
    "from pathlib import Path\n",
    "\n",
    "log = Path(\"judge_evaluations.log\")\n",
    "if not log.exists():\n",
    "    print(\"No judge_evaluations.log found.\")\n",
    "else:\n",
    "    lines = log.read_text().splitlines()\n",
    "    print(\"=== Last 5 Judge Log Lines ===\")\n",
    "    for line in lines[-5:]:\n",
    "        print(line)\n"
   ],
   "id": "59ec58c2b5c00c09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
