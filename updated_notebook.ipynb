{
 "cells": [
  {
   "cell_type": "code",
   "id": "4c0763f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:51:32.277776Z",
     "start_time": "2025-05-28T07:51:32.265649Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "LAST_UPDATE_FILE = \"results/last_update.txt\"\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def validate_chat_history_format():\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            return\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "        wrapped = [{\"timestamp\": datetime.now().isoformat(), \"conversation\": data}]\n",
    "        with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(wrapped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    validate_chat_history_format()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b237f03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:51:32.481295Z",
     "start_time": "2025-05-28T07:51:32.387297Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "\n",
    "def save_history(conversation: list[dict]):\n",
    "    \"\"\"\n",
    "    conversation: a list of plain dicts, each with keys \"role\" and \"content\".\n",
    "    Appends it as a new entry under a timestamp, preserving any existing history.\n",
    "    \"\"\"\n",
    "    # 1) Load existing timeline\n",
    "    timeline: list = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f)\n",
    "                if not isinstance(timeline, list):\n",
    "                    timeline = []\n",
    "        except (json.JSONDecodeError, OSError) as e:\n",
    "            print(f\"⚠️  Could not read/parse {HISTORY_FILE}: {e!r}. Starting fresh.\")\n",
    "            timeline = []\n",
    "\n",
    "    # 2) Append this turn\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": conversation\n",
    "    }\n",
    "    timeline.append(entry)\n",
    "\n",
    "    # 3) Write atomically\n",
    "    tmp_file = HISTORY_FILE + \".tmp\"\n",
    "    try:\n",
    "        with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(timeline, f, indent=2, ensure_ascii=False)\n",
    "        os.replace(tmp_file, HISTORY_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed writing history to {HISTORY_FILE}: {e!r}\")\n",
    "        # Cleanup tmp if it remains\n",
    "        try:\n",
    "            if os.path.exists(tmp_file):\n",
    "                os.remove(tmp_file)\n",
    "        except OSError:\n",
    "            pass\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:51:45.049780Z",
     "start_time": "2025-05-28T07:51:32.797260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "5ab9b2cf56268dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:51:45.095831Z",
     "start_time": "2025-05-28T07:51:45.070781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "a942e88111c09cd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:51:50.504911Z",
     "start_time": "2025-05-28T07:51:45.120780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "874e06872db67446",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:51:57.582164Z",
     "start_time": "2025-05-28T07:51:50.520357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "21879a8c139d5d58",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:51:57.612152Z",
     "start_time": "2025-05-28T07:51:57.599155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "596c5f2c6a86785b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:52:08.110968Z",
     "start_time": "2025-05-28T07:51:57.630546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Cell 2: fix_chat_history_format()\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    \"\"\"\n",
    "    Alias for validate_chat_history_format, intended to run\n",
    "    right before loading via datasets or similar.\n",
    "    \"\"\"\n",
    "    validate_chat_history_format()\n",
    "    print(\"Ran fix_chat_history_format()\")\n",
    "\n",
    "\n",
    "# Call this function before loading the dataset\n",
    "fix_chat_history_format()\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "f591cc7b42d6d292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37fa88d68ec04b59bf8252ffe09e0084"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "037e636af5144efab9a7c0f0215b444f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7880\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:52:08.393141Z",
     "start_time": "2025-05-28T07:52:08.130958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "11bf1be1d3d533b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8139 chunks from 7880 documents.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:30.326812Z",
     "start_time": "2025-05-28T07:52:08.411141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "1fa4a1269b344159",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_7628\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:30.374721Z",
     "start_time": "2025-05-28T07:54:30.370719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "f5cc4747dd970db4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:30.486833Z",
     "start_time": "2025-05-28T07:54:30.440830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "#\n",
    "# custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are a calm and knowledgeable tutor. Your task is to assist students by carefully analyzing each problem and providing clear, thoughtful solutions. Use the provided context if it contains relevant information, and otherwise rely on your own reasoning.\n",
    "#\n",
    "# Always begin your answer with:\n",
    "# \"I am here to help with a solution for this problem:\"\n",
    "#\n",
    "# Instructions:\n",
    "# - Carefully read the entire question. Pay close attention to:\n",
    "#   - Numbers, units, or constraints mentioned\n",
    "#   - Special conditions or exceptions\n",
    "#   - What exactly is being asked\n",
    "# - First look in the Context.\n",
    "#   - If you find the answer there, use it and briefly mention the source.\n",
    "#   - If the Context does not contain the answer, compute or explain it yourself using logical steps.\n",
    "# - Clearly show your step-by-step reasoning.\n",
    "# - Conclude with a direct and final answer prefixed by: \"Answer:\"\n",
    "# - Do not make up facts outside your reasoning.\n",
    "# - Stay calm and explanatory, like a patient teacher.\n",
    "#\n",
    "# Context:\n",
    "# {context}\n",
    "#\n",
    "# Question:\n",
    "# {question}\n",
    "#\n",
    "# Answer:\n",
    "# \"\"\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a calm, patient, and highly knowledgeable tutor. Your job is to answer exactly one student question per invocation. Questions may be:\n",
    "\n",
    "- **Math** (numeric, algebra, calculus)\n",
    "- **Informatics** (algorithms, data structures, programming)\n",
    "- **Pure Theory** (definitions, concepts)\n",
    "\n",
    "**0. Context Check**\n",
    "First, read the **Context** below.\n",
    "- If it contains the exact answer (a worked math solution, a definition, or code snippet), use that verbatim and note “(taken from context)”.\n",
    "- If not, proceed to the relevant step below.\n",
    "- Do *not* reprint the context unless asked.\n",
    "- *No extra answers*\n",
    "- **Do not invent or hallucinate.** If you are not absolutely certain of a fact or it’s not in the context, respond:\n",
    "\n",
    "\n",
    "**1. Math**\n",
    "- If the question is *only* a basic arithmetic expression (`+`, `-`, `*`, `/`, parentheses), compute it directly and return.\n",
    "- For all other math, pick the simplest method, show only necessary steps with clear reasoning.\n",
    "\n",
    "**2. Informatics**\n",
    "- Identify the concept or algorithm.\n",
    "- Explain concisely, using code examples only when they clarify.\n",
    "- Focus strictly on the question.\n",
    "\n",
    "**3. Pure Theory**\n",
    "- Provide a crisp authoritative definition.\n",
    "- If the context has it, use it verbatim with “(taken from context)”.\n",
    "- Otherwise, supply your own clear definition.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Respond in this format:\n",
    "\n",
    "---\n",
    "\n",
    "Problem Type: [Math or Informatics].\n",
    "\n",
    "[Your detailed explanation or step-by-step solution]\n",
    "\n",
    "Answer: <final result or conclusion>\n",
    "\n",
    "---\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ],
   "id": "f300ba52fd33a2f5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:33.376950Z",
     "start_time": "2025-05-28T07:54:30.494829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "# Chain to generate answers with a defined prompt\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=custom_prompt\n",
    ")\n",
    "\n",
    "\n",
    "# # ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "#     retriever=retriever,\n",
    "#     return_source_documents=True,\n",
    "# ) -> pre-fabricated\n",
    "\n",
    "# Plug this into a RetrievalQA system\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",  # or \"map_reduce\", \"refine\" if you prefer\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True,\n",
    ")#custom retrieval\n",
    "\n",
    "# # ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "# query = \"How would you implement binary search in Python?\"\n",
    "# result = qa_chain(query)\n",
    "# print(\"Answer:\\n\", result[\"result\"])\n",
    "# print(\"\\nSources:\")\n",
    "# for doc in result[\"source_documents\"]:\n",
    "#     print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "# Test the chain with the correct input format\n",
    "# test_query = {\"context\": \"\", \"query\": \"What is Python?\"}  # Ensure both keys are included\n",
    "# try:\n",
    "#     test_result = qa_chain(test_query)\n",
    "#     print(\"✅ QA chain test successful!\")\n",
    "#     print(f\"Result: {test_result['result'][:100]}...\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ QA chain test failed with error: {e}\")\n"
   ],
   "id": "8ec760ff28f20c6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_7628\\1712614391.py:30: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_7628\\1712614391.py:38: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:35.438204Z",
     "start_time": "2025-05-28T07:54:33.393982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "93c1c76dc68bc7b9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "17efb8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:35.469208Z",
     "start_time": "2025-05-28T07:54:35.455214Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "MODEL_ID = \"meta-llama-3.1-8b-instruct\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer lm-studio\"}\n",
    "\n",
    "def chat():\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful programming tutor.\"}]\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     save_history(messages)\n",
    "        #     print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        #     break\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        payload = {\"model\": MODEL_ID, \"messages\": messages, \"temperature\": 0.7}\n",
    "        response = requests.post(URL, headers=HEADERS, json=payload, timeout=1000)\n",
    "        answer = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        # print(answer)\n",
    "        # if user_input.lower() == \"exit\":\n",
    "        #     serialiable = [m.dict() if isinstance(m, BaseModel)\n",
    "        #                    else m\n",
    "        #                    for m in messages]\n",
    "        #     save_history(serialiable)\n",
    "        print(f\"Conversation saved to {HISTORY_FILE}\")\n",
    "        break"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "9c3739c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:45.775662Z",
     "start_time": "2025-05-28T07:54:35.485221Z"
    }
   },
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "fix_chat_history_format()\n",
    "raw_chat = load_dataset(\"json\", data_files={\"train\": HISTORY_FILE})\n",
    "\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        user = [m[\"content\"] for m in conv if m[\"role\"] == \"user\"]\n",
    "        asst = [m[\"content\"] for m in conv if m[\"role\"] == \"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(format_chat_batch, batched=True, remove_columns=[\"timestamp\",\"conversation\"])\n",
    "\n",
    "raw_intents = load_dataset(\"json\", data_files={\"train\": \"intents.json\"})\n",
    "def format_intents_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(format_intents_batch, batched=True, remove_columns=[\"intents\"])\n",
    "\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(format_gsm_batch, batched=True, remove_columns=gsm.column_names)\n",
    "\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40e6d93c2ba94c898729a9f98412c14d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7880\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "9ecf1b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:45.806854Z",
     "start_time": "2025-05-28T07:54:45.792933Z"
    }
   },
   "source": [
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def get_new_conversations() -> List[Dict]:\n",
    "    last_update = None\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            last_update = f.read().strip()\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return []\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        history = json.load(f)\n",
    "        if not isinstance(history, list):\n",
    "            history = [history]\n",
    "    if last_update:\n",
    "        return [c for c in history if c.get(\"timestamp\", \"\") > last_update]\n",
    "    return history\n",
    "\n",
    "def is_correction(conv: Dict) -> bool:\n",
    "    msgs = conv.get(\"conversation\", [])\n",
    "    for i in range(1, len(msgs)):\n",
    "        if msgs[i][\"role\"] == \"user\" and msgs[i-1][\"role\"] == \"assistant\":\n",
    "            txt = msgs[i][\"content\"].lower()\n",
    "            if any(kw in txt for kw in [\"wrong\",\"incorrect\",\"mistake\",\"no,\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def conversations_to_docs(convs: List[Dict]) -> List[Document]:\n",
    "    docs = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for idx, conv in enumerate(convs):\n",
    "        text = \"\"\n",
    "        for msg in conv[\"conversation\"]:\n",
    "            if msg[\"role\"] != \"system\":\n",
    "                prefix = \"Question: \" if msg[\"role\"]==\"user\" else \"Answer: \"\n",
    "                text += f\"{prefix}{msg['content']}\\n\\n\"\n",
    "        metadata = {\"source\": f\"conversation-{idx}\", \"timestamp\": conv.get(\"timestamp\",\"\")}\n",
    "        for chunk in splitter.split_text(text):\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def update_knowledge_base():\n",
    "    new_convs = get_new_conversations()\n",
    "    if not new_convs:\n",
    "        print(\"No new conversations found.\")\n",
    "        return\n",
    "    corrections = [c for c in new_convs if is_correction(c)]\n",
    "    print(f\"Found {len(new_convs)} convs, {len(corrections)} with corrections\")\n",
    "    docs = conversations_to_docs(new_convs)\n",
    "    if not docs:\n",
    "        print(\"No documents to add.\")\n",
    "        return\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    try:\n",
    "        vs = FAISS.load_local(INDEX_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "    except:\n",
    "        vs = FAISS.from_documents(docs, embedder)\n",
    "    vs.add_documents(docs)\n",
    "    vs.save_local(INDEX_PATH)\n",
    "    os.makedirs(os.path.dirname(LAST_UPDATE_FILE), exist_ok=True)\n",
    "    with open(LAST_UPDATE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "    print(\"Knowledge base updated\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a0854251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:45.853942Z",
     "start_time": "2025-05-28T07:54:45.838853Z"
    }
   },
   "source": [
    "# # ────────────────────────────────\n",
    "# # 1) FastAPI & Required Imports\n",
    "# # ────────────────────────────────\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List, Literal\n",
    "# import re\n",
    "# from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 2) Initialize Math Tool (Optional Shortcut for Arithmetic)\n",
    "# # ────────────────────────────────\n",
    "# # This tool allows your system to directly execute math expressions using Python\n",
    "# python_tool = PythonREPLTool()\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 3) FastAPI App Setup + CORS Config\n",
    "# # ────────────────────────────────\n",
    "# app = FastAPI()\n",
    "#\n",
    "# # This allows frontend (e.g., React at localhost:3000) to communicate with backend\n",
    "# app.add_middleware(\n",
    "#     CORSMiddleware,\n",
    "#     allow_origins=[\"http://localhost:3000\"],\n",
    "#     allow_credentials=True,\n",
    "#     allow_methods=[\"*\"],\n",
    "#     allow_headers=[\"*\"],\n",
    "# )\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 4) Data Models for Request and Response\n",
    "# # ────────────────────────────────\n",
    "#\n",
    "# # A single message in the conversation\n",
    "# class ChatMessage(BaseModel):\n",
    "#     role: Literal[\"user\", \"assistant\", \"system\"]  # Who sent the message\n",
    "#     content: str\n",
    "#\n",
    "# # The client sends a message + full history\n",
    "# class ChatRequest(BaseModel):\n",
    "#     message: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # The server returns the latest answer + updated history\n",
    "# class ChatResponse(BaseModel):\n",
    "#     answer: str\n",
    "#     history: List[ChatMessage]\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 5) Utility: Detect simple arithmetic expressions like \"17 * 19\"\n",
    "# # ────────────────────────────────\n",
    "# ARITH_PATTERN = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')  # Accepts + - * / and parentheses\n",
    "# def is_arithmetic(q: str) -> bool:\n",
    "#     return bool(ARITH_PATTERN.fullmatch(q.strip()))\n",
    "#\n",
    "# # ────────────────────────────────\n",
    "# # 6) Main Endpoint: POST /rag_chat\n",
    "# # ────────────────────────────────\n",
    "# @app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "# async def rag_chat_endpoint(req: ChatRequest):\n",
    "#     # A) Start from user-submitted chat history, or empty\n",
    "#     messages = req.history or []\n",
    "#\n",
    "#     # B) Sanitize history:\n",
    "#     # If any past assistant messages contain incorrect arithmetic, remove them\n",
    "#     clean_hist: List[ChatMessage] = []\n",
    "#     for m in messages:\n",
    "#         if m.role == \"assistant\" and is_arithmetic(m.content):\n",
    "#             correct = python_tool.run(m.content)\n",
    "#             if m.content.strip() != correct.strip():\n",
    "#                 # Replace with a warning message\n",
    "#                 clean_hist.append(ChatMessage(\n",
    "#                     role=\"system\",\n",
    "#                     content=\"⚠️ Removed previous unsupported arithmetic answer\"\n",
    "#                 ))\n",
    "#                 continue\n",
    "#         clean_hist.append(m)\n",
    "#     messages = clean_hist  # Use the cleaned list going forward\n",
    "#\n",
    "#     # C) Append the new user message to the conversation\n",
    "#     if not any(m.role == \"user\" and m.content == req.message for m in messages):\n",
    "#         messages.append(ChatMessage(role=\"user\", content=req.message))\n",
    "#\n",
    "#     try:\n",
    "#         # D) Short-circuit: if the message is just math, compute it directly\n",
    "#         if is_arithmetic(req.message):\n",
    "#             answer = python_tool.run(req.message)\n",
    "#\n",
    "#             # Log answer into history and save\n",
    "#             messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#             save_history([m.dict() for m in messages])  # Your implementation\n",
    "#             return ChatResponse(answer=answer, history=messages)\n",
    "#\n",
    "#         # E) Fallback: normal LLM-powered RAG pipeline\n",
    "#         result = qa_chain({\"query\": req.message})  # This uses your RetrievalQA\n",
    "#         answer = result[\"result\"]\n",
    "#         docs = result[\"source_documents\"]  # These are the context documents\n",
    "#\n",
    "#         # F) Filter for toxicity (banned keywords)\n",
    "#         toxic = {\n",
    "#             \"kill\", \"hate\", \"stupid\", \"dumb\", \"racist\", \"sexist\",\n",
    "#             \"violence\", \"bomb\", \"terror\", \"die\", \"suicide\"\n",
    "#         }\n",
    "#         if any(word in answer.lower() for word in toxic):\n",
    "#             answer = \"⚠️ Response blocked due to potentially inappropriate content.\"\n",
    "#\n",
    "#         else:\n",
    "#             # G) Hallucination filter:\n",
    "#             # Check if answer actually shares tokens with source context\n",
    "#             context_text = \" \".join(doc.page_content.lower() for doc in docs)\n",
    "#             shared = [word for word in answer.lower().split() if word in context_text]\n",
    "#             if len(shared) < 5:  # Tune this threshold to your needs\n",
    "#                 answer = \"⚠️ I'm not confident this answer is grounded in the provided context.\"\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         # If anything breaks in the chain (e.g. LLM crashed), report it cleanly\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "#\n",
    "#     # H) Append assistant's response, persist, and return everything\n",
    "#     messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "#     save_history([m.dict() for m in messages])  # Your implementation\n",
    "#     return ChatResponse(answer=answer, history=messages)\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:45.869330Z",
     "start_time": "2025-05-28T07:54:45.860897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Notebook cell: run the RAG FastAPI server in the background\n",
    "# import threading\n",
    "# import uvicorn\n",
    "#\n",
    "# def run_server():\n",
    "#     uvicorn.run(\n",
    "#         app,\n",
    "#         host=\"0.0.0.0\",\n",
    "#         port=8000,\n",
    "#         log_level=\"info\",\n",
    "#         reload=False,        # disable auto-reload in notebooks\n",
    "#     )\n",
    "#\n",
    "# # Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "# server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "# server_thread.start()\n",
    "#\n",
    "# print(\"🚀 RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "69cf251d74bd716e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:54:48.944620Z",
     "start_time": "2025-05-28T07:54:45.902419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ────────────────────────────────\n",
    "# 1) FastAPI & Required Imports\n",
    "# ────────────────────────────────\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 1a) Chat‐history persistence (messages only)\n",
    "# ────────────────────────────────\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "def save_history(conversation: List[Dict[str, str]]):\n",
    "    timeline = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                timeline = json.load(f) or []\n",
    "        except Exception:\n",
    "            timeline = []\n",
    "    timeline.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": conversation\n",
    "    })\n",
    "    tmp = HISTORY_FILE + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(timeline, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, HISTORY_FILE)\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 2) Metrics loaders & intents\n",
    "# ────────────────────────────────\n",
    "bleu_metric  = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    intents_data = json.load(f)[\"intents\"]\n",
    "\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 3) FastAPI + CORS\n",
    "# ────────────────────────────────\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 4) Pydantic Models\n",
    "# ────────────────────────────────\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 5) Utilities\n",
    "# ────────────────────────────────\n",
    "ARITH = re.compile(r'^[\\d\\s\\+\\-\\*\\/\\(\\)]+$')\n",
    "def is_arithmetic(q: str) -> bool:\n",
    "    return bool(ARITH.fullmatch(q.strip()))\n",
    "\n",
    "ALL_DATASETS = [\"gsm8k\",\"mbpp\",\"intents\"]\n",
    "\n",
    "def find_ground_truth(source: str, query: str) -> Optional[str]:\n",
    "    \"\"\"Safely look up the ground truth for `query` in the given dataset.\"\"\"\n",
    "    q = query.lower()\n",
    "    try:\n",
    "        if source == \"gsm8k\":\n",
    "            ds = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "            for it in ds:\n",
    "                question = it.get(\"question\", \"\")\n",
    "                if q in question.lower():\n",
    "                    return it.get(\"answer\")\n",
    "        elif source == \"mbpp\":\n",
    "            ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\", split=\"train\")\n",
    "            for it in ds:\n",
    "                text = it.get(\"text\", \"\")\n",
    "                if q in text.lower():\n",
    "                    return it.get(\"code\")\n",
    "        elif source == \"intents\":\n",
    "            for intent in intents_data:\n",
    "                for p in intent.get(\"patterns\", []):\n",
    "                    if q in p.lower():\n",
    "                        return intent.get(\"responses\", [None])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Ground-truth lookup error for {source}:\", e)\n",
    "    return None\n",
    "\n",
    "# ────────────────────────────────\n",
    "# 6) Main Endpoint – compute & print only non-null metrics\n",
    "# ────────────────────────────────\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_endpoint(req: ChatRequest):\n",
    "    messages: List[ChatMessage] = []\n",
    "\n",
    "    # A) Clean history of bad arithmetic\n",
    "    for m in req.history:\n",
    "        if m.role == \"assistant\" and is_arithmetic(m.content):\n",
    "            corr = python_tool.run(m.content)\n",
    "            if m.content.strip() != corr.strip():\n",
    "                messages.append(ChatMessage(\n",
    "                    role=\"system\",\n",
    "                    content=\"⚠️ Removed previous unsupported arithmetic answer\"\n",
    "                ))\n",
    "                continue\n",
    "        messages.append(m)\n",
    "\n",
    "    # B) Append new user turn\n",
    "    if not any(m.role == \"user\" and m.content == req.message for m in messages):\n",
    "        messages.append(ChatMessage(role=\"user\", content=req.message))\n",
    "\n",
    "    try:\n",
    "        # C) Math-only shortcut\n",
    "        if is_arithmetic(req.message):\n",
    "            answer = python_tool.run(req.message)\n",
    "            metrics_list = [{\n",
    "                \"dataset\": \"arithmetic\",\n",
    "                \"bleu\": 1.0,\n",
    "                \"rouge\": {\"recall\": 1.0}\n",
    "            }]\n",
    "            # print only non-null metrics\n",
    "            for m in metrics_list:\n",
    "                print(f\"bleu - {m['dataset']} = {m['bleu']}\")\n",
    "                for name, val in m[\"rouge\"].items():\n",
    "                    print(f\"{name} - {m['dataset']} = {val}\")\n",
    "\n",
    "            messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "            save_history([m.dict() for m in messages])\n",
    "            return ChatResponse(answer=answer, history=messages)\n",
    "\n",
    "        # D) Retrieval-augmented answer\n",
    "        result = qa_chain({\"query\": req.message})\n",
    "        answer = result[\"result\"]\n",
    "\n",
    "        # E) Compute BLEU+ROUGE for all datasets\n",
    "        metrics_list: List[Dict[str,Any]] = []\n",
    "        for ds in ALL_DATASETS:\n",
    "            b: Optional[float] = None\n",
    "            r: Optional[Dict[str,float]] = None\n",
    "            gt = find_ground_truth(ds, req.message)\n",
    "            if gt:\n",
    "                try:\n",
    "                    b = bleu_metric.compute(\n",
    "                        predictions=[answer],\n",
    "                        references=[[gt]]\n",
    "                    )[\"bleu\"]\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ BLEU error {ds}:\", e)\n",
    "                try:\n",
    "                    raw = rouge_metric.compute(\n",
    "                        predictions=[answer],\n",
    "                        references=[gt]\n",
    "                    )\n",
    "                    r = {}\n",
    "                    for nm, val in raw.items():\n",
    "                        if isinstance(val, dict):\n",
    "                            r[nm] = val.get(\"recall\", 0.0)\n",
    "                        elif isinstance(val, (float, int)):\n",
    "                            r[nm] = float(val)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ ROUGE error {ds}:\", e)\n",
    "            metrics_list.append({\"dataset\": ds, \"bleu\": b, \"rouge\": r})\n",
    "\n",
    "        # print only non-null metrics\n",
    "        for m in metrics_list:\n",
    "            if m[\"bleu\"] is not None:\n",
    "                print(f\"bleu - {m['dataset']} = {m['bleu']}\")\n",
    "            if m[\"rouge\"] is not None:\n",
    "                for name, val in m[\"rouge\"].items():\n",
    "                    if val is not None:\n",
    "                        print(f\"{name} - {m['dataset']} = {val}\")\n",
    "\n",
    "        # F) Toxicity / hallucination filters\n",
    "        toxic = {\"kill\",\"hate\",\"stupid\",\"dumb\",\"racist\",\"sexist\",\"violence\",\"bomb\",\"terror\",\"die\",\"suicide\"}\n",
    "        if any(w in answer.lower() for w in toxic):\n",
    "            answer = \"⚠️ Response blocked due to potentially inappropriate content.\"\n",
    "        else:\n",
    "            ctx = \" \".join(doc.page_content.lower() for doc in result[\"source_documents\"])\n",
    "            shared = [w for w in answer.lower().split() if w in ctx]\n",
    "            if len(shared) < 5:\n",
    "                answer = \"⚠️ I'm not confident this answer is grounded in context.\"\n",
    "\n",
    "    except Exception as exc:\n",
    "        raise HTTPException(status_code=500, detail=str(exc))\n",
    "\n",
    "    # G) Finalize and persist\n",
    "    messages.append(ChatMessage(role=\"assistant\", content=answer))\n",
    "    save_history([m.dict() for m in messages])\n",
    "    return ChatResponse(answer=answer, history=messages)\n",
    "\n"
   ],
   "id": "a495d7ab8bc83de",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:55:22.527406Z",
     "start_time": "2025-05-28T07:54:48.958420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ────────────────────────────────\n",
    "# 7) TEST CALL (in-notebook) to see prints under this cell\n",
    "# ────────────────────────────────\n",
    "# client = TestClient(app)\n",
    "# response = client.post(\n",
    "#     \"/rag_chat\",\n",
    "#     json={\n",
    "#         \"message\": \"What is computer architecture?\",\n",
    "#         \"history\": []\n",
    "#     }\n",
    "# )\n",
    "# print(\"Response JSON:\", response.json())\n"
   ],
   "id": "dfa0f3c5e2276a25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_7628\\546098977.py:154: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": req.message})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu - intents = 0.12494166449707533\n",
      "rouge1 - intents = 0.4903225806451613\n",
      "rouge2 - intents = 0.23529411764705882\n",
      "rougeL - intents = 0.3483870967741936\n",
      "rougeLsum - intents = 0.3354838709677419\n",
      "Response JSON: {'answer': '---\\n\\nProblem Type: Informatics.\\n\\nThe Von Neumann architecture is based on the following key principles:\\n\\n1. **Separation of Memory and Processing Units**: The memory stores both instructions and data, while the processing unit executes instructions.\\n2. **Fetch-Decode-Execute Cycle**: The processor fetches an instruction from memory, decodes it, and then executes it.\\n\\nComponents of a CPU in Von Neumann architecture include:\\n\\n* Control Unit\\n* Arithmetic Logic Unit (ALU)\\n* Registers\\n* Cache\\n\\nAnswer: The key principles of the Von Neumann architecture are the separation of memory and processing units and the fetch-decode-execute cycle.', 'history': [{'role': 'user', 'content': 'What is computer architecture?'}, {'role': 'assistant', 'content': '---\\n\\nProblem Type: Informatics.\\n\\nThe Von Neumann architecture is based on the following key principles:\\n\\n1. **Separation of Memory and Processing Units**: The memory stores both instructions and data, while the processing unit executes instructions.\\n2. **Fetch-Decode-Execute Cycle**: The processor fetches an instruction from memory, decodes it, and then executes it.\\n\\nComponents of a CPU in Von Neumann architecture include:\\n\\n* Control Unit\\n* Arithmetic Logic Unit (ALU)\\n* Registers\\n* Cache\\n\\nAnswer: The key principles of the Von Neumann architecture are the separation of memory and processing units and the fetch-decode-execute cycle.'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_7628\\546098977.py:210: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  save_history([m.dict() for m in messages])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:20:20.489612Z",
     "start_time": "2025-05-28T08:20:18.497767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ────────────────────────────────\n",
    "# Cell: LLM-Judge with real-time feedback\n",
    "# ────────────────────────────────\n",
    "import os\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Literal, Dict, Any, Optional\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1) Ensure API key is set (or replace with your key string)\n",
    "load_dotenv(dotenv_path=\"llm.env\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"Missing OPENAI_API_KEY environment variable. \"\n",
    "        \"Please create a .env file with your key.\"\n",
    "    )\n",
    "# 2) Build the judge prompt & chain\n",
    "judge_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert evaluator. Given a Question and an Answer, produce:\n",
    "Verdict: Correct or Incorrect\n",
    "Score: a number between 0.0 and 1.0\n",
    "Comments: concise feedback.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\")\n",
    "judge_llm = OpenAI(temperature=0, openai_api_key=api_key)\n",
    "judge_chain = LLMChain(llm=judge_llm, prompt=judge_prompt)\n",
    "\n",
    "# 3) (Re)define your FastAPI app and models as before\n",
    "app = FastAPI()\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "# (Include your is_arithmetic, find_ground_truth, metrics logic here…)\n",
    "\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_with_judge(req: ChatRequest):\n",
    "    # Call your original rag_chat logic (inline or imported)\n",
    "    # For brevity, assume rag_chat returns ChatResponse\n",
    "\n",
    "    resp: ChatResponse = await rag_chat_endpoint(req)\n",
    "\n",
    "    # 4) Run the judge immediately and print it\n",
    "    evaluation = judge_chain.run(question=req.message, answer=resp.answer)\n",
    "    print(\"\\n🔍 LLM Judge Evaluation:\\n\" + evaluation)\n",
    "\n",
    "    return resp\n",
    "\n",
    "# 5) Test in-notebook to see prints under this cell\n",
    "client = TestClient(app)\n",
    "response = client.post(\"/rag_chat\", json={\"message\":\"2+4\",\"history\":[]})\n",
    "print(\"Response JSON:\", response.json())\n"
   ],
   "id": "917270ec2f08ad8c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_20936\\65152343.py:39: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  judge_llm = OpenAI(temperature=0, openai_api_key=api_key)\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_20936\\65152343.py:40: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  judge_chain = LLMChain(llm=judge_llm, prompt=judge_prompt)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rag_chat_endpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 75\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# 5) Test in-notebook to see prints under this cell\u001B[39;00m\n\u001B[0;32m     74\u001B[0m client \u001B[38;5;241m=\u001B[39m TestClient(app)\n\u001B[1;32m---> 75\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/rag_chat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m2+4\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhistory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse JSON:\u001B[39m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mjson())\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\testclient.py:538\u001B[0m, in \u001B[0;36mTestClient.post\u001B[1;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[0m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[0;32m    523\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    524\u001B[0m     url: httpx\u001B[38;5;241m.\u001B[39m_types\u001B[38;5;241m.\u001B[39mURLTypes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    536\u001B[0m     extensions: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, typing\u001B[38;5;241m.\u001B[39mAny] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    537\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m httpx\u001B[38;5;241m.\u001B[39mResponse:\n\u001B[1;32m--> 538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    543\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m        \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\httpx\\_client.py:1144\u001B[0m, in \u001B[0;36mClient.post\u001B[1;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[0m\n\u001B[0;32m   1123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1124\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1125\u001B[0m     url: URL \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1137\u001B[0m     extensions: RequestExtensions \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1138\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Response:\n\u001B[0;32m   1139\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1140\u001B[0m \u001B[38;5;124;03m    Send a `POST` request.\u001B[39;00m\n\u001B[0;32m   1141\u001B[0m \n\u001B[0;32m   1142\u001B[0m \u001B[38;5;124;03m    **Parameters**: See `httpx.request`.\u001B[39;00m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1146\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1153\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\testclient.py:437\u001B[0m, in \u001B[0;36mTestClient.request\u001B[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[0m\n\u001B[0;32m    431\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    432\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou should not use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m argument with the TestClient. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    433\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/encode/starlette/issues/1108 for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    434\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m    435\u001B[0m     )\n\u001B[0;32m    436\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_url(url)\n\u001B[1;32m--> 437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    446\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    447\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    448\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    449\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    450\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    451\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\httpx\\_client.py:825\u001B[0m, in \u001B[0;36mClient.request\u001B[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[0m\n\u001B[0;32m    810\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    812\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_request(\n\u001B[0;32m    813\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    814\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    823\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mextensions,\n\u001B[0;32m    824\u001B[0m )\n\u001B[1;32m--> 825\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\httpx\\_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[0;32m    910\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_timeout(request)\n\u001B[0;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[1;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    921\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\httpx\\_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[1;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[0;32m    939\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[0;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    948\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\httpx\\_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[1;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    977\u001B[0m     hook(request)\n\u001B[1;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    981\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\httpx\\_client.py:1014\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m   1009\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1010\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1011\u001B[0m     )\n\u001B[0;32m   1013\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[1;32m-> 1014\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n\u001B[0;32m   1018\u001B[0m response\u001B[38;5;241m.\u001B[39mrequest \u001B[38;5;241m=\u001B[39m request\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\testclient.py:340\u001B[0m, in \u001B[0;36m_TestClientTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraise_server_exceptions:\n\u001B[1;32m--> 340\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraise_server_exceptions:\n\u001B[0;32m    343\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m response_started, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTestClient did not receive any response.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\testclient.py:337\u001B[0m, in \u001B[0;36m_TestClientTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mportal_factory() \u001B[38;5;28;01mas\u001B[39;00m portal:\n\u001B[0;32m    336\u001B[0m         response_complete \u001B[38;5;241m=\u001B[39m portal\u001B[38;5;241m.\u001B[39mcall(anyio\u001B[38;5;241m.\u001B[39mEvent)\n\u001B[1;32m--> 337\u001B[0m         \u001B[43mportal\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscope\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreceive\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msend\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraise_server_exceptions:\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\anyio\\from_thread.py:290\u001B[0m, in \u001B[0;36mBlockingPortal.call\u001B[1;34m(self, func, *args)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcall\u001B[39m(\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    277\u001B[0m     func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] \u001B[38;5;241m|\u001B[39m T_Retval],\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;241m*\u001B[39margs: Unpack[PosArgsT],\n\u001B[0;32m    279\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T_Retval:\n\u001B[0;32m    280\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;124;03m    Call the given function in the event loop thread.\u001B[39;00m\n\u001B[0;32m    282\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    288\u001B[0m \n\u001B[0;32m    289\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 290\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(T_Retval, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_task_soon\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:458\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 403\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\anyio\\from_thread.py:221\u001B[0m, in \u001B[0;36mBlockingPortal._call_func\u001B[1;34m(self, func, args, kwargs, future)\u001B[0m\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    219\u001B[0m             future\u001B[38;5;241m.\u001B[39madd_done_callback(callback)\n\u001B[1;32m--> 221\u001B[0m         retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m retval_or_awaitable\n\u001B[0;32m    222\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    223\u001B[0m     retval \u001B[38;5;241m=\u001B[39m retval_or_awaitable\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\fastapi\\applications.py:1054\u001B[0m, in \u001B[0;36mFastAPI.__call__\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_path:\n\u001B[0;32m   1053\u001B[0m     scope[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroot_path\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_path\n\u001B[1;32m-> 1054\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(scope, receive, send)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\applications.py:112\u001B[0m, in \u001B[0;36mStarlette.__call__\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmiddleware_stack \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmiddleware_stack \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_middleware_stack()\n\u001B[1;32m--> 112\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmiddleware_stack(scope, receive, send)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\middleware\\errors.py:187\u001B[0m, in \u001B[0;36mServerErrorMiddleware.__call__\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m response(scope, receive, send)\n\u001B[0;32m    184\u001B[0m \u001B[38;5;66;03m# We always continue to raise the exception.\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;66;03m# This allows servers to log the error, or allows test clients\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;66;03m# to optionally raise the error within the test case.\u001B[39;00m\n\u001B[1;32m--> 187\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\middleware\\errors.py:165\u001B[0m, in \u001B[0;36mServerErrorMiddleware.__call__\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m send(message)\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 165\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapp(scope, receive, _send)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    167\u001B[0m     request \u001B[38;5;241m=\u001B[39m Request(scope)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\middleware\\cors.py:85\u001B[0m, in \u001B[0;36mCORSMiddleware.__call__\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m     82\u001B[0m origin \u001B[38;5;241m=\u001B[39m headers\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morigin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m origin \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 85\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapp(scope, receive, send)\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPTIONS\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccess-control-request-method\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m headers:\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\middleware\\exceptions.py:62\u001B[0m, in \u001B[0;36mExceptionMiddleware.__call__\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     conn \u001B[38;5;241m=\u001B[39m WebSocket(scope, receive, send)\n\u001B[1;32m---> 62\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m wrap_app_handling_exceptions(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapp, conn)(scope, receive, send)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\_exception_handler.py:53\u001B[0m, in \u001B[0;36mwrap_app_handling_exceptions.<locals>.wrapped_app\u001B[1;34m(scope, receive, send)\u001B[0m\n\u001B[0;32m     50\u001B[0m     handler \u001B[38;5;241m=\u001B[39m _lookup_exception_handler(exception_handlers, exc)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response_started:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCaught handled exception, but response already started.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\_exception_handler.py:42\u001B[0m, in \u001B[0;36mwrap_app_handling_exceptions.<locals>.wrapped_app\u001B[1;34m(scope, receive, send)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m send(message)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m app(scope, receive, sender)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     44\u001B[0m     handler \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\routing.py:714\u001B[0m, in \u001B[0;36mRouter.__call__\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m    710\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, scope: Scope, receive: Receive, send: Send) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    711\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    712\u001B[0m \u001B[38;5;124;03m    The main entry point to the Router class.\u001B[39;00m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 714\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmiddleware_stack(scope, receive, send)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\routing.py:734\u001B[0m, in \u001B[0;36mRouter.app\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m    732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m match \u001B[38;5;241m==\u001B[39m Match\u001B[38;5;241m.\u001B[39mFULL:\n\u001B[0;32m    733\u001B[0m     scope\u001B[38;5;241m.\u001B[39mupdate(child_scope)\n\u001B[1;32m--> 734\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m route\u001B[38;5;241m.\u001B[39mhandle(scope, receive, send)\n\u001B[0;32m    735\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m match \u001B[38;5;241m==\u001B[39m Match\u001B[38;5;241m.\u001B[39mPARTIAL \u001B[38;5;129;01mand\u001B[39;00m partial \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\routing.py:288\u001B[0m, in \u001B[0;36mRoute.handle\u001B[1;34m(self, scope, receive, send)\u001B[0m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m response(scope, receive, send)\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 288\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapp(scope, receive, send)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\routing.py:76\u001B[0m, in \u001B[0;36mrequest_response.<locals>.app\u001B[1;34m(scope, receive, send)\u001B[0m\n\u001B[0;32m     73\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m f(request)\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m response(scope, receive, send)\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\_exception_handler.py:53\u001B[0m, in \u001B[0;36mwrap_app_handling_exceptions.<locals>.wrapped_app\u001B[1;34m(scope, receive, send)\u001B[0m\n\u001B[0;32m     50\u001B[0m     handler \u001B[38;5;241m=\u001B[39m _lookup_exception_handler(exception_handlers, exc)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response_started:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCaught handled exception, but response already started.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\_exception_handler.py:42\u001B[0m, in \u001B[0;36mwrap_app_handling_exceptions.<locals>.wrapped_app\u001B[1;34m(scope, receive, send)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m send(message)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m app(scope, receive, sender)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     44\u001B[0m     handler \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\starlette\\routing.py:73\u001B[0m, in \u001B[0;36mrequest_response.<locals>.app.<locals>.app\u001B[1;34m(scope, receive, send)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mapp\u001B[39m(scope: Scope, receive: Receive, send: Send) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 73\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m f(request)\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m response(scope, receive, send)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\fastapi\\routing.py:301\u001B[0m, in \u001B[0;36mget_request_handler.<locals>.app\u001B[1;34m(request)\u001B[0m\n\u001B[0;32m    299\u001B[0m errors \u001B[38;5;241m=\u001B[39m solved_result\u001B[38;5;241m.\u001B[39merrors\n\u001B[0;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m errors:\n\u001B[1;32m--> 301\u001B[0m     raw_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m run_endpoint_function(\n\u001B[0;32m    302\u001B[0m         dependant\u001B[38;5;241m=\u001B[39mdependant,\n\u001B[0;32m    303\u001B[0m         values\u001B[38;5;241m=\u001B[39msolved_result\u001B[38;5;241m.\u001B[39mvalues,\n\u001B[0;32m    304\u001B[0m         is_coroutine\u001B[38;5;241m=\u001B[39mis_coroutine,\n\u001B[0;32m    305\u001B[0m     )\n\u001B[0;32m    306\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(raw_response, Response):\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m raw_response\u001B[38;5;241m.\u001B[39mbackground \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\fastapi\\routing.py:212\u001B[0m, in \u001B[0;36mrun_endpoint_function\u001B[1;34m(dependant, values, is_coroutine)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m dependant\u001B[38;5;241m.\u001B[39mcall \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdependant.call must be a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_coroutine:\n\u001B[1;32m--> 212\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m dependant\u001B[38;5;241m.\u001B[39mcall(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mvalues)\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m run_in_threadpool(dependant\u001B[38;5;241m.\u001B[39mcall, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mvalues)\n",
      "Cell \u001B[1;32mIn[7], line 65\u001B[0m, in \u001B[0;36mrag_chat_with_judge\u001B[1;34m(req)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;129m@app\u001B[39m\u001B[38;5;241m.\u001B[39mpost(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/rag_chat\u001B[39m\u001B[38;5;124m\"\u001B[39m, response_model\u001B[38;5;241m=\u001B[39mChatResponse)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrag_chat_with_judge\u001B[39m(req: ChatRequest):\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;66;03m# Call your original rag_chat logic (inline or imported)\u001B[39;00m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;66;03m# For brevity, assume rag_chat returns ChatResponse\u001B[39;00m\n\u001B[1;32m---> 65\u001B[0m     resp: ChatResponse \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[43mrag_chat_endpoint\u001B[49m(req)\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# 4) Run the judge immediately and print it\u001B[39;00m\n\u001B[0;32m     68\u001B[0m     evaluation \u001B[38;5;241m=\u001B[39m judge_chain\u001B[38;5;241m.\u001B[39mrun(question\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39mmessage, answer\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39manswer)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'rag_chat_endpoint' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:55:22.591326Z",
     "start_time": "2025-05-28T07:55:22.544344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Notebook cell: run the RAG FastAPI server in the background\n",
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        reload=False,        # disable auto-reload in notebooks\n",
    "    )\n",
    "\n",
    "# Start Uvicorn in a daemon thread so the notebook stays interactive\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"🚀 RAG server is now running at http://localhost:8000\")\n"
   ],
   "id": "99b012da35df65fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 RAG server is now running at http://localhost:8000\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:05:33.170447Z",
     "start_time": "2025-05-28T08:05:31.782675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ────────────────────────────────\n",
    "# Cell: LLM-Judge with real-time feedback\n",
    "# ────────────────────────────────\n",
    "import os\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Literal, Dict, Any, Optional\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1) Ensure API key is set (or replace with your key string)\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"sk-YOUR_KEY_HERE\")\n",
    "\n",
    "# 2) Build the judge prompt & chain\n",
    "judge_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert evaluator. Given a Question and an Answer, produce:\n",
    "Verdict: Correct or Incorrect\n",
    "Score: a number between 0.0 and 1.0\n",
    "Comments: concise feedback.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\")\n",
    "judge_llm = OpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "judge_chain = LLMChain(llm=judge_llm, prompt=judge_prompt)\n",
    "\n",
    "# 3) (Re)define your FastAPI app and models as before\n",
    "app = FastAPI()\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "# (Include your is_arithmetic, find_ground_truth, metrics logic here…)\n",
    "\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_with_judge(req: ChatRequest):\n",
    "    # Call your original rag_chat logic (inline or imported)\n",
    "    # For brevity, assume rag_chat returns ChatResponse\n",
    "    resp: ChatResponse = await rag_chat_endpoint(req)\n",
    "\n",
    "    # 4) Run the judge immediately and print it\n",
    "    evaluation = judge_chain.run(question=req.message, answer=resp.answer)\n",
    "    print(\"\\n🔍 LLM Judge Evaluation:\\n\" + evaluation)\n",
    "\n",
    "    return resp\n",
    "\n",
    "# 5) Test in-notebook to see prints under this cell\n",
    "client = TestClient(app)\n",
    "response = client.post(\"/rag_chat\", json={\"message\":\"2+4\",\"history\":[]})\n",
    "print(\"Response JSON:\", response.json())\n"
   ],
   "id": "e18409e709dae3d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_7628\\3297525713.py:30: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  judge_llm = OpenAI(temperature=0)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0, 'model...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 30\u001B[0m\n\u001B[0;32m      9\u001B[0m judge_template \u001B[38;5;241m=\u001B[39m PromptTemplate\u001B[38;5;241m.\u001B[39mfrom_template(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124mYou are an expert evaluator of student answers.  Given a Question and an Answer, produce:\u001B[39m\n\u001B[0;32m     11\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;132;01m{answer}\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# 2) Instantiate the LLMChain (uses your default OpenAI key / model)\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m judge_llm \u001B[38;5;241m=\u001B[39m \u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m judge_chain \u001B[38;5;241m=\u001B[39m LLMChain(llm\u001B[38;5;241m=\u001B[39mjudge_llm, prompt\u001B[38;5;241m=\u001B[39mjudge_template)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# 3) Monkey-patch your existing endpoint to call the judge\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m#    (import the endpoint and models from the cell above)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:224\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    222\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    223\u001B[0m     emit_warning()\n\u001B[1;32m--> 224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    129\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: D419\u001B[39;00m\n\u001B[1;32m--> 130\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Facultate\\Sem4\\MachineLearning\\TeamProject\\Clona\\Machine-Learning_Project\\.venv\\lib\\site-packages\\pydantic\\main.py:253\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[1;34m(self, **data)\u001B[0m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001B[39;00m\n\u001B[0;32m    252\u001B[0m __tracebackhide__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 253\u001B[0m validated_self \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_instance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m validated_self:\n\u001B[0;32m    255\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    256\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA custom validator is returning a value other than `self`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    257\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReturning anything other than `self` from a top level model validator isn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt supported when validating via `__init__`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    258\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    259\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    260\u001B[0m     )\n",
      "\u001B[1;31mValidationError\u001B[0m: 1 validation error for OpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0, 'model...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
