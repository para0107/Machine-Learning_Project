{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring the Capabilities of LLM Models\n",
    "\n",
    "In this notebook, I aim to evaluate and compare the capabilities of two large language models (LLMs):\n",
    "\n",
    "1. **Codestral22B**\n",
    "   A state-of-the-art model designed for advanced code generation and natural language understanding tasks.\n",
    "\n",
    "2. **Llama 3.1-8B**\n",
    "   A highly efficient and compact model optimized for general-purpose language tasks with an 8-billion parameter architecture.\n",
    "\n",
    "The goal is to analyze their performance across various tasks, including but not limited to:\n",
    "\n",
    "- Code generation and completion\n",
    "- Natural language understanding\n",
    "- Contextual reasoning\n",
    "- Problem-solving capabilities\n",
    "\n",
    "This comparison will help identify the strengths and weaknesses of each model and provide insights into their practical applications."
   ],
   "id": "f51589c43481d44c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI-Powered Programming Tutor with RAG\n",
    "\n",
    "This project focuses on building an AI-powered programming tutor designed to assist students in understanding code and solving problems. The tutor leverages **Retrieval-Augmented Generation (RAG)** to provide accurate and personalized explanations grounded in real university materials, such as:\n",
    "\n",
    "- Past assignments\n",
    "- Lecture notes\n",
    "- Tutorials`\n",
    "\n",
    "The system integrates two large language models (LLMs), **Codestral22B** and **Llama 3.1-8B**, to evaluate their performance in generating solutions and explanations for programming-related queries. The goal is to determine which model provides better support for students in a university setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Personalized Explanations**: Tailored responses based on retrieved university materials.\n",
    "- **Code Understanding**: Helps students debug and understand code snippets.\n",
    "- **Problem Solving**: Provides step-by-step solutions to programming problems.\n",
    "- **Model Comparison**: Evaluates the performance of Codestral22B and Llama 3.1-8B.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "7d228540125879a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Basically, I will try to implement and test Codestral22B and Llama 3.1-8B",
   "id": "66487be05849c784"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:05.908301Z",
     "start_time": "2025-05-24T19:30:05.898293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datetime import datetime\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# from pprint import pprint\n",
    "# from os.path import exists\n",
    "#\n",
    "# import requests\n",
    "#\n",
    "# #API endpoint exposed in Lm studio\n",
    "# url = \"http://localhost:1234/v1/chat/completions\"\n",
    "#\n",
    "# #model ID\n",
    "# model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "#\n",
    "# headers={\n",
    "#     \"Content-Type\" : \"application/json\",\n",
    "#     \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "# }\n",
    "#\n",
    "# # messages: [ #keep conversation history\n",
    "# #                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "# #                  \"content\":user_input}\n",
    "# #             ]\n",
    "#\n",
    "# #Keep the message history\n",
    "#\n",
    "# #History file path, to keep conversation\n",
    "# history_file = \"chat-history.json\"\n",
    "# def save_history(messages):\n",
    "#     # Load existing history if the file exists\n",
    "#     if exists(history_file):\n",
    "#         with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             full_history = json.load(f)\n",
    "#             if isinstance(full_history, list):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 full_history = [full_history]\n",
    "#     else:\n",
    "#         full_history = []\n",
    "#\n",
    "#     # Add this session with timestamp\n",
    "#     full_history.append({\n",
    "#         \"timestamp\": datetime.now().isoformat(),\n",
    "#         \"conversation\": messages\n",
    "#     })\n",
    "#\n",
    "#     # Save the full conversation list\n",
    "#     with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "#\n",
    "#\n",
    "# #Prompt loop\n",
    "# def chat():\n",
    "#     print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "#     messages = [\n",
    "#     {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "#      \"content\": \"You are a helpful programming tutor.\"}\n",
    "# ] #Messages reset each time\n",
    "#\n",
    "#     while True:\n",
    "#         user_input = input(\" You: \")\n",
    "#         if user_input.lower() == \"exit\":\n",
    "#             #save chat history\n",
    "#             save_history(messages)\n",
    "#             print(f\"\\n Conversation saved to {history_file}\")\n",
    "#             break\n",
    "#\n",
    "#         # Add user message\n",
    "#         messages.append({\"role\": \"user\",\n",
    "#                          \"content\": user_input})\n",
    "#\n",
    "#         payload = {\n",
    "#             \"model\": model_id,#id of model\n",
    "#             \"messages\": messages,#chat history to preserve context\n",
    "#             \"temperature\": 0.7 #control creativiy\n",
    "#         }\n",
    "#         print(\"Your question is: \")\n",
    "#         print(user_input)\n",
    "#         print(\"\\n\")\n",
    "#\n",
    "#         try:#send request to lm api\n",
    "#             response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "#\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 answer = data['choices'][0]['message']['content'].strip()\n",
    "#\n",
    "#                 # Add assistant message\n",
    "#                 messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "#\n",
    "#                 print(\"\\n LLaMA:\", flush=True)\n",
    "#                 print(answer, flush=True)\n",
    "#                 print(\"-\" * 60 + \"\\n\")\n",
    "#\n",
    "#             else:\n",
    "#                 print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "#\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(\" Connection error:\", e)\n",
    "#             break\n"
   ],
   "id": "ca6449393a982411",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:05.923837Z",
     "start_time": "2025-05-24T19:30:05.917831Z"
    }
   },
   "cell_type": "code",
   "source": "# chat()",
   "id": "b60581168b183876",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Load the Datasets\n",
    "## 1.  OpenMathInstruct-1 (from Hugging Face)\n",
    "- This dataset contains 1.8 million math problem-solution pairs, making it ideal for enhancing mathematical reasoning in LLMs."
   ],
   "id": "f2e29ce7c06eda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:05.939134Z",
     "start_time": "2025-05-24T19:30:05.927830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datasets import load_dataset\n",
    "# from IPython import get_ipython\n",
    "# from IPython.display import display\n",
    "#\n",
    "# #Load training split\n",
    "# dataset = load_dataset(\"nvidia/OpenMathInstruct-1\", split=\"train\")\n",
    "#\n",
    "# first_element = next(iter(dataset))\n",
    "#\n",
    "# print(first_element)\n",
    "\n",
    "#Give up on it, waaaay to much data in dataset"
   ],
   "id": "12f7c6faccca5cb3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Small & Clean Math Datasets\n",
    "## 1. GSM8K\n",
    "- Size: ~8.5K problems\n",
    "\n",
    "- Focus: Grade school math word problems\n",
    "\n",
    "- Good for: step-by-step reasoning, small LLM finetuning"
   ],
   "id": "954a019fae299897"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:21.413562Z",
     "start_time": "2025-05-24T19:30:06.674586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "1ae3182c572fe879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Computer Science Theory QA Dataset (from Kaggle)\n",
    "- This dataset offers a comprehensive collection of theoretical computer science questions, suitable for training chatbots and QA systems."
   ],
   "id": "b5932817b8f7a049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:21.506922Z",
     "start_time": "2025-05-24T19:30:21.477683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "9ff53e22afa20485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:28.929493Z",
     "start_time": "2025-05-24T19:30:21.515345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "d0b3e31e6e85f56d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Install & Import Dependencies\n",
    "We’ll need:\n",
    "\n",
    "-  Transformers & Datasets\n",
    "\n",
    "-  LangChain & an embedding backend (here HuggingFaceEmbeddings)\n",
    "\n",
    "-  FAISS for the vector index\n",
    "\n",
    "-  Accelerate + PEFT if you plan to fine-tune your generator"
   ],
   "id": "d218ed8cd6b830ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:28.960850Z",
     "start_time": "2025-05-24T19:30:28.946374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install \\\n",
    "#  transformers datasets faiss-cpu \\\n",
    "#  langchain sentence-transformers \\\n",
    "#  accelerate peft evaluate"
   ],
   "id": "39aff69f1b83cd51",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e79cd82c56fd576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:28.992849Z",
     "start_time": "2025-05-24T19:30:28.978844Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install --upgrade peft",
   "id": "7fcfb287c01d7c07",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:56.221641Z",
     "start_time": "2025-05-24T19:30:29.010330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "15fef1ad9d22b2db",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Configuration\n",
    "Centralize all paths, model names, and hyperparameters."
   ],
   "id": "7d4f39dce9ad0059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:30:56.251651Z",
     "start_time": "2025-05-24T19:30:56.238637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "18f205f852935553",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Load & Merge Datasets\n",
    "Load your local Q&A (if any), plus MBPP (test split) and GSM8K train. Then standardize to a single list of “documents” with id and text."
   ],
   "id": "4f4598c6ff4a49d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:31:09.490328Z",
     "start_time": "2025-05-24T19:30:56.269124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "4d2f48cea8c5a6d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3638738eae5e4d25bae9f6f8f01227f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fd59b193264483ab86ca58d52b47d07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7889\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Build & Chunk the Retrieval Corpus\n",
    "We’ll treat each training example as a “document” by concatenating input_text + target_text and splitting into overlapping chunks."
   ],
   "id": "69ce26036be862fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:31:09.758003Z",
     "start_time": "2025-05-24T19:31:09.508330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "ca4d714962ad40d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8153 chunks from 7889 documents.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Embed & Build a FAISS Vector Index\n",
    "Use a Sentence-Transformer to embed each chunk, then store in FAISS for fast nearest-neighbour lookup."
   ],
   "id": "c7b19aa8fa25874a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:33:33.987160Z",
     "start_time": "2025-05-24T19:31:09.775945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "37ae62208b9e7c92",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_19416\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Wire Up a LangChain RetrievalQA Pipeline\n",
    "We now plug your FAISS store and the Meta-Llama-3.1-8b generator into a single retrieval-augmented chain."
   ],
   "id": "9adb9f015e48c8e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:33:34.033147Z",
     "start_time": "2025-05-24T19:33:34.019076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "45baecdb609ecdcb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:33:36.097781Z",
     "start_time": "2025-05-24T19:33:34.038082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "3b992f350d616aa6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:34:49.591783Z",
     "start_time": "2025-05-24T19:33:36.115342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "query = \"How would you implement binary search in Python?\"\n",
    "result = qa_chain(query)\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "\n"
   ],
   "id": "526fbcb07a89502f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_19416\\243406714.py:29: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_19416\\243406714.py:47: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Binary Search is a fast searching algorithm with an average and worst-case time complexity of O(log n). Here's how you can implement it in Python:\n",
      "\n",
      "```python\n",
      "def binary_search(arr, target):\n",
      "    \"\"\"\n",
      "    Searches for the target element in the given sorted array using Binary Search.\n",
      "\n",
      "    Args:\n",
      "        arr (list): A sorted list of elements.\n",
      "        target: The element to be searched.\n",
      "\n",
      "    Returns:\n",
      "        int: The index of the target element if found. -1 otherwise.\n",
      "    \"\"\"\n",
      "\n",
      "    # Initialize the low and high pointers\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "\n",
      "    while low <= high:\n",
      "        # Calculate the mid index\n",
      "        mid = (low + high) // 2\n",
      "\n",
      "        # If the target is found at the mid index, return it\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "\n",
      "        # If the target is less than the mid element, move the high pointer to mid - 1\n",
      "        elif arr[mid] > target:\n",
      "            high = mid - 1\n",
      "\n",
      "        # If the target is greater than the mid element, move the low pointer to mid + 1\n",
      "        else:\n",
      "            low = mid + 1\n",
      "\n",
      "    # If the target is not found, return -1\n",
      "    return -1\n",
      "\n",
      "\n",
      "# Example usage\n",
      "arr = [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]\n",
      "target = 23\n",
      "\n",
      "index = binary_search(arr, target)\n",
      "\n",
      "if index != -1:\n",
      "    print(f\"Target {target} found at index {index}.\")\n",
      "else:\n",
      "    print(f\"Target {target} not found in the array.\")\n",
      "```\n",
      "\n",
      "In this implementation:\n",
      "\n",
      "*   We define a function `binary_search` that takes two arguments: `arr`, which is the sorted list to be searched, and `target`, which is the element we're trying to find.\n",
      "*   The function initializes the `low` pointer at index 0 (the first element) and the `high` pointer at the last index of the array (`len(arr) - 1`).\n",
      "*   It enters a while loop that continues as long as `low <= high`.\n",
      "*   Inside the loop, it calculates the mid index using `(low + high) // 2`.\n",
      "*   It then checks if the element at the mid index is equal to the target. If so, it returns the mid index.\n",
      "*   If the mid\n",
      "\n",
      "Sources:\n",
      "- doc-161\n",
      "- doc-30\n",
      "- doc-160\n",
      "- doc-155\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:34:49.638703Z",
     "start_time": "2025-05-24T19:34:49.623707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from os.path import exists\n",
    "\n",
    "import requests\n",
    "\n",
    "#API endpoint exposed in Lm studio\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "#model ID\n",
    "model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "\n",
    "headers={\n",
    "    \"Content-Type\" : \"application/json\",\n",
    "    \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "}\n",
    "\n",
    "# messages: [ #keep conversation history\n",
    "#                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "#                  \"content\":user_input}\n",
    "#             ]\n",
    "\n",
    "#Keep the message history\n",
    "\n",
    "#History file path, to keep conversation\n",
    "history_file = \"chat-history.json\"\n",
    "def save_history(messages):\n",
    "    # Load existing history if the file exists\n",
    "    if exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_history = json.load(f)\n",
    "            if isinstance(full_history, list):\n",
    "                pass\n",
    "            else:\n",
    "                full_history = [full_history]\n",
    "    else:\n",
    "        full_history = []\n",
    "\n",
    "    # Add this session with timestamp\n",
    "    full_history.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": messages\n",
    "    })\n",
    "\n",
    "    # Save the full conversation list\n",
    "    with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "#Prompt loop\n",
    "def chat():\n",
    "    print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "     \"content\": \"You are a helpful programming tutor.\"}\n",
    "] #Messages reset each time\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            #save chat history\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            break\n",
    "\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\",\n",
    "                         \"content\": user_input})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_id,#id of model\n",
    "            \"messages\": messages,#chat history to preserve context\n",
    "            \"temperature\": 0.7 #control creativiy\n",
    "        }\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:#send request to lm api\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=1000)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "                # Add assistant message\n",
    "                messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "                print(\"\\n LLaMA:\", flush=True)\n",
    "                print(answer, flush=True)\n",
    "                print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\" Connection error:\", e)\n",
    "            break\n"
   ],
   "id": "34443ddc8c9e41c9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:34:53.763822Z",
     "start_time": "2025-05-24T19:34:49.645212Z"
    }
   },
   "cell_type": "code",
   "source": "chat()",
   "id": "e519a2c24c2734ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Talk to LLaMA 3.1 (type 'exit' to quit)\n",
      "\n",
      "\n",
      " Conversation saved to chat-history.json\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:34:53.794832Z",
     "start_time": "2025-05-24T19:34:53.781822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Search for Pythagorean theorem related questions in GSM8K\n",
    "# pythagorean_questions = []\n",
    "#\n",
    "# # Load the dataset if not already loaded\n",
    "# gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "# #\n",
    "# # Search for relevant keywords\n",
    "# keywords = [\"pythagora\", \"pythagorean\", \"right triangle\", \"hypotenuse\", \"a^2 + b^2\"]\n",
    "#\n",
    "# for i, example in enumerate(gsm):\n",
    "#     question = example[\"question\"].lower()\n",
    "#     for keyword in keywords:\n",
    "#         if keyword.lower() in question:\n",
    "#             pythagorean_questions.append({\n",
    "#                 \"index\": i,\n",
    "#                 \"question\": example[\"question\"],\n",
    "#                 \"answer\": example[\"answer\"]\n",
    "#             })\n",
    "#             break\n",
    "#\n",
    "# # Print the number of matching questions\n",
    "# print(f\"Found {len(pythagorean_questions)} questions related to the Pythagorean theorem\")\n",
    "#\n",
    "# # Display the first few matches if any exist\n",
    "# for i, q in enumerate(pythagorean_questions[:3]):\n",
    "#     print(f\"\\nQuestion {i+1}:\")\n",
    "#     print(q[\"question\"])\n",
    "#     print(\"\\nAnswer:\")\n",
    "#     print(q[\"answer\"])"
   ],
   "id": "3ae90eb18febee3e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:34:53.825829Z",
     "start_time": "2025-05-24T19:34:53.811823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "#\n",
    "# # Load the FAISS index file\n",
    "# index = faiss.read_index(\"results/faiss_index/index.faiss\")\n",
    "#\n",
    "# # Print basic info\n",
    "# print(\"Index type:\", type(index).__name__)\n",
    "# print(\"Dimension:\", index.d)\n",
    "# # print(\"Is trained:\", index.is_trained)\n",
    "# # print(\"Total vectors stored:\", index.ntotal)\n",
    "#\n",
    "# # Example: retrieve all stored vectors (if they fit in memory)\n",
    "# try:\n",
    "#     xb = index.reconstruct_n(0, index.ntotal)  # returns all vectors\n",
    "#     print(\"Sample vector (first one):\", xb[0])\n",
    "# except Exception as e:\n",
    "#     print(\"Cannot reconstruct vectors:\", e)\n"
   ],
   "id": "e73fbafadbe7a90c",
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
