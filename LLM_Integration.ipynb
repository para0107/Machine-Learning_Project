{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring the Capabilities of LLM Models\n",
    "\n",
    "In this notebook, I aim to evaluate and compare the capabilities of two large language models (LLMs):\n",
    "\n",
    "1. **Codestral22B**\n",
    "   A state-of-the-art model designed for advanced code generation and natural language understanding tasks.\n",
    "\n",
    "2. **Llama 3.1-8B**\n",
    "   A highly efficient and compact model optimized for general-purpose language tasks with an 8-billion parameter architecture.\n",
    "\n",
    "The goal is to analyze their performance across various tasks, including but not limited to:\n",
    "\n",
    "- Code generation and completion\n",
    "- Natural language understanding\n",
    "- Contextual reasoning\n",
    "- Problem-solving capabilities\n",
    "\n",
    "This comparison will help identify the strengths and weaknesses of each model and provide insights into their practical applications."
   ],
   "id": "f51589c43481d44c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI-Powered Programming Tutor with RAG\n",
    "\n",
    "This project focuses on building an AI-powered programming tutor designed to assist students in understanding code and solving problems. The tutor leverages **Retrieval-Augmented Generation (RAG)** to provide accurate and personalized explanations grounded in real university materials, such as:\n",
    "\n",
    "- Past assignments\n",
    "- Lecture notes\n",
    "- Tutorials`\n",
    "\n",
    "The system integrates two large language models (LLMs), **Codestral22B** and **Llama 3.1-8B**, to evaluate their performance in generating solutions and explanations for programming-related queries. The goal is to determine which model provides better support for students in a university setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Personalized Explanations**: Tailored responses based on retrieved university materials.\n",
    "- **Code Understanding**: Helps students debug and understand code snippets.\n",
    "- **Problem Solving**: Provides step-by-step solutions to programming problems.\n",
    "- **Model Comparison**: Evaluates the performance of Codestral22B and Llama 3.1-8B.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "7d228540125879a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Basically, I will try to implement and test Codestral22B and Llama 3.1-8B",
   "id": "66487be05849c784"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:10.009458Z",
     "start_time": "2025-05-24T12:29:09.992455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datetime import datetime\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# from pprint import pprint\n",
    "# from os.path import exists\n",
    "#\n",
    "# import requests\n",
    "#\n",
    "# #API endpoint exposed in Lm studio\n",
    "# url = \"http://localhost:1234/v1/chat/completions\"\n",
    "#\n",
    "# #model ID\n",
    "# model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "#\n",
    "# headers={\n",
    "#     \"Content-Type\" : \"application/json\",\n",
    "#     \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "# }\n",
    "#\n",
    "# # messages: [ #keep conversation history\n",
    "# #                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "# #                  \"content\":user_input}\n",
    "# #             ]\n",
    "#\n",
    "# #Keep the message history\n",
    "#\n",
    "# #History file path, to keep conversation\n",
    "# history_file = \"chat-history.json\"\n",
    "# def save_history(messages):\n",
    "#     # Load existing history if the file exists\n",
    "#     if exists(history_file):\n",
    "#         with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             full_history = json.load(f)\n",
    "#             if isinstance(full_history, list):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 full_history = [full_history]\n",
    "#     else:\n",
    "#         full_history = []\n",
    "#\n",
    "#     # Add this session with timestamp\n",
    "#     full_history.append({\n",
    "#         \"timestamp\": datetime.now().isoformat(),\n",
    "#         \"conversation\": messages\n",
    "#     })\n",
    "#\n",
    "#     # Save the full conversation list\n",
    "#     with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "#\n",
    "#\n",
    "# #Prompt loop\n",
    "# def chat():\n",
    "#     print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "#     messages = [\n",
    "#     {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "#      \"content\": \"You are a helpful programming tutor.\"}\n",
    "# ] #Messages reset each time\n",
    "#\n",
    "#     while True:\n",
    "#         user_input = input(\" You: \")\n",
    "#         if user_input.lower() == \"exit\":\n",
    "#             #save chat history\n",
    "#             save_history(messages)\n",
    "#             print(f\"\\n Conversation saved to {history_file}\")\n",
    "#             break\n",
    "#\n",
    "#         # Add user message\n",
    "#         messages.append({\"role\": \"user\",\n",
    "#                          \"content\": user_input})\n",
    "#\n",
    "#         payload = {\n",
    "#             \"model\": model_id,#id of model\n",
    "#             \"messages\": messages,#chat history to preserve context\n",
    "#             \"temperature\": 0.7 #control creativiy\n",
    "#         }\n",
    "#         print(\"Your question is: \")\n",
    "#         print(user_input)\n",
    "#         print(\"\\n\")\n",
    "#\n",
    "#         try:#send request to lm api\n",
    "#             response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "#\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 answer = data['choices'][0]['message']['content'].strip()\n",
    "#\n",
    "#                 # Add assistant message\n",
    "#                 messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "#\n",
    "#                 print(\"\\n LLaMA:\", flush=True)\n",
    "#                 print(answer, flush=True)\n",
    "#                 print(\"-\" * 60 + \"\\n\")\n",
    "#\n",
    "#             else:\n",
    "#                 print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "#\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(\" Connection error:\", e)\n",
    "#             break\n"
   ],
   "id": "ca6449393a982411",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:10.025460Z",
     "start_time": "2025-05-24T12:29:10.017452Z"
    }
   },
   "cell_type": "code",
   "source": "# chat()",
   "id": "b60581168b183876",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Load the Datasets\n",
    "## 1.  OpenMathInstruct-1 (from Hugging Face)\n",
    "- This dataset contains 1.8 million math problem-solution pairs, making it ideal for enhancing mathematical reasoning in LLMs."
   ],
   "id": "f2e29ce7c06eda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:10.073556Z",
     "start_time": "2025-05-24T12:29:10.059557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datasets import load_dataset\n",
    "# from IPython import get_ipython\n",
    "# from IPython.display import display\n",
    "#\n",
    "# #Load training split\n",
    "# dataset = load_dataset(\"nvidia/OpenMathInstruct-1\", split=\"train\")\n",
    "#\n",
    "# first_element = next(iter(dataset))\n",
    "#\n",
    "# print(first_element)\n",
    "\n",
    "#Give up on it, waaaay to much data in dataset"
   ],
   "id": "12f7c6faccca5cb3",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Small & Clean Math Datasets\n",
    "## 1. GSM8K\n",
    "- Size: ~8.5K problems\n",
    "\n",
    "- Focus: Grade school math word problems\n",
    "\n",
    "- Good for: step-by-step reasoning, small LLM finetuning"
   ],
   "id": "954a019fae299897"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:15.754344Z",
     "start_time": "2025-05-24T12:29:10.088893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "1ae3182c572fe879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Computer Science Theory QA Dataset (from Kaggle)\n",
    "- This dataset offers a comprehensive collection of theoretical computer science questions, suitable for training chatbots and QA systems."
   ],
   "id": "b5932817b8f7a049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:15.817366Z",
     "start_time": "2025-05-24T12:29:15.772347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "9ff53e22afa20485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:20.814236Z",
     "start_time": "2025-05-24T12:29:15.834060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "d0b3e31e6e85f56d",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Install & Import Dependencies\n",
    "We’ll need:\n",
    "\n",
    "-  Transformers & Datasets\n",
    "\n",
    "-  LangChain & an embedding backend (here HuggingFaceEmbeddings)\n",
    "\n",
    "-  FAISS for the vector index\n",
    "\n",
    "-  Accelerate + PEFT if you plan to fine-tune your generator"
   ],
   "id": "d218ed8cd6b830ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:20.844963Z",
     "start_time": "2025-05-24T12:29:20.831952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install \\\n",
    "#  transformers datasets faiss-cpu \\\n",
    "#  langchain sentence-transformers \\\n",
    "#  accelerate peft evaluate"
   ],
   "id": "39aff69f1b83cd51",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e79cd82c56fd576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:20.876207Z",
     "start_time": "2025-05-24T12:29:20.861491Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install --upgrade peft",
   "id": "7fcfb287c01d7c07",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:20.907061Z",
     "start_time": "2025-05-24T12:29:20.893307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "15fef1ad9d22b2db",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Configuration\n",
    "Centralize all paths, model names, and hyperparameters."
   ],
   "id": "7d4f39dce9ad0059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:20.938337Z",
     "start_time": "2025-05-24T12:29:20.924078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "18f205f852935553",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Load & Merge Datasets\n",
    "Load your local Q&A (if any), plus MBPP (test split) and GSM8K train. Then standardize to a single list of “documents” with id and text."
   ],
   "id": "4f4598c6ff4a49d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:30.366505Z",
     "start_time": "2025-05-24T12:29:20.956351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "4d2f48cea8c5a6d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c852594280645ed9be25904233a0a27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05cda0a8b53e40f392b59662eb007d0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7882\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Build & Chunk the Retrieval Corpus\n",
    "We’ll treat each training example as a “document” by concatenating input_text + target_text and splitting into overlapping chunks."
   ],
   "id": "69ce26036be862fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:29:30.694756Z",
     "start_time": "2025-05-24T12:29:30.383988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "ca4d714962ad40d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8134 chunks from 7882 documents.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Embed & Build a FAISS Vector Index\n",
    "Use a Sentence-Transformer to embed each chunk, then store in FAISS for fast nearest-neighbour lookup."
   ],
   "id": "c7b19aa8fa25874a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:31:49.838993Z",
     "start_time": "2025-05-24T12:29:30.712758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "37ae62208b9e7c92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Wire Up a LangChain RetrievalQA Pipeline\n",
    "We now plug your FAISS store and the Meta-Llama-3.1-8b generator into a single retrieval-augmented chain."
   ],
   "id": "9adb9f015e48c8e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:31:49.886917Z",
     "start_time": "2025-05-24T12:31:49.871929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "45baecdb609ecdcb",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:31:51.715959Z",
     "start_time": "2025-05-24T12:31:49.892916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "3b992f350d616aa6",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:34:02.945469Z",
     "start_time": "2025-05-24T12:31:51.732969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "query = \"How would you implement binary search in Python?\"\n",
    "result = qa_chain(query)\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "\n"
   ],
   "id": "526fbcb07a89502f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Here's an implementation of binary search in Python:\n",
      "\n",
      "```python\n",
      "def binary_search(arr, target):\n",
      "    \"\"\"\n",
      "    Searches for a target element in a sorted array using binary search.\n",
      "\n",
      "    Args:\n",
      "        arr (list): A sorted list of elements.\n",
      "        target: The element to be searched.\n",
      "\n",
      "    Returns:\n",
      "        int or None: The index of the target element if found, otherwise None.\n",
      "    \"\"\"\n",
      "\n",
      "    # Initialize the low and high indices\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "\n",
      "    while low <= high:\n",
      "        # Calculate the mid index\n",
      "        mid = (low + high) // 2\n",
      "\n",
      "        # If the target is found at the mid index, return it\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "\n",
      "        # If the target is less than the element at the mid index,\n",
      "        # search in the left half\n",
      "        elif arr[mid] > target:\n",
      "            high = mid - 1\n",
      "\n",
      "        # If the target is greater than the element at the mid index,\n",
      "        # search in the right half\n",
      "        else:\n",
      "            low = mid + 1\n",
      "\n",
      "    # If the target is not found, return None\n",
      "    return None\n",
      "\n",
      "\n",
      "# Example usage:\n",
      "arr = [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]\n",
      "target = 23\n",
      "\n",
      "result = binary_search(arr, target)\n",
      "\n",
      "if result is not None:\n",
      "    print(f\"Target {target} found at index {result}\")\n",
      "else:\n",
      "    print(\"Target not found in the array\")\n",
      "```\n",
      "\n",
      "This implementation takes a sorted list `arr` and a target element `target` as input. It returns the index of the target element if it's found, or `None` otherwise.\n",
      "\n",
      "Note that binary search requires the input array to be sorted in ascending order for this algorithm to work correctly. Also, the time complexity of this implementation is O(log n), where n is the number of elements in the array.\n",
      "\n",
      "You can modify the array and target values to test the function with different inputs.\n",
      "\n",
      "Sources:\n",
      "- doc-154\n",
      "- doc-23\n",
      "- doc-153\n",
      "- doc-148\n",
      "Answer:\n",
      " Here's a simple implementation of the binary search algorithm in Python:\n",
      "\n",
      "```python\n",
      "def binary_search(arr, target):\n",
      "    \"\"\"\n",
      "    Searches for an element in a sorted array using binary search.\n",
      "\n",
      "    Args:\n",
      "        arr (list): The sorted list to search.\n",
      "        target: The element to search for.\n",
      "\n",
      "    Returns:\n",
      "        int: The index of the target element if found. -1 otherwise.\n",
      "    \"\"\"\n",
      "\n",
      "    # Initialize the low and high pointers\n",
      "    low = 0\n",
      "    high = len(arr) - 1\n",
      "\n",
      "    while low <= high:\n",
      "        # Calculate the mid index\n",
      "        mid = (low + high) // 2\n",
      "\n",
      "        # If the target is found, return its index\n",
      "        if arr[mid] == target:\n",
      "            return mid\n",
      "\n",
      "        # If the target is less than the middle element, move the high pointer to the left\n",
      "        elif arr[mid] > target:\n",
      "            high = mid - 1\n",
      "\n",
      "        # If the target is greater than the middle element, move the low pointer to the right\n",
      "        else:\n",
      "            low = mid + 1\n",
      "\n",
      "    # If the target is not found, return -1\n",
      "    return -1\n",
      "\n",
      "\n",
      "# Example usage:\n",
      "arr = [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]\n",
      "target = 23\n",
      "\n",
      "index = binary_search(arr, target)\n",
      "\n",
      "if index != -1:\n",
      "    print(f\"Target {target} found at index {index}\")\n",
      "else:\n",
      "    print(f\"Target {target} not found in the array\")\n",
      "```\n",
      "\n",
      "In this implementation:\n",
      "\n",
      "*   We define a function `binary_search` that takes a sorted list `arr` and a target element as input.\n",
      "*   The function uses two pointers, `low` and `high`, to represent the current search interval within the array.\n",
      "*   In each iteration of the loop, we calculate the middle index (`mid`) and compare the middle element with the target.\n",
      "*   If the target matches the middle element, we return its index. Otherwise, we adjust the `low` or `high` pointer based on whether the target is less than or greater than the middle element.\n",
      "*   The loop continues until the target is found (returning its index) or the search interval becomes empty (returning -1 to indicate that the target is not in the array).\n",
      "\n",
      "This implementation has a time complexity of O(log n), making it efficient for searching large sorted arrays.\n",
      "\n",
      "Sources:\n",
      "- doc-154\n",
      "- doc-23\n",
      "- doc-153\n",
      "- doc-148\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:34:02.993453Z",
     "start_time": "2025-05-24T12:34:02.979453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from os.path import exists\n",
    "\n",
    "import requests\n",
    "\n",
    "#API endpoint exposed in Lm studio\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "#model ID\n",
    "model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "\n",
    "headers={\n",
    "    \"Content-Type\" : \"application/json\",\n",
    "    \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "}\n",
    "\n",
    "# messages: [ #keep conversation history\n",
    "#                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "#                  \"content\":user_input}\n",
    "#             ]\n",
    "\n",
    "#Keep the message history\n",
    "\n",
    "#History file path, to keep conversation\n",
    "history_file = \"chat-history.json\"\n",
    "def save_history(messages):\n",
    "    # Load existing history if the file exists\n",
    "    if exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_history = json.load(f)\n",
    "            if isinstance(full_history, list):\n",
    "                pass\n",
    "            else:\n",
    "                full_history = [full_history]\n",
    "    else:\n",
    "        full_history = []\n",
    "\n",
    "    # Add this session with timestamp\n",
    "    full_history.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": messages\n",
    "    })\n",
    "\n",
    "    # Save the full conversation list\n",
    "    with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "#Prompt loop\n",
    "def chat():\n",
    "    print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "     \"content\": \"You are a helpful programming tutor.\"}\n",
    "] #Messages reset each time\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            #save chat history\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            break\n",
    "\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\",\n",
    "                         \"content\": user_input})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_id,#id of model\n",
    "            \"messages\": messages,#chat history to preserve context\n",
    "            \"temperature\": 0.7 #control creativiy\n",
    "        }\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:#send request to lm api\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=1000)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "                # Add assistant message\n",
    "                messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "                print(\"\\n LLaMA:\", flush=True)\n",
    "                print(answer, flush=True)\n",
    "                print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\" Connection error:\", e)\n",
    "            break\n"
   ],
   "id": "34443ddc8c9e41c9",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T13:36:07.541318Z",
     "start_time": "2025-05-24T13:26:05.548368Z"
    }
   },
   "cell_type": "code",
   "source": "chat()",
   "id": "e519a2c24c2734ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Talk to LLaMA 3.1 (type 'exit' to quit)\n",
      "\n",
      "Your question is: \n",
      "\"A trail has 1000 km. A bike want to ride the entire trail.\\nFirstly, he rode for an hour, at a speed of 75 meters per second\\nThen, for the second part, he rode 30 percent of the remaining part.\\nWhat is the length of the last part?\n",
      "\n",
      "\n",
      "\n",
      " LLaMA:\n",
      "To solve this problem, we need to break it down into steps.\n",
      "\n",
      "**Step 1: Convert the initial speed from meters per second to kilometers per hour**\n",
      "\n",
      "There are 1000 meters in a kilometer and 3600 seconds in an hour. \n",
      "\n",
      "```python\n",
      "# Given values\n",
      "initial_speed_mps = 75  # Initial speed in meters per second\n",
      "hours_in_second = 3600  # Number of seconds in an hour\n",
      "\n",
      "# Convert initial speed from m/s to km/h\n",
      "initial_speed_kmh = (initial_speed_mps * 1000) / hours_in_second\n",
      "```\n",
      "\n",
      "**Step 2: Calculate the distance covered in the first part**\n",
      "\n",
      "The bike rode for 1 hour at a speed of `initial_speed_kmh` kilometers per hour. We can calculate this using simple multiplication.\n",
      "\n",
      "```python\n",
      "# Given values\n",
      "trail_length_km = 1000  # Total length of the trail in kilometers\n",
      "\n",
      "# Distance covered in the first part\n",
      "distance_covered_part1 = initial_speed_kmh * 1\n",
      "```\n",
      "\n",
      "**Step 3: Calculate the remaining distance after the first part**\n",
      "\n",
      "We subtract the distance covered in the first part from the total trail length.\n",
      "\n",
      "```python\n",
      "# Remaining distance after the first part\n",
      "remaining_distance = trail_length_km - distance_covered_part1\n",
      "```\n",
      "\n",
      "**Step 4: Calculate the distance covered in the second part**\n",
      "\n",
      "The bike rode for 30% of the remaining distance.\n",
      "\n",
      "```python\n",
      "# Distance covered in the second part (30% of the remaining distance)\n",
      "distance_covered_part2 = remaining_distance * 0.3\n",
      "```\n",
      "\n",
      "**Step 5: Calculate the length of the last part**\n",
      "\n",
      "We subtract the distance covered in the second part from the remaining distance after the first part.\n",
      "\n",
      "```python\n",
      "# Length of the last part\n",
      "last_part_length = remaining_distance - distance_covered_part2\n",
      "```\n",
      "\n",
      "Now, let's combine all these steps into a single Python function:\n",
      "\n",
      "```python\n",
      "def calculate_last_part_length():\n",
      "    initial_speed_mps = 75  # Initial speed in meters per second\n",
      "    hours_in_second = 3600  # Number of seconds in an hour\n",
      "    trail_length_km = 1000  # Total length of the trail in kilometers\n",
      "\n",
      "    initial_speed_kmh = (initial_speed_mps * 1000) / hours_in_second\n",
      "    distance_covered_part1 = initial_speed_kmh * 1\n",
      "    remaining_distance = trail_length_km - distance_covered_part1\n",
      "    distance_covered_part2 = remaining_distance * 0.3\n",
      "    last_part_length = remaining_distance - distance_covered_part2\n",
      "\n",
      "    return last_part_length\n",
      "\n",
      "# Execute the function to get the length of the last part\n",
      "last_part_length = calculate_last_part_length()\n",
      "print(last_part_length)\n",
      "```\n",
      "\n",
      "This will output the length of the last part, which is the answer we're looking for.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Your question is: \n",
      "I mean, do not generate code, give me the mathematical solution for it, and the values at each step\n",
      "\n",
      "\n",
      "\n",
      " LLaMA:\n",
      "Let's break down the problem into steps with mathematical solutions:\n",
      "\n",
      "**Step 1: Calculate the distance covered in the first part**\n",
      "\n",
      "The bike rode for 1 hour at a speed of 75 meters per second. \n",
      "\n",
      "First, we need to convert this speed from meters per second to kilometers per hour:\n",
      "`75 m/s * (1000 m/km) / (3600 s/h) = 20.83 km/h`\n",
      "\n",
      "Now, let's calculate the distance covered in the first part:\n",
      "\n",
      "`Distance covered = Speed × Time = 20.83 km/h × 1 h = 20.83 km`\n",
      "\n",
      "**Step 2: Calculate the remaining distance after the first part**\n",
      "\n",
      "We subtract the distance covered in the first part from the total trail length (1000 km):\n",
      "\n",
      "`Remaining distance = Trail length - Distance covered = 1000 km - 20.83 km ≈ 979.17 km`\n",
      "\n",
      "**Step 3: Calculate the distance covered in the second part**\n",
      "\n",
      "The bike rode for 30% of the remaining distance:\n",
      "\n",
      "`Distance covered = 30% × Remaining distance = 0.3 × 979.17 km ≈ 293.85 km`\n",
      "\n",
      "**Step 4: Calculate the length of the last part**\n",
      "\n",
      "We subtract the distance covered in the second part from the remaining distance after the first part:\n",
      "\n",
      "`Length of the last part = Remaining distance - Distance covered = 979.17 km - 293.85 km ≈ 685.32 km`\n",
      "\n",
      "Therefore, the length of the last part is approximately **685.32 kilometers**.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Your question is: \n",
      "You are wrong. Calculate again from m/s into km/h\n",
      "Then, give me the new solution\n",
      "\n",
      "\n",
      "\n",
      " LLaMA:\n",
      "Let's recalculate:\n",
      "\n",
      "`75 m/s * (1000 m/km) / (3600 s/h) = 20.83 km/h`\n",
      "\n",
      "However, this is incorrect because we need to convert meters per second to kilometers per hour.\n",
      "\n",
      "The correct conversion is:\n",
      "\n",
      "`Speed in km/h = Speed in m/s × (1000 m/km) / (3600 s/h)`\n",
      "`= 75 m/s × 1000 m/km ÷ 3600 s/h`\n",
      "`= 20.83 km/h`\n",
      "\n",
      "However, we actually need to use this formula\n",
      "\n",
      "`Speed in km/h = Speed in m/s * (3600s/h)`\n",
      "\n",
      "So\n",
      "`Speed in km/h = 75m/s * 3.6km/hr/1m/s = 270 km/h`\n",
      "\n",
      "Now that we have the correct speed in kilometers per hour, let's continue with the solution:\n",
      "\n",
      "**Step 1: Calculate the distance covered in the first part**\n",
      "\n",
      "`Distance covered = Speed × Time = 270 km/h × 1 h = 270 km`\n",
      "\n",
      "**Step 2: Calculate the remaining distance after the first part**\n",
      "\n",
      "We subtract the distance covered in the first part from the total trail length (1000 km):\n",
      "\n",
      "`Remaining distance = Trail length - Distance covered = 1000 km - 270 km ≈ 730 km`\n",
      "\n",
      "**Step 3: Calculate the distance covered in the second part**\n",
      "\n",
      "The bike rode for 30% of the remaining distance:\n",
      "\n",
      "`Distance covered = 30% × Remaining distance = 0.3 × 730 km ≈ 219 km`\n",
      "\n",
      "**Step 4: Calculate the length of the last part**\n",
      "\n",
      "We subtract the distance covered in the second part from the remaining distance after the first part:\n",
      "\n",
      "`Length of the last part = Remaining distance - Distance covered = 730 km - 219 km ≈ 511 km`\n",
      "\n",
      "Therefore, the length of the last part is approximately **511 kilometers**.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Your question is: \n",
      "OK. This is the right answer. Thank you\n",
      "\n",
      "\n",
      "\n",
      " LLaMA:\n",
      "I'm glad I could eventually provide the correct solution for you. If you have any more questions or need help with anything else, feel free to ask!\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      " Conversation saved to chat-history.json\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T12:42:30.836032100Z",
     "start_time": "2025-05-24T12:26:05.402654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Search for Pythagorean theorem related questions in GSM8K\n",
    "pythagorean_questions = []\n",
    "\n",
    "# Load the dataset if not already loaded\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "#\n",
    "# Search for relevant keywords\n",
    "keywords = [\"pythagora\", \"pythagorean\", \"right triangle\", \"hypotenuse\", \"a^2 + b^2\"]\n",
    "\n",
    "for i, example in enumerate(gsm):\n",
    "    question = example[\"question\"].lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in question:\n",
    "            pythagorean_questions.append({\n",
    "                \"index\": i,\n",
    "                \"question\": example[\"question\"],\n",
    "                \"answer\": example[\"answer\"]\n",
    "            })\n",
    "            break\n",
    "\n",
    "# Print the number of matching questions\n",
    "print(f\"Found {len(pythagorean_questions)} questions related to the Pythagorean theorem\")\n",
    "\n",
    "# Display the first few matches if any exist\n",
    "for i, q in enumerate(pythagorean_questions[:3]):\n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(q[\"question\"])\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(q[\"answer\"])"
   ],
   "id": "3ae90eb18febee3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 questions related to the Pythagorean theorem\n",
      "\n",
      "Question 1:\n",
      "How many right triangles with a height of 2 inches and a width of two inches could fit inside a square with 2-inch sides?\n",
      "\n",
      "Answer:\n",
      "A triangle with a height of 2 inches and width of 2 inches has an area of 2 square inches because (1/2) x 2 x 2 = <<(1/2)*2*2=2>>2\n",
      "The square has an area of 4 square inches because 2 x 2 = <<2*2=4>>4\n",
      "2 triangles would fit because 4 / 2 = <<4/2=2>>2\n",
      "#### 2\n",
      "\n",
      "Question 2:\n",
      "Elliott drew a right-angle triangle on his book. It had a base of 4 inches, a height of 3 inches and a certain length of the hypotenuse. What was the length of the perimeter of the triangle that he drew?\n",
      "\n",
      "Answer:\n",
      "Since the hypotenuse of a triangle is found by the square root of the base squared plus the height squared.\n",
      "The square of the base of the first triangle is 4*4=<<4*4=16>>16 square inches\n",
      "The square of the height of the first triangle is 3*3=<<3*3=9>>9 square inches.\n",
      "The sum of the squares of the base and the height of the first triangle is 16+9=<<16+9=25>>25\n",
      "The square root of the sum of the base and the height of the first triangle, which is the hypotenuse of the triangle, is √25=5 inches.\n",
      "Since the perimeter of a triangle is found by adding the sides of the triangle, the base and height of the triangle sum up to 3+4=<<3+4=7>>7 inches\n",
      "If you add the hypotenuse of the triangle the perimeter of the triangle becomes 7+5=<<7+5=12>>12 inches.\n",
      "#### 12\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T13:41:41.319031Z",
     "start_time": "2025-05-24T13:41:41.277212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load the FAISS index file\n",
    "index = faiss.read_index(\"results/faiss_index/index.faiss\")\n",
    "\n",
    "# Print basic info\n",
    "print(\"Index type:\", type(index).__name__)\n",
    "print(\"Dimension:\", index.d)\n",
    "print(\"Is trained:\", index.is_trained)\n",
    "print(\"Total vectors stored:\", index.ntotal)\n",
    "\n",
    "# Example: retrieve all stored vectors (if they fit in memory)\n",
    "try:\n",
    "    xb = index.reconstruct_n(0, index.ntotal)  # returns all vectors\n",
    "    print(\"Sample vector (first one):\", xb[0])\n",
    "except Exception as e:\n",
    "    print(\"Cannot reconstruct vectors:\", e)\n"
   ],
   "id": "e73fbafadbe7a90c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index type: IndexFlatL2\n",
      "Dimension: 384\n",
      "Is trained: True\n",
      "Total vectors stored: 8134\n",
      "Sample vector (first one): [-3.02624907e-02  7.22383931e-02 -6.68388456e-02  1.75723713e-02\n",
      " -2.41824351e-02  3.70532759e-02 -9.77797061e-02  6.47424087e-02\n",
      "  6.48947656e-02 -2.07606964e-02 -1.24335475e-02  9.45907843e-04\n",
      " -2.78414711e-02  6.33043125e-02  1.54461376e-02 -2.11358052e-02\n",
      " -9.38261226e-02  7.47624934e-02 -9.11829323e-02 -2.27865204e-02\n",
      "  2.05206946e-02 -8.64782780e-02 -5.81227727e-02  3.87482978e-02\n",
      " -8.19137320e-02  1.53690781e-02 -2.25819871e-02 -8.36394802e-02\n",
      " -2.00431757e-02  5.63042648e-02  7.17877503e-03 -2.15586852e-02\n",
      "  1.13507301e-01  2.20802566e-03 -7.06357881e-02 -5.60491486e-03\n",
      " -4.40893508e-02  4.36154939e-02  4.65896949e-02  6.44860938e-02\n",
      " -3.21809463e-02  8.65907222e-03 -1.81582831e-02  1.13618091e-01\n",
      " -8.32099468e-02  6.22076308e-03 -4.50080680e-03 -3.25722620e-02\n",
      " -4.34244834e-02  3.14856209e-02 -4.16125432e-02  5.70429005e-02\n",
      " -1.35750741e-01  4.99690808e-02  6.87148422e-02  5.55350073e-02\n",
      " -1.73626747e-02  5.05289435e-02  7.26816952e-02  4.41992357e-02\n",
      "  3.80623005e-02 -2.46894341e-02 -2.28745360e-02 -4.18314599e-02\n",
      " -4.90794368e-02 -3.57603617e-02 -1.77178869e-03 -4.47373725e-02\n",
      " -4.50720191e-02 -3.76983993e-02 -2.93574221e-02  4.71875556e-02\n",
      " -3.24826539e-02  6.17433824e-02  5.18594943e-02 -5.97414114e-02\n",
      "  3.64417695e-02  8.77248943e-02 -2.78643053e-03 -5.15421629e-02\n",
      "  5.45396795e-03  1.91740375e-02 -4.73174453e-02  6.67118281e-03\n",
      " -1.76317040e-02  4.88698222e-02 -1.48742450e-02  2.11040694e-02\n",
      "  6.70900494e-02 -3.68609205e-02  8.73995274e-02 -4.03654203e-02\n",
      " -1.07541934e-01 -9.02424101e-03  7.29118288e-02 -2.36766189e-02\n",
      "  5.38964123e-02 -7.01383734e-03 -5.05659506e-02 -3.30202058e-02\n",
      "  9.62843597e-02 -3.08082737e-02 -5.99464253e-02 -2.80544572e-02\n",
      "  5.83596751e-02  3.48494239e-02 -1.08171571e-02  6.07849956e-02\n",
      "  9.62873921e-02  3.91665436e-02 -3.01371375e-03 -3.33057414e-03\n",
      "  2.14236639e-02  5.18284999e-02  2.02196185e-02  5.96266948e-02\n",
      " -3.18730511e-02 -2.31898744e-02  5.09654842e-02 -8.62271246e-03\n",
      "  7.29564428e-02  5.36229368e-03  8.32235664e-02  4.60455660e-03\n",
      "  2.70759407e-02 -4.84297350e-02 -7.01745898e-02  8.60885869e-34\n",
      " -9.30826813e-02  3.26354466e-02  6.00619353e-02  5.16111180e-02\n",
      " -1.66789498e-02 -8.62234756e-02 -8.67691040e-02  4.41854149e-02\n",
      "  1.38238505e-01  7.59476051e-02 -3.32016312e-02 -3.29217874e-02\n",
      "  2.00368930e-02  1.90206207e-02  5.32924496e-02  4.94097956e-02\n",
      " -8.92968383e-03  6.28983974e-02 -3.39066163e-02  8.09199177e-03\n",
      " -2.26265080e-02  1.67557821e-02  1.45929977e-02  1.04366206e-02\n",
      "  1.31562899e-03 -3.47076692e-02  2.40507703e-02  5.24313711e-02\n",
      " -8.19243118e-03 -1.40762506e-02  2.24919617e-02 -2.13740431e-02\n",
      " -5.75783402e-02 -6.14987500e-02  1.46301659e-02 -1.06881350e-01\n",
      "  1.93208158e-02 -1.72359608e-02 -4.21077199e-02 -1.91528797e-02\n",
      " -6.82955682e-02  4.56632152e-02  4.51305369e-03 -3.36135179e-02\n",
      "  2.93965451e-02 -1.69405695e-02 -3.57040986e-02 -4.89357971e-02\n",
      "  2.12132782e-02 -1.39172534e-02 -6.29993826e-02 -1.11320112e-02\n",
      "  2.66618095e-02 -4.19501634e-03  1.57772060e-02  4.15535457e-02\n",
      " -1.49317831e-02  5.72256371e-03 -2.83229724e-02  9.22000706e-02\n",
      "  1.99791584e-02  7.31931925e-02  1.37553392e-02  3.04012955e-03\n",
      " -8.29752535e-02 -1.59747098e-02 -1.08044066e-01  6.59611598e-02\n",
      "  3.55108455e-02 -9.10178944e-03 -1.47266313e-02 -4.90705445e-02\n",
      "  1.19992550e-02  6.87110722e-02 -3.27497311e-02 -1.32389575e-01\n",
      " -6.93674339e-03  1.63893998e-02 -2.08687666e-03 -2.40231189e-03\n",
      " -4.20149565e-02  4.50700447e-02  7.56439194e-02 -8.55102614e-02\n",
      " -4.33950573e-02 -7.13297874e-02 -3.10431700e-02 -8.84773061e-02\n",
      " -2.43927650e-02 -6.43796921e-02 -5.28331324e-02  2.36066561e-02\n",
      "  1.23607006e-03  2.25281380e-02  3.23708393e-02 -3.45235013e-33\n",
      " -1.46909570e-02  1.37784081e-02 -9.70709324e-02 -5.92362657e-02\n",
      " -5.42843118e-02  3.02088670e-02 -1.90271041e-03  8.10524635e-03\n",
      " -2.51208451e-02 -7.30578275e-03 -2.61047930e-02 -2.99252346e-02\n",
      " -3.43480660e-03 -3.63833681e-02  1.23243928e-02  1.27283921e-02\n",
      " -5.22396378e-02  1.15864323e-02 -1.06899746e-01  1.88159868e-02\n",
      "  3.51820923e-02 -8.82379934e-02 -2.06824355e-02 -8.98488536e-02\n",
      "  2.88093966e-02  3.49323004e-02  3.18665756e-04 -5.76154999e-02\n",
      "  5.30472258e-03 -1.22074299e-02 -3.99465337e-02 -7.35264271e-02\n",
      " -7.74059221e-02 -3.74777541e-02 -6.86296299e-02 -9.93640870e-02\n",
      " -3.70817892e-02 -4.73668836e-02  2.12256499e-02  4.16661846e-03\n",
      "  1.04278348e-01 -3.38916183e-02  1.23174205e-01 -1.08330511e-03\n",
      "  2.37645325e-03  9.56698284e-02  9.70302001e-02  1.90250240e-02\n",
      " -5.47967199e-03 -4.57818098e-02  2.34673452e-02  1.03772562e-02\n",
      "  1.89602878e-02 -3.45035898e-03  1.24566659e-01 -7.90553540e-02\n",
      " -3.00709940e-02  2.56550517e-02  4.51376550e-02  1.12722069e-02\n",
      " -2.94948369e-02 -5.13441823e-02 -3.35898437e-02  1.01824902e-01\n",
      " -5.58556318e-02  6.59712255e-02  3.93985100e-02  5.50416857e-02\n",
      " -5.61586348e-04 -2.20448840e-02  1.64119829e-03  7.46745020e-02\n",
      " -6.93473220e-02  1.03441644e-02 -8.46221745e-02  5.89015372e-02\n",
      " -2.22491883e-02 -4.57200818e-02  7.48379156e-04 -1.94720160e-02\n",
      "  4.51704487e-02 -4.20103036e-02  1.35541903e-02 -9.35625434e-02\n",
      " -2.38072071e-02 -5.55793159e-02  8.98805410e-02  6.66434765e-02\n",
      " -1.20318774e-02  2.97571830e-02 -1.56787820e-02  3.07523012e-02\n",
      " -1.81249678e-02  2.39895694e-02  7.81591386e-02 -4.78124704e-08\n",
      " -7.46104866e-02  7.41205886e-02 -5.07157221e-02 -3.99997644e-02\n",
      " -4.64815944e-02 -1.82636231e-02  7.31968209e-02  6.63182735e-02\n",
      " -8.15213174e-02 -6.65399386e-03 -9.08669755e-02  1.66668184e-02\n",
      " -9.23803914e-03  6.72996640e-02 -2.47914158e-02  6.46117851e-02\n",
      " -1.21997483e-02 -6.13054447e-02  6.12647757e-02  5.90059683e-02\n",
      " -5.38649522e-02 -5.55842966e-02  5.29451622e-03  5.56608588e-02\n",
      " -5.51686697e-02  5.34721464e-02  4.73495461e-02  1.41075462e-01\n",
      "  6.69531003e-02 -1.27565553e-02  1.75453555e-02 -1.86801031e-02\n",
      " -8.18819925e-03  1.62169728e-02  3.53680402e-02 -1.21413276e-03\n",
      " -7.32665509e-02  1.83390398e-02  1.76480170e-02  2.05820799e-02\n",
      " -6.82666972e-02 -4.37984914e-02 -3.29741687e-02 -1.14266947e-02\n",
      "  8.18527862e-02  7.78639242e-02  4.29567099e-02  6.00760616e-02\n",
      " -1.18260346e-01  2.35932246e-02  1.75139681e-02  1.79554299e-02\n",
      "  5.36948107e-02 -2.58219019e-02  1.28816236e-02 -6.17596135e-02\n",
      " -1.78748257e-02  3.74099985e-03  3.21210884e-02  2.25860961e-02\n",
      "  6.09379932e-02  6.98496848e-02  4.78541702e-02  2.60180607e-02]\n"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
