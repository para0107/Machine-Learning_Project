{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring the Capabilities of LLM Models\n",
    "\n",
    "In this notebook, I aim to evaluate and compare the capabilities of two large language models (LLMs):\n",
    "\n",
    "1. **Codestral22B**\n",
    "   A state-of-the-art model designed for advanced code generation and natural language understanding tasks.\n",
    "\n",
    "2. **Llama 3.1-8B**\n",
    "   A highly efficient and compact model optimized for general-purpose language tasks with an 8-billion parameter architecture.\n",
    "\n",
    "The goal is to analyze their performance across various tasks, including but not limited to:\n",
    "\n",
    "- Code generation and completion\n",
    "- Natural language understanding\n",
    "- Contextual reasoning\n",
    "- Problem-solving capabilities\n",
    "\n",
    "This comparison will help identify the strengths and weaknesses of each model and provide insights into their practical applications."
   ],
   "id": "f51589c43481d44c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI-Powered Programming Tutor with RAG\n",
    "\n",
    "This project focuses on building an AI-powered programming tutor designed to assist students in understanding code and solving problems. The tutor leverages **Retrieval-Augmented Generation (RAG)** to provide accurate and personalized explanations grounded in real university materials, such as:\n",
    "\n",
    "- Past assignments\n",
    "- Lecture notes\n",
    "- Tutorials`\n",
    "\n",
    "The system integrates two large language models (LLMs), **Codestral22B** and **Llama 3.1-8B**, to evaluate their performance in generating solutions and explanations for programming-related queries. The goal is to determine which model provides better support for students in a university setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Personalized Explanations**: Tailored responses based on retrieved university materials.\n",
    "- **Code Understanding**: Helps students debug and understand code snippets.\n",
    "- **Problem Solving**: Provides step-by-step solutions to programming problems.\n",
    "- **Model Comparison**: Evaluates the performance of Codestral22B and Llama 3.1-8B.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "7d228540125879a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Basically, I will try to implement and test Codestral22B and Llama 3.1-8B",
   "id": "66487be05849c784"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:36:51.922184Z",
     "start_time": "2025-05-25T05:36:51.909690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datetime import datetime\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# from pprint import pprint\n",
    "# from os.path import exists\n",
    "#\n",
    "# import requests\n",
    "#\n",
    "# #API endpoint exposed in Lm studio\n",
    "# url = \"http://localhost:1234/v1/chat/completions\"\n",
    "#\n",
    "# #model ID\n",
    "# model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "#\n",
    "# headers={\n",
    "#     \"Content-Type\" : \"application/json\",\n",
    "#     \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "# }\n",
    "#\n",
    "# # messages: [ #keep conversation history\n",
    "# #                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "# #                  \"content\":user_input}\n",
    "# #             ]\n",
    "#\n",
    "# #Keep the message history\n",
    "#\n",
    "# #History file path, to keep conversation\n",
    "# history_file = \"chat-history.json\"\n",
    "# def save_history(messages):\n",
    "#     # Load existing history if the file exists\n",
    "#     if exists(history_file):\n",
    "#         with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             full_history = json.load(f)\n",
    "#             if isinstance(full_history, list):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 full_history = [full_history]\n",
    "#     else:\n",
    "#         full_history = []\n",
    "#\n",
    "#     # Add this session with timestamp\n",
    "#     full_history.append({\n",
    "#         \"timestamp\": datetime.now().isoformat(),\n",
    "#         \"conversation\": messages\n",
    "#     })\n",
    "#\n",
    "#     # Save the full conversation list\n",
    "#     with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "#\n",
    "#\n",
    "# #Prompt loop\n",
    "# def chat():\n",
    "#     print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "#     messages = [\n",
    "#     {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "#      \"content\": \"You are a helpful programming tutor.\"}\n",
    "# ] #Messages reset each time\n",
    "#\n",
    "#     while True:\n",
    "#         user_input = input(\" You: \")\n",
    "#         if user_input.lower() == \"exit\":\n",
    "#             #save chat history\n",
    "#             save_history(messages)\n",
    "#             print(f\"\\n Conversation saved to {history_file}\")\n",
    "#             break\n",
    "#\n",
    "#         # Add user message\n",
    "#         messages.append({\"role\": \"user\",\n",
    "#                          \"content\": user_input})\n",
    "#\n",
    "#         payload = {\n",
    "#             \"model\": model_id,#id of model\n",
    "#             \"messages\": messages,#chat history to preserve context\n",
    "#             \"temperature\": 0.7 #control creativiy\n",
    "#         }\n",
    "#         print(\"Your question is: \")\n",
    "#         print(user_input)\n",
    "#         print(\"\\n\")\n",
    "#\n",
    "#         try:#send request to lm api\n",
    "#             response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "#\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 answer = data['choices'][0]['message']['content'].strip()\n",
    "#\n",
    "#                 # Add assistant message\n",
    "#                 messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "#\n",
    "#                 print(\"\\n LLaMA:\", flush=True)\n",
    "#                 print(answer, flush=True)\n",
    "#                 print(\"-\" * 60 + \"\\n\")\n",
    "#\n",
    "#             else:\n",
    "#                 print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "#\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(\" Connection error:\", e)\n",
    "#             break\n"
   ],
   "id": "ca6449393a982411",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:36:51.952908Z",
     "start_time": "2025-05-25T05:36:51.937849Z"
    }
   },
   "cell_type": "code",
   "source": "# chat()",
   "id": "b60581168b183876",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Load the Datasets\n",
    "## 1.  OpenMathInstruct-1 (from Hugging Face)\n",
    "- This dataset contains 1.8 million math problem-solution pairs, making it ideal for enhancing mathematical reasoning in LLMs."
   ],
   "id": "f2e29ce7c06eda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:36:51.984138Z",
     "start_time": "2025-05-25T05:36:51.969619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datasets import load_dataset\n",
    "# from IPython import get_ipython\n",
    "# from IPython.display import display\n",
    "#\n",
    "# #Load training split\n",
    "# dataset = load_dataset(\"nvidia/OpenMathInstruct-1\", split=\"train\")\n",
    "#\n",
    "# first_element = next(iter(dataset))\n",
    "#\n",
    "# print(first_element)\n",
    "\n",
    "#Give up on it, waaaay to much data in dataset"
   ],
   "id": "12f7c6faccca5cb3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Small & Clean Math Datasets\n",
    "## 1. GSM8K\n",
    "- Size: ~8.5K problems\n",
    "\n",
    "- Focus: Grade school math word problems\n",
    "\n",
    "- Good for: step-by-step reasoning, small LLM finetuning"
   ],
   "id": "954a019fae299897"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:05.456718Z",
     "start_time": "2025-05-25T05:36:52.003179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "1ae3182c572fe879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Computer Science Theory QA Dataset (from Kaggle)\n",
    "- This dataset offers a comprehensive collection of theoretical computer science questions, suitable for training chatbots and QA systems."
   ],
   "id": "b5932817b8f7a049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:05.487322Z",
     "start_time": "2025-05-25T05:37:05.473792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "9ff53e22afa20485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:11.585714Z",
     "start_time": "2025-05-25T05:37:05.509884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "d0b3e31e6e85f56d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Install & Import Dependencies\n",
    "We’ll need:\n",
    "\n",
    "-  Transformers & Datasets\n",
    "\n",
    "-  LangChain & an embedding backend (here HuggingFaceEmbeddings)\n",
    "\n",
    "-  FAISS for the vector index\n",
    "\n",
    "-  Accelerate + PEFT if you plan to fine-tune your generator"
   ],
   "id": "d218ed8cd6b830ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:11.616384Z",
     "start_time": "2025-05-25T05:37:11.601863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install \\\n",
    "#  transformers datasets faiss-cpu \\\n",
    "#  langchain sentence-transformers \\\n",
    "#  accelerate peft evaluate"
   ],
   "id": "39aff69f1b83cd51",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e79cd82c56fd576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:11.647946Z",
     "start_time": "2025-05-25T05:37:11.634423Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install --upgrade peft",
   "id": "7fcfb287c01d7c07",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:39.224883Z",
     "start_time": "2025-05-25T05:37:11.681095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "15fef1ad9d22b2db",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Configuration\n",
    "Centralize all paths, model names, and hyperparameters."
   ],
   "id": "7d4f39dce9ad0059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:39.271016Z",
     "start_time": "2025-05-25T05:37:39.257370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "18f205f852935553",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Load & Merge Datasets\n",
    "Load your local Q&A (if any), plus MBPP (test split) and GSM8K train. Then standardize to a single list of “documents” with id and text."
   ],
   "id": "4f4598c6ff4a49d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:50.316066Z",
     "start_time": "2025-05-25T05:37:39.279118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "4d2f48cea8c5a6d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0fc358016ef432bb37e3bf7c2bdd898"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e467db1f6104a00a02371dac73ffee1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7899\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Build & Chunk the Retrieval Corpus\n",
    "We’ll treat each training example as a “document” by concatenating input_text + target_text and splitting into overlapping chunks."
   ],
   "id": "69ce26036be862fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:50.675834Z",
     "start_time": "2025-05-25T05:37:50.349137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "ca4d714962ad40d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8168 chunks from 7899 documents.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Embed & Build a FAISS Vector Index\n",
    "Use a Sentence-Transformer to embed each chunk, then store in FAISS for fast nearest-neighbour lookup."
   ],
   "id": "c7b19aa8fa25874a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:35.587284Z",
     "start_time": "2025-05-25T05:37:50.694849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "37ae62208b9e7c92",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_1636\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Wire Up a LangChain RetrievalQA Pipeline\n",
    "We now plug your FAISS store and the Meta-Llama-3.1-8b generator into a single retrieval-augmented chain."
   ],
   "id": "9adb9f015e48c8e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:35.698586Z",
     "start_time": "2025-05-25T05:40:35.692067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "45baecdb609ecdcb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:37.659428Z",
     "start_time": "2025-05-25T05:40:35.780205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "3b992f350d616aa6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:41.234914Z",
     "start_time": "2025-05-25T05:40:37.665954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# # ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "# query = \"How would you implement binary search in Python?\"\n",
    "# result = qa_chain(query)\n",
    "# print(\"Answer:\\n\", result[\"result\"])\n",
    "# print(\"\\nSources:\")\n",
    "# for doc in result[\"source_documents\"]:\n",
    "#     print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "\n"
   ],
   "id": "526fbcb07a89502f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tudor\\AppData\\Local\\Temp\\ipykernel_1636\\2054238658.py:29: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:41.297769Z",
     "start_time": "2025-05-25T05:40:41.284754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from os.path import exists\n",
    "\n",
    "import requests\n",
    "\n",
    "#API endpoint exposed in Lm studio\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "#model ID\n",
    "model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "\n",
    "headers={\n",
    "    \"Content-Type\" : \"application/json\",\n",
    "    \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "}\n",
    "\n",
    "# messages: [ #keep conversation history\n",
    "#                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "#                  \"content\":user_input}\n",
    "#             ]\n",
    "\n",
    "#Keep the message history\n",
    "\n",
    "#History file path, to keep conversation\n",
    "history_file = \"chat-history.json\"\n",
    "def save_history(messages):\n",
    "    # Load existing history if the file exists\n",
    "    if exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_history = json.load(f)\n",
    "            if isinstance(full_history, list):\n",
    "                pass\n",
    "            else:\n",
    "                full_history = [full_history]\n",
    "    else:\n",
    "        full_history = []\n",
    "\n",
    "    # Add this session with timestamp\n",
    "    full_history.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": messages\n",
    "    })\n",
    "\n",
    "    # Save the full conversation list\n",
    "    with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "#Prompt loop\n",
    "def chat():\n",
    "    print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "     \"content\": \"You are a helpful programming tutor.\"}\n",
    "] #Messages reset each time\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            #save chat history\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            break\n",
    "\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\",\n",
    "                         \"content\": user_input})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_id,#id of model\n",
    "            \"messages\": messages,#chat history to preserve context\n",
    "            \"temperature\": 0.7 #control creativiy\n",
    "        }\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:#send request to lm api\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=1000)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "                # Add assistant message\n",
    "                messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "                print(\"\\n LLaMA:\", flush=True)\n",
    "                print(answer, flush=True)\n",
    "                print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\" Connection error:\", e)\n",
    "            break\n"
   ],
   "id": "34443ddc8c9e41c9",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:41.313753Z",
     "start_time": "2025-05-25T05:40:41.309755Z"
    }
   },
   "cell_type": "code",
   "source": "# chat()",
   "id": "e519a2c24c2734ac",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:41.344551Z",
     "start_time": "2025-05-25T05:40:41.330039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Search for Pythagorean theorem related questions in GSM8K\n",
    "# pythagorean_questions = []\n",
    "#\n",
    "# # Load the dataset if not already loaded\n",
    "# gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "# #\n",
    "# # Search for relevant keywords\n",
    "# keywords = [\"pythagora\", \"pythagorean\", \"right triangle\", \"hypotenuse\", \"a^2 + b^2\"]\n",
    "#\n",
    "# for i, example in enumerate(gsm):\n",
    "#     question = example[\"question\"].lower()\n",
    "#     for keyword in keywords:\n",
    "#         if keyword.lower() in question:\n",
    "#             pythagorean_questions.append({\n",
    "#                 \"index\": i,\n",
    "#                 \"question\": example[\"question\"],\n",
    "#                 \"answer\": example[\"answer\"]\n",
    "#             })\n",
    "#             break\n",
    "#\n",
    "# # Print the number of matching questions\n",
    "# print(f\"Found {len(pythagorean_questions)} questions related to the Pythagorean theorem\")\n",
    "#\n",
    "# # Display the first few matches if any exist\n",
    "# for i, q in enumerate(pythagorean_questions[:3]):\n",
    "#     print(f\"\\nQuestion {i+1}:\")\n",
    "#     print(q[\"question\"])\n",
    "#     print(\"\\nAnswer:\")\n",
    "#     print(q[\"answer\"])"
   ],
   "id": "3ae90eb18febee3e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:41.531501Z",
     "start_time": "2025-05-25T05:40:41.514500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "#\n",
    "# # Load the FAISS index file\n",
    "# index = faiss.read_index(\"results/faiss_index/index.faiss\")\n",
    "#\n",
    "# # Print basic info\n",
    "# print(\"Index type:\", type(index).__name__)\n",
    "# print(\"Dimension:\", index.d)\n",
    "# # print(\"Is trained:\", index.is_trained)\n",
    "# # print(\"Total vectors stored:\", index.ntotal)\n",
    "#\n",
    "# # Example: retrieve all stored vectors (if they fit in memory)\n",
    "# try:\n",
    "#     xb = index.reconstruct_n(0, index.ntotal)  # returns all vectors\n",
    "#     print(\"Sample vector (first one):\", xb[0])\n",
    "# except Exception as e:\n",
    "#     print(\"Cannot reconstruct vectors:\", e)\n"
   ],
   "id": "e73fbafadbe7a90c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:41.577434Z",
     "start_time": "2025-05-25T05:40:41.549435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell: RAG Learning System - Update Knowledge Base with New Conversations\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Configuration\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LAST_UPDATE_FILE = \"results/last_update.txt\"\n",
    "\n",
    "def get_new_conversations():\n",
    "    \"\"\"Get conversations since last update\"\"\"\n",
    "    last_update = None\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\") as f:\n",
    "            last_update = f.read().strip()\n",
    "\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            history = json.load(f)\n",
    "            if not isinstance(history, list):\n",
    "                history = [history]\n",
    "\n",
    "        if last_update:\n",
    "            # Return only new conversations\n",
    "            return [conv for conv in history if \"timestamp\" in conv and conv[\"timestamp\"] > last_update]\n",
    "        return history\n",
    "    return []\n",
    "\n",
    "def is_correction(conv):\n",
    "    \"\"\"Check if conversation contains a correction\"\"\"\n",
    "    msgs = conv.get(\"conversation\", [])\n",
    "    for i in range(1, len(msgs)):\n",
    "        if msgs[i][\"role\"] == \"user\" and msgs[i-1][\"role\"] == \"assistant\":\n",
    "            content = msgs[i][\"content\"].lower()\n",
    "            if any(kw in content for kw in [\"wrong\", \"incorrect\", \"mistake\", \"no,\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def conversations_to_docs(conversations):\n",
    "    \"\"\"Convert conversations to documents for embedding\"\"\"\n",
    "    docs = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "    for i, conv in enumerate(conversations):\n",
    "        # Format the conversation as a document\n",
    "        text = \"\"\n",
    "        for msg in conv.get(\"conversation\", []):\n",
    "            if msg[\"role\"] != \"system\":\n",
    "                prefix = \"Question: \" if msg[\"role\"] == \"user\" else \"Answer: \"\n",
    "                text += f\"{prefix}{msg['content']}\\n\\n\"\n",
    "\n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            \"source\": f\"conversation-{conv.get('conversation_id', i)}\",\n",
    "            \"timestamp\": conv.get(\"timestamp\", \"\")\n",
    "        }\n",
    "\n",
    "        # Split and add chunks\n",
    "        for chunk in splitter.split_text(text):\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "    return docs\n",
    "\n",
    "def update_knowledge_base():\n",
    "    \"\"\"Update FAISS index with new conversations\"\"\"\n",
    "    print(\"Updating knowledge base...\")\n",
    "\n",
    "    # Get new conversations with focus on corrections\n",
    "    new_convs = get_new_conversations()\n",
    "    if not new_convs:\n",
    "        print(\"No new conversations found.\")\n",
    "        return\n",
    "\n",
    "    # Prioritize conversations with corrections\n",
    "    corrections = [c for c in new_convs if is_correction(c)]\n",
    "    print(f\"Found {len(new_convs)} new conversations, {len(corrections)} with corrections\")\n",
    "\n",
    "    # Convert to documents\n",
    "    docs = conversations_to_docs(new_convs)\n",
    "    if not docs:\n",
    "        print(\"No documents to add.\")\n",
    "        return\n",
    "\n",
    "    # Update vector store\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    try:\n",
    "        # Load existing index\n",
    "        vectorstore = FAISS.load_local(\n",
    "            INDEX_PATH,\n",
    "            embedder,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "        # Add new documents\n",
    "        vectorstore.add_documents(docs)\n",
    "        print(f\"Added {len(docs)} new documents to vector store\")\n",
    "\n",
    "        # Save updated index\n",
    "        vectorstore.save_local(INDEX_PATH)\n",
    "\n",
    "        # Update timestamp\n",
    "        os.makedirs(os.path.dirname(LAST_UPDATE_FILE), exist_ok=True)\n",
    "        with open(LAST_UPDATE_FILE, \"w\") as f:\n",
    "            f.write(datetime.now().isoformat())\n",
    "\n",
    "        print(\"Knowledge base updated successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating knowledge base: {str(e)}\")\n",
    "\n",
    "# Run knowledge base update\n",
    "update_knowledge_base()\n",
    "\n"
   ],
   "id": "5f5ec0b6114dda7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating knowledge base...\n",
      "No new conversations found.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:09:24.306918Z",
     "start_time": "2025-05-25T06:09:24.298916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rag_chat():\n",
    "    \"\"\"Interactive chat function that uses the RAG-enhanced model instead of direct API calls\"\"\"\n",
    "    print(\" Talk to RAG-enhanced LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a helpful programming tutor.\"}\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            update_knowledge_base()\n",
    "            break\n",
    "\n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:\n",
    "            # Use RAG chain instead of direct LM Studio API\n",
    "            result = qa_chain(user_input)\n",
    "            answer = result[\"result\"]\n",
    "\n",
    "            # Add assistant message to conversation history\n",
    "            messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "            print(\"\\n RAG-LLaMA:\", flush=True)\n",
    "            print(answer, flush=True)\n",
    "            print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {str(e)}\\n\")\n",
    "            break\n",
    "\n",
    "# Start the RAG-enhanced chat\n"
   ],
   "id": "1c8cd6f3d054927f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:40:48.819563Z",
     "start_time": "2025-05-25T05:40:41.900986Z"
    }
   },
   "cell_type": "code",
   "source": "# rag_chat()",
   "id": "b9e7a2543ab14725",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Talk to RAG-enhanced LLaMA 3.1 (type 'exit' to quit)\n",
      "\n",
      "\n",
      " Conversation saved to chat-history.json\n",
      "Updating knowledge base...\n",
      "Found 1 new conversations, 0 with corrections\n",
      "No documents to add.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:09:30.176068Z",
     "start_time": "2025-05-25T06:09:30.150074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict, Any\n",
    "import uvicorn\n",
    "import json\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# assume you’ve already done:\n",
    "#   from your_langchain_setup import qa_chain, save_history, update_knowledge_base, history_file\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# allow your React dev server to talk to us\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],  # adjust if needed\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: Optional[List[Dict[str, str]]] = None  # prior turn-by-turn\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[Dict[str, str]]              # full transcript including this turn\n",
    "\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_endpoint(req: ChatRequest):\n",
    "    # start from existing history or fresh system prompt\n",
    "    if req.history:\n",
    "        messages = req.history.copy()\n",
    "    else:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful programming tutor.\"}]\n",
    "\n",
    "    # append the new user turn\n",
    "    messages.append({\"role\": \"user\", \"content\": req.message})\n",
    "\n",
    "    try:\n",
    "        # run your RetrievalQA chain\n",
    "        result: Dict[str, Any] = qa_chain({\"query\": req.message})\n",
    "        answer = result[\"result\"]\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    # append assistant turn\n",
    "    messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    # optionally persist the conversation\n",
    "    try:\n",
    "        # load existing file or start new\n",
    "        full_history = messages\n",
    "        with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(full_history, f, indent=2, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        # if persisting fails, we won’t block the response\n",
    "        pass\n",
    "\n",
    "    return ChatResponse(answer=answer, history=messages)\n",
    "\n",
    "def start_rag_server():\n",
    "    \"\"\"Run the FastAPI server on :8000\"\"\"\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Uncomment the next line to start serving when you run the notebook:\n",
    "# start_rag_server()\n"
   ],
   "id": "ef0db09bc6e3b7f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:09:33.220830Z",
     "start_time": "2025-05-25T06:09:33.210770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run_server, daemon=True)\n",
    "thread.start()\n",
    "print(\"RAG server is now running on http://localhost:8000\")\n"
   ],
   "id": "3cc563116853fdfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG server is now running on http://localhost:8000\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
