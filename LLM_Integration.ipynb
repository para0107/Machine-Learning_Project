{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring the Capabilities of LLM Models\n",
    "\n",
    "In this notebook, I aim to evaluate and compare the capabilities of two large language models (LLMs):\n",
    "\n",
    "1. **Codestral22B**\n",
    "   A state-of-the-art model designed for advanced code generation and natural language understanding tasks.\n",
    "\n",
    "2. **Llama 3.1-8B**\n",
    "   A highly efficient and compact model optimized for general-purpose language tasks with an 8-billion parameter architecture.\n",
    "\n",
    "The goal is to analyze their performance across various tasks, including but not limited to:\n",
    "\n",
    "- Code generation and completion\n",
    "- Natural language understanding\n",
    "- Contextual reasoning\n",
    "- Problem-solving capabilities\n",
    "\n",
    "This comparison will help identify the strengths and weaknesses of each model and provide insights into their practical applications."
   ],
   "id": "e998a0cbbfb16865"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI-Powered Programming Tutor with RAG\n",
    "\n",
    "This project focuses on building an AI-powered programming tutor designed to assist students in understanding code and solving problems. The tutor leverages **Retrieval-Augmented Generation (RAG)** to provide accurate and personalized explanations grounded in real university materials, such as:\n",
    "\n",
    "- Past assignments\n",
    "- Lecture notes\n",
    "- Tutorials`\n",
    "\n",
    "The system integrates two large language models (LLMs), **Codestral22B** and **Llama 3.1-8B**, to evaluate their performance in generating solutions and explanations for programming-related queries. The goal is to determine which model provides better support for students in a university setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Personalized Explanations**: Tailored responses based on retrieved university materials.\n",
    "- **Code Understanding**: Helps students debug and understand code snippets.\n",
    "- **Problem Solving**: Provides step-by-step solutions to programming problems.\n",
    "- **Model Comparison**: Evaluates the performance of Codestral22B and Llama 3.1-8B.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "564c5ae159c2e7d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Basically, I will try to implement and test Codestral22B and Llama 3.1-8B"
   ],
   "id": "ac466a7a41b538cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:28:34.416647900Z",
     "start_time": "2025-05-24T09:28:34.329612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from os.path import exists\n",
    "\n",
    "import requests\n",
    "\n",
    "#API endpoint exposed in Lm studio\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "#model ID\n",
    "model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "\n",
    "headers={\n",
    "    \"Content-Type\" : \"application/json\",\n",
    "    \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "}\n",
    "\n",
    "# messages: [ #keep conversation history\n",
    "#                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "#                  \"content\":user_input}\n",
    "#             ]\n",
    "\n",
    "#Keep the message history\n",
    "\n",
    "#History file path, to keep conversation\n",
    "history_file = \"chat-history.json\"\n",
    "def save_history(messages):\n",
    "    # Load existing history if the file exists\n",
    "    if exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_history = json.load(f)\n",
    "            if isinstance(full_history, list):\n",
    "                pass\n",
    "            else:\n",
    "                full_history = [full_history]\n",
    "    else:\n",
    "        full_history = []\n",
    "\n",
    "    # Add this session with timestamp\n",
    "    full_history.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": messages\n",
    "    })\n",
    "\n",
    "    # Save the full conversation list\n",
    "    with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "#Prompt loop\n",
    "def chat():\n",
    "    print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "     \"content\": \"You are a helpful programming tutor.\"}\n",
    "] #Messages reset each time\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            #save chat history\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            break\n",
    "\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\",\n",
    "                         \"content\": user_input})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_id,#id of model\n",
    "            \"messages\": messages,#chat history to preserve context\n",
    "            \"temperature\": 0.7 #control creativiy\n",
    "        }\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:#send request to lm api\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "                # Add assistant message\n",
    "                messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "                print(\"\\n LLaMA:\", flush=True)\n",
    "                print(answer, flush=True)\n",
    "                print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\" Connection error:\", e)\n",
    "            break\n"
   ],
   "id": "563b527baebdc0d7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:37.406352200Z",
     "start_time": "2025-05-24T09:28:34.415647200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat()"
   ],
   "id": "64c4f903e35e8b0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Talk to LLaMA 3.1 (type 'exit' to quit)\n",
      "Your question is: \n",
      "What is 2+2?\n",
      "\n",
      "\n",
      " LLaMA:\n",
      "A simple but fundamental question!\n",
      "\n",
      "The answer to 2 + 2 is... 4.\n",
      "\n",
      "However, as your programming tutor, I'd also like to mention that this kind of calculation can be easily performed in various programming languages, such as Python:\n",
      "\n",
      "```python\n",
      "print(2 + 2)  # Outputs: 4\n",
      "```\n",
      "\n",
      "Let me know if you have any questions about how this works or if you'd like to explore more examples!\n",
      "------------------------------------------------------------\n",
      "\n",
      " Conversation saved to chat-history.json\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Load the Datasets\n",
    "## 1.  OpenMathInstruct-1 (from Hugging Face)\n",
    "- This dataset contains 1.8 million math problem-solution pairs, making it ideal for enhancing mathematical reasoning in LLMs."
   ],
   "id": "e783b140318afef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:37.407350200Z",
     "start_time": "2025-05-24T09:29:37.379357400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datasets import load_dataset\n",
    "# from IPython import get_ipython\n",
    "# from IPython.display import display\n",
    "#\n",
    "# #Load training split\n",
    "# dataset = load_dataset(\"nvidia/OpenMathInstruct-1\", split=\"train\")\n",
    "#\n",
    "# first_element = next(iter(dataset))\n",
    "#\n",
    "# print(first_element)\n",
    "\n",
    "#Give up on it, waaaay to much data in dataset"
   ],
   "id": "6e22bd11d37f410d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Small & Clean Math Datasets\n",
    "## 1. GSM8K\n",
    "- Size: ~8.5K problems\n",
    "\n",
    "- Focus: Grade school math word problems\n",
    "\n",
    "- Good for: step-by-step reasoning, small LLM finetuning"
   ],
   "id": "29e5ef5f3ac8e47f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:50.087473900Z",
     "start_time": "2025-05-24T09:29:37.388358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "103b6abd4efba505",
   "outputs": [
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e716c69a63a04b5c84540be2715d3320"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MihaiPC\\.cache\\huggingface\\hub\\datasets--gsm8k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e789ec4bd8d2465e9ae98a5ace30e5ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc1f1606d1c748a3b3f8e1c8e33ee2d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbf5534da8364ded92f353fb52081c48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "780f8bf9608b4a30b77ec5807b73269e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Computer Science Theory QA Dataset (from Kaggle)\n",
    "- This dataset offers a comprehensive collection of theoretical computer science questions, suitable for training chatbots and QA systems."
   ],
   "id": "74dbff67600058c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:50.103461700Z",
     "start_time": "2025-05-24T09:29:50.061469800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "652d5f795177fbea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/9.06k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ac91178702041eeb1f89303fa21535e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MihaiPC\\.cache\\huggingface\\hub\\datasets--google-research-datasets--mbpp. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/33.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7270d22d01d0498e8abf8f66887aec7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/60.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40943f8661ee43649edfe2f9d39ca291"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/14.0k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2443f25a9b744a98c59d3ffa41e90b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "prompt-00000-of-00001.parquet:   0%|          | 0.00/6.72k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "834de8e90d0543f0be2b33e94f4074b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/120 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "907008b08a9d461f923c6b0bdc7e0b6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/257 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ccb3afd279a4b999e44fda98653e4be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/43 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7c004c2f16f4d48976703101dc957bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating prompt split:   0%|          | 0/7 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa0e3e717a3640b09a9147d04b67777b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:56.395840Z",
     "start_time": "2025-05-24T09:29:50.091465200Z"
    }
   },
   "id": "f5671f30c45d5107"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Install & Import Dependencies\n",
    "We’ll need:\n",
    "\n",
    "-  Transformers & Datasets\n",
    "\n",
    "-  LangChain & an embedding backend (here HuggingFaceEmbeddings)\n",
    "\n",
    "-  FAISS for the vector index\n",
    "\n",
    "-  Accelerate + PEFT if you plan to fine-tune your generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aaf2d772552e3e9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.6.0)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp39-cp39-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: peft in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Using cached langchain_core-0.3.61-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.41-cp39-cp39-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.18-cp39-cp39-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp39-cp39-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.33.2-cp39-cp39-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.2.2-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading faiss_cpu-1.11.0-cp39-cp39-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 19.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.3/15.0 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.6/15.0 MB 23.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 23.0 MB/s eta 0:00:00\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached langchain_core-0.3.61-py3-none-any.whl (438 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Using cached pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Using cached sqlalchemy-2.0.41-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached greenlet-3.2.2-cp39-cp39-win_amd64.whl (294 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.10.18-cp39-cp39-win_amd64.whl (134 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached zstandard-0.23.0-cp39-cp39-win_amd64.whl (495 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, pydantic-core, orjson, jsonpatch, greenlet, faiss-cpu, async-timeout, annotated-types, SQLAlchemy, requests-toolbelt, pydantic, langsmith, langchain-core, sentence-transformers, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "Successfully installed SQLAlchemy-2.0.41 annotated-types-0.7.0 async-timeout-4.0.3 faiss-cpu-1.11.0 greenlet-3.2.2 jsonpatch-1.33 langchain-0.3.25 langchain-core-0.3.61 langchain-text-splitters-0.3.8 langsmith-0.3.42 orjson-3.10.18 pydantic-2.11.5 pydantic-core-2.33.2 requests-toolbelt-1.0.0 sentence-transformers-4.1.0 tenacity-9.1.2 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install \\\n",
    "#  transformers datasets faiss-cpu \\\n",
    "#  langchain sentence-transformers \\\n",
    "#  accelerate peft evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T09:55:10.340868300Z",
     "start_time": "2025-05-24T09:54:35.619963400Z"
    }
   },
   "id": "d8ea8e56da808e8c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5206b80e036a23c0"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from peft) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (4.52.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (1.7.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (0.32.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade peft"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T09:58:44.331229400Z",
     "start_time": "2025-05-24T09:58:41.883118300Z"
    }
   },
   "id": "93bc5a6c9d9260ef"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:02:31.136662500Z",
     "start_time": "2025-05-24T10:02:29.980932100Z"
    }
   },
   "id": "39396291d7387b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Configuration\n",
    "Centralize all paths, model names, and hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e58d96ff18bbc47"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:03:09.979319Z",
     "start_time": "2025-05-24T10:03:09.939812600Z"
    }
   },
   "id": "a5e205d61a24b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Load & Merge Datasets\n",
    "Load your local Q&A (if any), plus MBPP (test split) and GSM8K train. Then standardize to a single list of “documents” with id and text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3abbc354104d866"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57446c0955254897b993539d384da97d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "082eebb4fd8148eeaae894d97d394aee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/43 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65b69aba5db3464e8322fa83ef972634"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4c60797c8a44ee980e37e6b58e9c237"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "578d7f29cbfc4a5fa6cf1a80c282bcfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7877\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:15:38.055725900Z",
     "start_time": "2025-05-24T10:15:28.514886Z"
    }
   },
   "id": "a16dab2f46be4c65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Build & Chunk the Retrieval Corpus\n",
    "We’ll treat each training example as a “document” by concatenating input_text + target_text and splitting into overlapping chunks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c58cc26e7e0f6722"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8129 chunks from 7877 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:19:07.342105Z",
     "start_time": "2025-05-24T10:19:06.974848200Z"
    }
   },
   "id": "eb7eb230bc99f678"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Embed & Build a FAISS Vector Index\n",
    "Use a Sentence-Transformer to embed each chunk, then store in FAISS for fast nearest-neighbour lookup."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8974b76f51c560"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Temp\\ipykernel_24984\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "data": {
      "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88b2523595d341e6b3d7bdacd262d640"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MihaiPC\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d1f9cb0688445e7a01d1103c773c85b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a87caf8015434ca9a349aff016f95df1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55c607adcfcb4220b48fcf503ae0cd44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "890258466f0841caa1553680ca6ec44b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "441ced7a49254cf696a1cb23a545e180"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4fd7964b7194c37957b19b426d621b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e894b611cf944e25ae3835a1457a45d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f88f80b8a6da4307b8ac3fe6a3379ffc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fee132435394f759158fa98933fe682"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f8d1afed53e4aed8f23db20bec07556"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:21:58.498865200Z",
     "start_time": "2025-05-24T10:19:35.167708400Z"
    }
   },
   "id": "200d47f2a7344cf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Wire Up a LangChain RetrievalQA Pipeline\n",
    "We now plug your FAISS store and the Meta-Llama-3.1-8b generator into a single retrieval-augmented chain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daa6ddeeb67ebe92"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:43:24.936442Z",
     "start_time": "2025-05-24T10:43:24.909441900Z"
    }
   },
   "id": "87ddc9f54ff739d"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:51:54.952185700Z",
     "start_time": "2025-05-24T10:51:52.445613500Z"
    }
   },
   "id": "bc81aa3c1d028459"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d99677b5b2894a7b86786e75e37e79fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MihaiPC\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9128b6ed4a4d457f9c088357b26d76bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "\n",
    "# ── 2) Load Meta-Llama-3.1-8b in 8-bit with auth ─────────────────────────────────\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "GEN_MODEL = \"meta-llama/Llama-3.1-8b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    use_fast=True,\n",
    "    use_auth_token=hf_token,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_token,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ── 3) Build the HF generation pipeline ─────────────────────────────────────────\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
    "\n",
    "# ── 4) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# ── 5) Test query ──────────────────────────────────────────────────────────────\n",
    "query = \"How would you implement binary search in Python?\"\n",
    "result = qa_chain(query)\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata[\"source\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-05-24T11:26:28.093805300Z"
    }
   },
   "id": "eb19ec65ffa219ec"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bitsandbytes) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-win_amd64.whl (75.4 MB)\n",
      "   ---------------------------------------- 0.0/75.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 3.1/75.4 MB 23.1 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 8.1/75.4 MB 22.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 11.5/75.4 MB 23.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 18.4/75.4 MB 24.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 23.9/75.4 MB 24.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 29.4/75.4 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 35.4/75.4 MB 25.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 41.4/75.4 MB 26.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 47.7/75.4 MB 26.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 53.5/75.4 MB 26.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 59.8/75.4 MB 27.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 65.8/75.4 MB 27.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 72.4/75.4 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  75.2/75.4 MB 27.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.4/75.4 MB 26.1 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 991.5/991.5 kB 23.5 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.5 sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:50:47.661832600Z",
     "start_time": "2025-05-24T10:50:39.554620100Z"
    }
   },
   "id": "32b2ca524b5864f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c118fe3050167741"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
