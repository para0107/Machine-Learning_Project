{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring the Capabilities of LLM Models\n",
    "\n",
    "In this notebook, I aim to evaluate and compare the capabilities of two large language models (LLMs):\n",
    "\n",
    "1. **Codestral22B**\n",
    "   A state-of-the-art model designed for advanced code generation and natural language understanding tasks.\n",
    "\n",
    "2. **Llama 3.1-8B**\n",
    "   A highly efficient and compact model optimized for general-purpose language tasks with an 8-billion parameter architecture.\n",
    "\n",
    "The goal is to analyze their performance across various tasks, including but not limited to:\n",
    "\n",
    "- Code generation and completion\n",
    "- Natural language understanding\n",
    "- Contextual reasoning\n",
    "- Problem-solving capabilities\n",
    "\n",
    "This comparison will help identify the strengths and weaknesses of each model and provide insights into their practical applications."
   ],
   "id": "e998a0cbbfb16865"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI-Powered Programming Tutor with RAG\n",
    "\n",
    "This project focuses on building an AI-powered programming tutor designed to assist students in understanding code and solving problems. The tutor leverages **Retrieval-Augmented Generation (RAG)** to provide accurate and personalized explanations grounded in real university materials, such as:\n",
    "\n",
    "- Past assignments\n",
    "- Lecture notes\n",
    "- Tutorials`\n",
    "\n",
    "The system integrates two large language models (LLMs), **Codestral22B** and **Llama 3.1-8B**, to evaluate their performance in generating solutions and explanations for programming-related queries. The goal is to determine which model provides better support for students in a university setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Personalized Explanations**: Tailored responses based on retrieved university materials.\n",
    "- **Code Understanding**: Helps students debug and understand code snippets.\n",
    "- **Problem Solving**: Provides step-by-step solutions to programming problems.\n",
    "- **Model Comparison**: Evaluates the performance of Codestral22B and Llama 3.1-8B.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "564c5ae159c2e7d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Basically, I will try to implement and test Codestral22B and Llama 3.1-8B"
   ],
   "id": "ac466a7a41b538cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:28:34.416647900Z",
     "start_time": "2025-05-24T09:28:34.329612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from os.path import exists\n",
    "\n",
    "import requests\n",
    "\n",
    "#API endpoint exposed in Lm studio\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "#model ID\n",
    "model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "\n",
    "headers={\n",
    "    \"Content-Type\" : \"application/json\",\n",
    "    \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "}\n",
    "\n",
    "# messages: [ #keep conversation history\n",
    "#                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "#                  \"content\":user_input}\n",
    "#             ]\n",
    "\n",
    "#Keep the message history\n",
    "\n",
    "#History file path, to keep conversation\n",
    "history_file = \"chat-history.json\"\n",
    "def save_history(messages):\n",
    "    # Load existing history if the file exists\n",
    "    if exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_history = json.load(f)\n",
    "            if isinstance(full_history, list):\n",
    "                pass\n",
    "            else:\n",
    "                full_history = [full_history]\n",
    "    else:\n",
    "        full_history = []\n",
    "\n",
    "    # Add this session with timestamp\n",
    "    full_history.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": messages\n",
    "    })\n",
    "\n",
    "    # Save the full conversation list\n",
    "    with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "#Prompt loop\n",
    "def chat():\n",
    "    print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "     \"content\": \"You are a helpful programming tutor.\"}\n",
    "] #Messages reset each time\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            #save chat history\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            break\n",
    "\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\",\n",
    "                         \"content\": user_input})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_id,#id of model\n",
    "            \"messages\": messages,#chat history to preserve context\n",
    "            \"temperature\": 0.7 #control creativiy\n",
    "        }\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:#send request to lm api\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "                # Add assistant message\n",
    "                messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "                print(\"\\n LLaMA:\", flush=True)\n",
    "                print(answer, flush=True)\n",
    "                print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\" Connection error:\", e)\n",
    "            break\n"
   ],
   "id": "563b527baebdc0d7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:37.406352200Z",
     "start_time": "2025-05-24T09:28:34.415647200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat()"
   ],
   "id": "64c4f903e35e8b0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Talk to LLaMA 3.1 (type 'exit' to quit)\n",
      "Your question is: \n",
      "What is 2+2?\n",
      "\n",
      "\n",
      " LLaMA:\n",
      "A simple but fundamental question!\n",
      "\n",
      "The answer to 2 + 2 is... 4.\n",
      "\n",
      "However, as your programming tutor, I'd also like to mention that this kind of calculation can be easily performed in various programming languages, such as Python:\n",
      "\n",
      "```python\n",
      "print(2 + 2)  # Outputs: 4\n",
      "```\n",
      "\n",
      "Let me know if you have any questions about how this works or if you'd like to explore more examples!\n",
      "------------------------------------------------------------\n",
      "\n",
      " Conversation saved to chat-history.json\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Load the Datasets\n",
    "## 1.  OpenMathInstruct-1 (from Hugging Face)\n",
    "- This dataset contains 1.8 million math problem-solution pairs, making it ideal for enhancing mathematical reasoning in LLMs."
   ],
   "id": "e783b140318afef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:37.407350200Z",
     "start_time": "2025-05-24T09:29:37.379357400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datasets import load_dataset\n",
    "# from IPython import get_ipython\n",
    "# from IPython.display import display\n",
    "#\n",
    "# #Load training split\n",
    "# dataset = load_dataset(\"nvidia/OpenMathInstruct-1\", split=\"train\")\n",
    "#\n",
    "# first_element = next(iter(dataset))\n",
    "#\n",
    "# print(first_element)\n",
    "\n",
    "#Give up on it, waaaay to much data in dataset"
   ],
   "id": "6e22bd11d37f410d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Small & Clean Math Datasets\n",
    "## 1. GSM8K\n",
    "- Size: ~8.5K problems\n",
    "\n",
    "- Focus: Grade school math word problems\n",
    "\n",
    "- Good for: step-by-step reasoning, small LLM finetuning"
   ],
   "id": "29e5ef5f3ac8e47f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:50.087473900Z",
     "start_time": "2025-05-24T09:29:37.388358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "103b6abd4efba505",
   "outputs": [
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e716c69a63a04b5c84540be2715d3320"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MihaiPC\\.cache\\huggingface\\hub\\datasets--gsm8k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e789ec4bd8d2465e9ae98a5ace30e5ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc1f1606d1c748a3b3f8e1c8e33ee2d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbf5534da8364ded92f353fb52081c48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "780f8bf9608b4a30b77ec5807b73269e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Computer Science Theory QA Dataset (from Kaggle)\n",
    "- This dataset offers a comprehensive collection of theoretical computer science questions, suitable for training chatbots and QA systems."
   ],
   "id": "74dbff67600058c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:50.103461700Z",
     "start_time": "2025-05-24T09:29:50.061469800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "652d5f795177fbea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/9.06k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ac91178702041eeb1f89303fa21535e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MihaiPC\\.cache\\huggingface\\hub\\datasets--google-research-datasets--mbpp. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/33.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7270d22d01d0498e8abf8f66887aec7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/60.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40943f8661ee43649edfe2f9d39ca291"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/14.0k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2443f25a9b744a98c59d3ffa41e90b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "prompt-00000-of-00001.parquet:   0%|          | 0.00/6.72k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "834de8e90d0543f0be2b33e94f4074b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/120 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "907008b08a9d461f923c6b0bdc7e0b6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/257 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ccb3afd279a4b999e44fda98653e4be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/43 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7c004c2f16f4d48976703101dc957bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating prompt split:   0%|          | 0/7 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa0e3e717a3640b09a9147d04b67777b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T09:29:56.395840Z",
     "start_time": "2025-05-24T09:29:50.091465200Z"
    }
   },
   "id": "f5671f30c45d5107"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Install & Import Dependencies\n",
    "We’ll need:\n",
    "\n",
    "-  Transformers & Datasets\n",
    "\n",
    "-  LangChain & an embedding backend (here HuggingFaceEmbeddings)\n",
    "\n",
    "-  FAISS for the vector index\n",
    "\n",
    "-  Accelerate + PEFT if you plan to fine-tune your generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aaf2d772552e3e9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.6.0)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp39-cp39-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: peft in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Using cached langchain_core-0.3.61-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.41-cp39-cp39-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.18-cp39-cp39-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp39-cp39-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.33.2-cp39-cp39-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.2.2-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading faiss_cpu-1.11.0-cp39-cp39-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 19.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.3/15.0 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.6/15.0 MB 23.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 23.0 MB/s eta 0:00:00\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached langchain_core-0.3.61-py3-none-any.whl (438 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Using cached pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Using cached sqlalchemy-2.0.41-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached greenlet-3.2.2-cp39-cp39-win_amd64.whl (294 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached orjson-3.10.18-cp39-cp39-win_amd64.whl (134 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached zstandard-0.23.0-cp39-cp39-win_amd64.whl (495 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, pydantic-core, orjson, jsonpatch, greenlet, faiss-cpu, async-timeout, annotated-types, SQLAlchemy, requests-toolbelt, pydantic, langsmith, langchain-core, sentence-transformers, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "Successfully installed SQLAlchemy-2.0.41 annotated-types-0.7.0 async-timeout-4.0.3 faiss-cpu-1.11.0 greenlet-3.2.2 jsonpatch-1.33 langchain-0.3.25 langchain-core-0.3.61 langchain-text-splitters-0.3.8 langsmith-0.3.42 orjson-3.10.18 pydantic-2.11.5 pydantic-core-2.33.2 requests-toolbelt-1.0.0 sentence-transformers-4.1.0 tenacity-9.1.2 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install \\\n",
    "#  transformers datasets faiss-cpu \\\n",
    "#  langchain sentence-transformers \\\n",
    "#  accelerate peft evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T09:55:10.340868300Z",
     "start_time": "2025-05-24T09:54:35.619963400Z"
    }
   },
   "id": "d8ea8e56da808e8c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5206b80e036a23c0"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from peft) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (4.52.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (1.7.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (0.32.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihaipc\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihaipc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade peft"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T09:58:44.331229400Z",
     "start_time": "2025-05-24T09:58:41.883118300Z"
    }
   },
   "id": "93bc5a6c9d9260ef"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:02:31.136662500Z",
     "start_time": "2025-05-24T10:02:29.980932100Z"
    }
   },
   "id": "39396291d7387b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Configuration\n",
    "Centralize all paths, model names, and hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e58d96ff18bbc47"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:03:09.979319Z",
     "start_time": "2025-05-24T10:03:09.939812600Z"
    }
   },
   "id": "a5e205d61a24b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Load & Merge Datasets\n",
    "Load your local Q&A (if any), plus MBPP (test split) and GSM8K train. Then standardize to a single list of “documents” with id and text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3abbc354104d866"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57446c0955254897b993539d384da97d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "082eebb4fd8148eeaae894d97d394aee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/43 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65b69aba5db3464e8322fa83ef972634"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4c60797c8a44ee980e37e6b58e9c237"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "578d7f29cbfc4a5fa6cf1a80c282bcfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7877\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:15:38.055725900Z",
     "start_time": "2025-05-24T10:15:28.514886Z"
    }
   },
   "id": "a16dab2f46be4c65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Build & Chunk the Retrieval Corpus\n",
    "We’ll treat each training example as a “document” by concatenating input_text + target_text and splitting into overlapping chunks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c58cc26e7e0f6722"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8129 chunks from 7877 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:19:07.342105Z",
     "start_time": "2025-05-24T10:19:06.974848200Z"
    }
   },
   "id": "eb7eb230bc99f678"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Embed & Build a FAISS Vector Index\n",
    "Use a Sentence-Transformer to embed each chunk, then store in FAISS for fast nearest-neighbour lookup."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8974b76f51c560"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Temp\\ipykernel_24984\\3096433952.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
     ]
    },
    {
     "data": {
      "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88b2523595d341e6b3d7bdacd262d640"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MihaiPC\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d1f9cb0688445e7a01d1103c773c85b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a87caf8015434ca9a349aff016f95df1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55c607adcfcb4220b48fcf503ae0cd44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "890258466f0841caa1553680ca6ec44b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "441ced7a49254cf696a1cb23a545e180"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4fd7964b7194c37957b19b426d621b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e894b611cf944e25ae3835a1457a45d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f88f80b8a6da4307b8ac3fe6a3379ffc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fee132435394f759158fa98933fe682"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f8d1afed53e4aed8f23db20bec07556"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:21:58.498865200Z",
     "start_time": "2025-05-24T10:19:35.167708400Z"
    }
   },
   "id": "200d47f2a7344cf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Wire Up a LangChain RetrievalQA Pipeline\n",
    "We now plug your FAISS store and the Meta-Llama-3.1-8b generator into a single retrieval-augmented chain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daa6ddeeb67ebe92"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e39650401934e7c9502c7f9c06db957"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell: Authenticate to Hugging Face Hub so you can access gated models\n",
    "\n",
    "# If you're in a notebook, you can run:\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  # this will prompt you to paste your HF token\n",
    "\n",
    "# Alternatively, set your token in an environment variable:\n",
    "# import os\n",
    "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"YOUR_TOKEN_HERE\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:31:56.798296400Z",
     "start_time": "2025-05-24T10:31:56.741672200Z"
    }
   },
   "id": "87ddc9f54ff739d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MihaiPC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "LocalTokenNotFoundError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLocalTokenNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m bnb_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(load_in_8bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, llm_int8_threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6.0\u001B[39m)\n\u001B[0;32m     20\u001B[0m GEN_MODEL \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Llama-3.1-8b\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 22\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mGEN_MODEL\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m     26\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token\n\u001B[0;32m     29\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     30\u001B[0m     GEN_MODEL,\n\u001B[0;32m     31\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     32\u001B[0m     quantization_config\u001B[38;5;241m=\u001B[39mbnb_config,\n\u001B[0;32m     33\u001B[0m     use_auth_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     34\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:950\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    947\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    949\u001B[0m \u001B[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001B[39;00m\n\u001B[1;32m--> 950\u001B[0m tokenizer_config \u001B[38;5;241m=\u001B[39m get_tokenizer_config(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    951\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m tokenizer_config:\n\u001B[0;32m    952\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:782\u001B[0m, in \u001B[0;36mget_tokenizer_config\u001B[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001B[0m\n\u001B[0;32m    779\u001B[0m     token \u001B[38;5;241m=\u001B[39m use_auth_token\n\u001B[0;32m    781\u001B[0m commit_hash \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 782\u001B[0m resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mTOKENIZER_CONFIG_FILE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_gated_repo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_missing_entries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_connection_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    799\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:312\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, **kwargs)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcached_file\u001B[39m(\n\u001B[0;32m    255\u001B[0m     path_or_repo_id: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[0;32m    256\u001B[0m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    258\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    259\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n\u001B[0;32m    261\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 312\u001B[0m     file \u001B[38;5;241m=\u001B[39m cached_files(path_or_repo_id\u001B[38;5;241m=\u001B[39mpath_or_repo_id, filenames\u001B[38;5;241m=\u001B[39m[filename], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    313\u001B[0m     file \u001B[38;5;241m=\u001B[39m file[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:557\u001B[0m, in \u001B[0;36mcached_files\u001B[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    554\u001B[0m     \u001B[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001B[39;00m\n\u001B[0;32m    555\u001B[0m     \u001B[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001B[39;00m\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, EntryNotFoundError):\n\u001B[1;32m--> 557\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    559\u001B[0m resolved_files \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    560\u001B[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m full_filenames\n\u001B[0;32m    561\u001B[0m ]\n\u001B[0;32m    562\u001B[0m \u001B[38;5;66;03m# If there are any missing file and the flag is active, raise\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:470\u001B[0m, in \u001B[0;36mcached_files\u001B[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    469\u001B[0m         \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n\u001B[1;32m--> 470\u001B[0m         \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m            \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    475\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m            \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m            \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    485\u001B[0m         snapshot_download(\n\u001B[0;32m    486\u001B[0m             path_or_repo_id,\n\u001B[0;32m    487\u001B[0m             allow_patterns\u001B[38;5;241m=\u001B[39mfull_filenames,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    496\u001B[0m             local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m    497\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:970\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[0;32m    967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repo_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m constants\u001B[38;5;241m.\u001B[39mREPO_TYPES:\n\u001B[0;32m    968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid repo type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Accepted repo types are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(constants\u001B[38;5;241m.\u001B[39mREPO_TYPES)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 970\u001B[0m hf_headers \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_hf_headers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlibrary_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlibrary_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlibrary_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlibrary_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    976\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m local_dir \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    979\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m local_dir_use_symlinks \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001B[0m, in \u001B[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     99\u001B[0m         message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m custom_message\n\u001B[0;32m    100\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[1;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_headers.py:126\u001B[0m, in \u001B[0;36mbuild_hf_headers\u001B[1;34m(token, library_name, library_version, user_agent, headers, is_write_action)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;124;03mBuild headers dictionary to send in a HF Hub call.\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;124;03m        If `token=True` but token is not saved locally.\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;66;03m# Get auth token to send\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m token_to_send \u001B[38;5;241m=\u001B[39m \u001B[43mget_token_to_send\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# Combine headers\u001B[39;00m\n\u001B[0;32m    129\u001B[0m hf_headers \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser-agent\u001B[39m\u001B[38;5;124m\"\u001B[39m: _http_user_agent(\n\u001B[0;32m    131\u001B[0m         library_name\u001B[38;5;241m=\u001B[39mlibrary_name,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    134\u001B[0m     )\n\u001B[0;32m    135\u001B[0m }\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_headers.py:159\u001B[0m, in \u001B[0;36mget_token_to_send\u001B[1;34m(token)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cached_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 159\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LocalTokenNotFoundError(\n\u001B[0;32m    160\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToken is required (`token=True`), but no token found. You\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    161\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m need to provide a token or be logged in to Hugging Face with\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `huggingface-cli login` or `huggingface_hub.login`. See\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    163\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m https://huggingface.co/settings/tokens.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    164\u001B[0m         )\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cached_token\n\u001B[0;32m    167\u001B[0m \u001B[38;5;66;03m# Case implicit use of the token is forbidden by env variable\u001B[39;00m\n",
      "\u001B[1;31mLocalTokenNotFoundError\u001B[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "# Cell: Load FAISS index and wire up Meta-Llama-3.1-8b with auth\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Reload FAISS index (trusting it’s your own)\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 2) Load Llama-3.1-8b in 8-bit, passing your HF token\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "GEN_MODEL = \"meta-llama/Llama-3.1-8b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    use_fast=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# 3) Build the LangChain RetrievalQA pipeline\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# 4) Test it out\n",
    "query = \"How would you implement binary search in Python?\"\n",
    "result = qa_chain(query)\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata[\"source\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:31:29.437541900Z",
     "start_time": "2025-05-24T10:31:27.532928800Z"
    }
   },
   "id": "bc81aa3c1d028459"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8b.\n401 Client Error. (Request ID: Root=1-68319fba-4e4e79f923dadf3e5d0dea77;724526ea-117a-49d0-a5aa-1223dbe0c329)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8b/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 409\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    410\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.1-8b/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mGatedRepoError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:470\u001B[0m, in \u001B[0;36mcached_files\u001B[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n\u001B[1;32m--> 470\u001B[0m     \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    475\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[0;32m   1007\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1008\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[0;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[0;32m   1012\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1016\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[0;32m   1017\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1018\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1019\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1020\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1022\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[0;32m   1023\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1024\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1115\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[0;32m   1114\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[1;32m-> 1115\u001B[0m     \u001B[43m_raise_on_head_call_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_call_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1117\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[1;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[0;32m   1640\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   1641\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(head_call_error, HfHubHTTPError) \u001B[38;5;129;01mand\u001B[39;00m head_call_error\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m401\u001B[39m\n\u001B[0;32m   1642\u001B[0m ):\n\u001B[0;32m   1643\u001B[0m     \u001B[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001B[39;00m\n\u001B[0;32m   1644\u001B[0m     \u001B[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001B[39;00m\n\u001B[1;32m-> 1645\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[0;32m   1646\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1533\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[0;32m   1532\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1533\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1534\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\n\u001B[0;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1450\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[0;32m   1449\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[1;32m-> 1450\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1451\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1452\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1453\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1455\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1456\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1457\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1458\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1459\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:286\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[1;32m--> 286\u001B[0m     response \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[0;32m    287\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[0;32m    288\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m    289\u001B[0m         follow_relative_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    290\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    291\u001B[0m     )\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:310\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    309\u001B[0m response \u001B[38;5;241m=\u001B[39m http_backoff(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, retry_on_exceptions\u001B[38;5;241m=\u001B[39m(), retry_on_status_codes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m429\u001B[39m,))\n\u001B[1;32m--> 310\u001B[0m \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:426\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    423\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    424\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot access gated repo for url \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    425\u001B[0m     )\n\u001B[1;32m--> 426\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(GatedRepoError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m error_message \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccess to this resource is disabled.\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mGatedRepoError\u001B[0m: 401 Client Error. (Request ID: Root=1-68319fba-4e4e79f923dadf3e5d0dea77;724526ea-117a-49d0-a5aa-1223dbe0c329)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8b/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m bnb_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(load_in_8bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, llm_int8_threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6.0\u001B[39m)\n\u001B[0;32m      9\u001B[0m GEN_MODEL \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Llama-3.1-8b\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 10\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mGEN_MODEL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token\n\u001B[0;32m     13\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     14\u001B[0m     GEN_MODEL,\n\u001B[0;32m     15\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     16\u001B[0m     quantization_config\u001B[38;5;241m=\u001B[39mbnb_config\n\u001B[0;32m     17\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:970\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    968\u001B[0m         config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfor_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_dict)\n\u001B[0;32m    969\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 970\u001B[0m         config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    971\u001B[0m             pretrained_model_name_or_path, trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    972\u001B[0m         )\n\u001B[0;32m    973\u001B[0m config_tokenizer_class \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mtokenizer_class\n\u001B[0;32m    974\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoTokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config\u001B[38;5;241m.\u001B[39mauto_map:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1153\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m   1150\u001B[0m trust_remote_code \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m   1151\u001B[0m code_revision \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m-> 1153\u001B[0m config_dict, unused_kwargs \u001B[38;5;241m=\u001B[39m PretrainedConfig\u001B[38;5;241m.\u001B[39mget_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1154\u001B[0m has_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1155\u001B[0m has_local_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:595\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    593\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[0;32m    594\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[1;32m--> 595\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_get_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    596\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    597\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}, kwargs\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:654\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    650\u001B[0m configuration_file \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_configuration_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, CONFIG_NAME) \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m gguf_file\n\u001B[0;32m    652\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    653\u001B[0m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[1;32m--> 654\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    655\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfiguration_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    657\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    658\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    659\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    660\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    661\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    662\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    664\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    665\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    666\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    667\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    668\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, kwargs\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:312\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, **kwargs)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcached_file\u001B[39m(\n\u001B[0;32m    255\u001B[0m     path_or_repo_id: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[0;32m    256\u001B[0m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    258\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    259\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n\u001B[0;32m    261\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 312\u001B[0m     file \u001B[38;5;241m=\u001B[39m cached_files(path_or_repo_id\u001B[38;5;241m=\u001B[39mpath_or_repo_id, filenames\u001B[38;5;241m=\u001B[39m[filename], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    313\u001B[0m     file \u001B[38;5;241m=\u001B[39m file[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:533\u001B[0m, in \u001B[0;36mcached_files\u001B[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    531\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_gated_repo:\n\u001B[0;32m    532\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 533\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    534\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to have access to it at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    535\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    536\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, LocalEntryNotFoundError):\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001B[1;31mOSError\u001B[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8b.\n401 Client Error. (Request ID: Root=1-68319fba-4e4e79f923dadf3e5d0dea77;724526ea-117a-49d0-a5aa-1223dbe0c329)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8b/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "# Cell: Wire up Meta-Llama-3.1-8b (8-bit) for generation, build RetrievalQA, and run a query\n",
    "\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1) Load the generator in 8-bit\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "GEN_MODEL = \"meta-llama/Llama-3.1-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# 2) Wrap in an HF text-generation pipeline\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
    "\n",
    "# 3) Build and run the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",     # or \"map_reduce\" / \"refine\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# 4) Example query\n",
    "query = \"How would you implement binary search in Python?\"\n",
    "result = qa_chain(query)\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata[\"source\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T10:30:19.981049300Z",
     "start_time": "2025-05-24T10:30:18.275290200Z"
    }
   },
   "id": "eb19ec65ffa219ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "32b2ca524b5864f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
