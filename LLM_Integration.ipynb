{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:34.699918Z",
     "start_time": "2025-05-25T13:11:34.676915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "\n",
    "def save_history(messages):\n",
    "    \"\"\"\n",
    "    Append a new timestamped conversation to chat-history.json,\n",
    "    wrapping any existing flat list into the proper format first.\n",
    "    \"\"\"\n",
    "    existing = []\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "\n",
    "        # If data is a flat list of {\"role\", \"content\"} messages, wrap it\n",
    "        if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "            existing = [{\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"conversation\": data\n",
    "            }]\n",
    "        else:\n",
    "            existing = data if isinstance(data, list) else []\n",
    "\n",
    "    # Append the new session\n",
    "    existing.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"conversation\": messages\n",
    "    })\n",
    "\n",
    "    # Write back the full history\n",
    "    with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Appended chat entry at {existing[-1]['timestamp']}\")\n"
   ],
   "id": "e666adf07a526b2f",
   "outputs": [],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:34.730918Z",
     "start_time": "2025-05-25T13:11:34.707919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def ensure_correct_format():\n",
    "    \"\"\"Ensures chat-history.json is always in the correct format\"\"\"\n",
    "    history_file = \"chat-history.json\"\n",
    "\n",
    "    if not os.path.exists(history_file):\n",
    "        return\n",
    "\n",
    "    # Read current content\n",
    "    with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # If already in flat format (list of messages with 'role'), convert to correct format\n",
    "    if isinstance(data, list) and data and \"role\" in data[0]:\n",
    "        corrected = [{\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"conversation\": data\n",
    "        }]\n",
    "\n",
    "        with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(corrected, f, indent=4, ensure_ascii=False)\n",
    "            print(\"Fixed chat history format\")\n",
    "\n",
    "# Run this function periodically or before any operation that reads/writes to the file"
   ],
   "id": "7ce6a8eae838d016",
   "outputs": [],
   "execution_count": 252
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring the Capabilities of LLM Models\n",
    "\n",
    "In this notebook, I aim to evaluate and compare the capabilities of two large language models (LLMs):\n",
    "\n",
    "1. **Codestral22B**\n",
    "   A state-of-the-art model designed for advanced code generation and natural language understanding tasks.\n",
    "\n",
    "2. **Llama 3.1-8B**\n",
    "   A highly efficient and compact model optimized for general-purpose language tasks with an 8-billion parameter architecture.\n",
    "\n",
    "The goal is to analyze their performance across various tasks, including but not limited to:\n",
    "\n",
    "- Code generation and completion\n",
    "- Natural language understanding\n",
    "- Contextual reasoning\n",
    "- Problem-solving capabilities\n",
    "\n",
    "This comparison will help identify the strengths and weaknesses of each model and provide insights into their practical applications."
   ],
   "id": "f51589c43481d44c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI-Powered Programming Tutor with RAG\n",
    "\n",
    "This project focuses on building an AI-powered programming tutor designed to assist students in understanding code and solving problems. The tutor leverages **Retrieval-Augmented Generation (RAG)** to provide accurate and personalized explanations grounded in real university materials, such as:\n",
    "\n",
    "- Past assignments\n",
    "- Lecture notes\n",
    "- Tutorials`\n",
    "\n",
    "The system integrates two large language models (LLMs), **Codestral22B** and **Llama 3.1-8B**, to evaluate their performance in generating solutions and explanations for programming-related queries. The goal is to determine which model provides better support for students in a university setting.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Personalized Explanations**: Tailored responses based on retrieved university materials.\n",
    "- **Code Understanding**: Helps students debug and understand code snippets.\n",
    "- **Problem Solving**: Provides step-by-step solutions to programming problems.\n",
    "- **Model Comparison**: Evaluates the performance of Codestral22B and Llama 3.1-8B.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "7d228540125879a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Basically, I will try to implement and test Codestral22B and Llama 3.1-8B",
   "id": "66487be05849c784"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:34.808187Z",
     "start_time": "2025-05-25T13:11:34.794187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datetime import datetime\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# from pprint import pprint\n",
    "# from os.path import exists\n",
    "#\n",
    "# import requests\n",
    "#\n",
    "# #API endpoint exposed in Lm studio\n",
    "# url = \"http://localhost:1234/v1/chat/completions\"\n",
    "#\n",
    "# #model ID\n",
    "# model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "#\n",
    "# headers={\n",
    "#     \"Content-Type\" : \"application/json\",\n",
    "#     \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "# }\n",
    "#\n",
    "# # messages: [ #keep conversation history\n",
    "# #                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "# #                  \"content\":user_input}\n",
    "# #             ]\n",
    "#\n",
    "# #Keep the message history\n",
    "#\n",
    "# #History file path, to keep conversation\n",
    "# history_file = \"chat-history.json\"\n",
    "# def save_history(messages):\n",
    "#     # Load existing history if the file exists\n",
    "#     if exists(history_file):\n",
    "#         with open(history_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             full_history = json.load(f)\n",
    "#             if isinstance(full_history, list):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 full_history = [full_history]\n",
    "#     else:\n",
    "#         full_history = []\n",
    "#\n",
    "#     # Add this session with timestamp\n",
    "#     full_history.append({\n",
    "#         \"timestamp\": datetime.now().isoformat(),\n",
    "#         \"conversation\": messages\n",
    "#     })\n",
    "#\n",
    "#     # Save the full conversation list\n",
    "#     with open(history_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(full_history, f, indent=4, ensure_ascii=False)\n",
    "#\n",
    "#\n",
    "# #Prompt loop\n",
    "# def chat():\n",
    "#     print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "#     messages = [\n",
    "#     {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "#      \"content\": \"You are a helpful programming tutor.\"}\n",
    "# ] #Messages reset each time\n",
    "#\n",
    "#     while True:\n",
    "#         user_input = input(\" You: \")\n",
    "#         if user_input.lower() == \"exit\":\n",
    "#             #save chat history\n",
    "#             save_history(messages)\n",
    "#             print(f\"\\n Conversation saved to {history_file}\")\n",
    "#             break\n",
    "#\n",
    "#         # Add user message\n",
    "#         messages.append({\"role\": \"user\",\n",
    "#                          \"content\": user_input})\n",
    "#\n",
    "#         payload = {\n",
    "#             \"model\": model_id,#id of model\n",
    "#             \"messages\": messages,#chat history to preserve context\n",
    "#             \"temperature\": 0.7 #control creativiy\n",
    "#         }\n",
    "#         print(\"Your question is: \")\n",
    "#         print(user_input)\n",
    "#         print(\"\\n\")\n",
    "#\n",
    "#         try:#send request to lm api\n",
    "#             response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "#\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 answer = data['choices'][0]['message']['content'].strip()\n",
    "#\n",
    "#                 # Add assistant message\n",
    "#                 messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "#\n",
    "#                 print(\"\\n LLaMA:\", flush=True)\n",
    "#                 print(answer, flush=True)\n",
    "#                 print(\"-\" * 60 + \"\\n\")\n",
    "#\n",
    "#             else:\n",
    "#                 print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "#\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(\" Connection error:\", e)\n",
    "#             break\n"
   ],
   "id": "ca6449393a982411",
   "outputs": [],
   "execution_count": 253
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:34.840414Z",
     "start_time": "2025-05-25T13:11:34.826161Z"
    }
   },
   "cell_type": "code",
   "source": "# chat()",
   "id": "b60581168b183876",
   "outputs": [],
   "execution_count": 254
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Load the Datasets\n",
    "## 1.  OpenMathInstruct-1 (from Hugging Face)\n",
    "- This dataset contains 1.8 million math problem-solution pairs, making it ideal for enhancing mathematical reasoning in LLMs."
   ],
   "id": "f2e29ce7c06eda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:34.871056Z",
     "start_time": "2025-05-25T13:11:34.857485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from datasets import load_dataset\n",
    "# from IPython import get_ipython\n",
    "# from IPython.display import display\n",
    "#\n",
    "# #Load training split\n",
    "# dataset = load_dataset(\"nvidia/OpenMathInstruct-1\", split=\"train\")\n",
    "#\n",
    "# first_element = next(iter(dataset))\n",
    "#\n",
    "# print(first_element)\n",
    "\n",
    "#Give up on it, waaaay to much data in dataset"
   ],
   "id": "12f7c6faccca5cb3",
   "outputs": [],
   "execution_count": 255
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Small & Clean Math Datasets\n",
    "## 1. GSM8K\n",
    "- Size: ~8.5K problems\n",
    "\n",
    "- Focus: Grade school math word problems\n",
    "\n",
    "- Good for: step-by-step reasoning, small LLM finetuning"
   ],
   "id": "954a019fae299897"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:41.597589Z",
     "start_time": "2025-05-25T13:11:34.889328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "first_element = next(iter(dataset))\n",
    "\n",
    "print(first_element)"
   ],
   "id": "1ae3182c572fe879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "execution_count": 256
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Computer Science Theory QA Dataset (from Kaggle)\n",
    "- This dataset offers a comprehensive collection of theoretical computer science questions, suitable for training chatbots and QA systems."
   ],
   "id": "b5932817b8f7a049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:41.628879Z",
     "start_time": "2025-05-25T13:11:41.615590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "ensure_correct_format()\n",
    "with open(\"intents.json\", \"r\") as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "df = pd.json_normalize(intents_data[\"intents\"])\n",
    "print(df.head())"
   ],
   "id": "9ff53e22afa20485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tag                                           patterns  \\\n",
      "0    abstraction  [Explain data abstraction., What is data abstr...   \n",
      "1          error  [What is a syntax error, Explain syntax error,...   \n",
      "2  documentation  [Explain program documentation. Why is it impo...   \n",
      "3        testing                        [What is software testing?]   \n",
      "4  datastructure             [How do you explain a data structure?]   \n",
      "\n",
      "                                           responses  \n",
      "0  [Data abstraction is a technique used in compu...  \n",
      "1  [A syntax error is an error in the structure o...  \n",
      "2  [Program documentation is written information ...  \n",
      "3  [Software testing is the process of evaluating...  \n",
      "4  [A data structure is a way of organizing and s...  \n"
     ]
    }
   ],
   "execution_count": 257
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:46.413194Z",
     "start_time": "2025-05-25T13:11:41.661383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")"
   ],
   "id": "d0b3e31e6e85f56d",
   "outputs": [],
   "execution_count": 258
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Install & Import Dependencies\n",
    "We’ll need:\n",
    "\n",
    "-  Transformers & Datasets\n",
    "\n",
    "-  LangChain & an embedding backend (here HuggingFaceEmbeddings)\n",
    "\n",
    "-  FAISS for the vector index\n",
    "\n",
    "-  Accelerate + PEFT if you plan to fine-tune your generator"
   ],
   "id": "d218ed8cd6b830ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:46.429193Z",
     "start_time": "2025-05-25T13:11:46.418188Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "50a683a9a5dfd6ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:46.461192Z",
     "start_time": "2025-05-25T13:11:46.447189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install \\\n",
    "#  transformers datasets faiss-cpu \\\n",
    "#  langchain sentence-transformers \\\n",
    "#  accelerate peft evaluate"
   ],
   "id": "39aff69f1b83cd51",
   "outputs": [],
   "execution_count": 259
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8e79cd82c56fd576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:46.491221Z",
     "start_time": "2025-05-25T13:11:46.477205Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install --upgrade peft",
   "id": "7fcfb287c01d7c07",
   "outputs": [],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:46.522476Z",
     "start_time": "2025-05-25T13:11:46.508258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    RagTokenizer,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n"
   ],
   "id": "15fef1ad9d22b2db",
   "outputs": [],
   "execution_count": 261
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Configuration\n",
    "Centralize all paths, model names, and hyperparameters."
   ],
   "id": "7d4f39dce9ad0059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:46.552843Z",
     "start_time": "2025-05-25T13:11:46.538707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Paths & names ─────────────────────────────────────────────────────────────\n",
    "OUTPUT_DIR         = \"results/rag-llama\"\n",
    "FAISS_INDEX_PATH   = os.path.join(OUTPUT_DIR, \"faiss_index\")\n",
    "DOCS_PATH          = os.path.join(OUTPUT_DIR, \"docs.jsonl\")\n",
    "\n",
    "# ── Hugging Face models ───────────────────────────────────────────────────────\n",
    "GEN_MODEL_NAME     = \"meta-llama/Llama-3.1-8b\"\n",
    "EMBED_MODEL_NAME   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ── Datasets ─────────────────────────────────────────────────────────────────\n",
    "MBPP_ID            = \"google-research-datasets/mbpp\"\n",
    "MBPP_CFG           = \"sanitized\"\n",
    "GSM8K_ID           = \"gsm8k\"\n",
    "GSM8K_SPLIT        = \"train\"\n",
    "\n",
    "# ── RAG / Retrieval params ────────────────────────────────────────────────────\n",
    "CHUNK_SIZE         = 1000\n",
    "CHUNK_OVERLAP      = 200\n",
    "\n",
    "# ── LoRA fine-tuning (optional) ───────────────────────────────────────────────\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "\n",
    "# ── Trainer hyperparameters (for fine-tuning generator) ──────────────────────\n",
    "NUM_EPOCHS         = 3\n",
    "TRAIN_BS           = 2\n",
    "EVAL_BS            = 2\n",
    "GRAD_ACCUM_STEPS   = 8\n",
    "LEARNING_RATE      = 2e-4\n"
   ],
   "id": "18f205f852935553",
   "outputs": [],
   "execution_count": 262
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:46.584083Z",
     "start_time": "2025-05-25T13:11:46.568987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 1: validate_chat_history_format()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "\n",
    "def validate_chat_history_format():\n",
    "    \"\"\"\n",
    "    Ensure chat-history.json is a list of timestamped conversations.\n",
    "    If it finds a flat list of messages, wrap them in a single timestamped entry.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return\n",
    "\n",
    "    # Load existing data\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            # Invalid JSON; skip fixing\n",
    "            return\n",
    "\n",
    "    # Detect flat list of messages (each item has a 'role' key)\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "        wrapped = [{\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"conversation\": data\n",
    "        }]\n",
    "        # Overwrite with wrapped format\n",
    "        with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(wrapped, f, indent=2, ensure_ascii=False)\n",
    "        print(\"Fixed chat-history format: wrapped flat list into timestamped conversation.\")\n"
   ],
   "id": "389b3e48dde428df",
   "outputs": [],
   "execution_count": 263
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Load & Merge Datasets\n",
    "Load your local Q&A (if any), plus MBPP (test split) and GSM8K train. Then standardize to a single list of “documents” with id and text."
   ],
   "id": "4f4598c6ff4a49d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:57.203103Z",
     "start_time": "2025-05-25T13:11:46.602092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Cell 2: fix_chat_history_format()\n",
    "\n",
    "def fix_chat_history_format():\n",
    "    \"\"\"\n",
    "    Alias for validate_chat_history_format, intended to run\n",
    "    right before loading via datasets or similar.\n",
    "    \"\"\"\n",
    "    validate_chat_history_format()\n",
    "    print(\"Ran fix_chat_history_format()\")\n",
    "\n",
    "\n",
    "# Call this function before loading the dataset\n",
    "fix_chat_history_format()\n",
    "\n",
    "# 1) Chat‐history (no built‐in validation split here, only “train”):\n",
    "raw_chat = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"chat-history.json\"}\n",
    ")\n",
    "def format_chat_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for conv in batch[\"conversation\"]:\n",
    "        # conv is a list of {role,content} dicts\n",
    "        user = [t[\"content\"] for t in conv if t[\"role\"]==\"user\"]\n",
    "        asst = [t[\"content\"] for t in conv if t[\"role\"]==\"assistant\"]\n",
    "        inps.append(\" \".join(user))\n",
    "        tgts.append(\" \".join(asst))\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "chat_ds = raw_chat[\"train\"].map(\n",
    "    format_chat_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"timestamp\",\"conversation\"]\n",
    ")\n",
    "\n",
    "# 2) Intents.json\n",
    "raw_intents = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"intents.json\"}\n",
    ")\n",
    "def format_intents_batch(batch):\n",
    "    # assume batch[\"intents\"] is a list-of-lists of intent dicts\n",
    "    inps, tgts = [], []\n",
    "    for intents_list in batch[\"intents\"]:\n",
    "        for intent in intents_list:\n",
    "            for pat in intent[\"patterns\"]:\n",
    "                inps.append(pat)\n",
    "                tgts.append(intent[\"responses\"][0])\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "intents_ds = raw_intents[\"train\"].map(\n",
    "    format_intents_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"intents\"]\n",
    ")\n",
    "\n",
    "# 3) MBPP “sanitized” (splits: validation & prompt)\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "def format_mbpp_batch(batch):\n",
    "    inps, tgts = [], []\n",
    "    for p, c in zip(batch[\"prompt\"], batch[\"code\"]):\n",
    "        inps.append(p)\n",
    "        tgts.append(f\"```python\\n{c}\\n```\")\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "# concatenate both splits\n",
    "mbpp_ds = concatenate_datasets([\n",
    "    mbpp[\"validation\"].map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"validation\"].column_names),\n",
    "    mbpp[\"prompt\"].    map(format_mbpp_batch, batched=True, remove_columns=mbpp[\"prompt\"].column_names),\n",
    "])\n",
    "\n",
    "# 4) GSM8K “main” train\n",
    "gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "def format_gsm_batch(batch):\n",
    "    inps = [\"Problem:\\n\"+q for q in batch[\"question\"]]\n",
    "    tgts = [\"Answer:\\n\"+a   for a in batch[\"answer\"]]\n",
    "    return {\"input_text\": inps, \"target_text\": tgts}\n",
    "\n",
    "gsm_ds = gsm.map(\n",
    "    format_gsm_batch,\n",
    "    batched=True,\n",
    "    remove_columns=gsm.column_names\n",
    ")\n",
    "\n",
    "# 5) Combine all training sets\n",
    "train_ds = concatenate_datasets([chat_ds, intents_ds, mbpp_ds, gsm_ds])\n",
    "print(\"Total training examples:\", len(train_ds))\n"
   ],
   "id": "4d2f48cea8c5a6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran fix_chat_history_format()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d06ffd207b114b208e5b6fd53416565e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da63eec885704584bd95133458479898"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 7871\n"
     ]
    }
   ],
   "execution_count": 264
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Build & Chunk the Retrieval Corpus\n",
    "We’ll treat each training example as a “document” by concatenating input_text + target_text and splitting into overlapping chunks."
   ],
   "id": "69ce26036be862fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:11:57.504877Z",
     "start_time": "2025-05-25T13:11:57.221098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 4.1 Concatenate input+target into a list of raw docs\n",
    "raw_texts = [\n",
    "    ex[\"input_text\"] + \"\\n\\n\" + ex[\"target_text\"]\n",
    "    for ex in train_ds\n",
    "]\n",
    "metadatas = [\n",
    "    {\"source\": f\"doc-{i}\"}\n",
    "    for i in range(len(raw_texts))\n",
    "]\n",
    "\n",
    "# 4.2 Chunk long docs into 1 000-token windows with 200-token overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text, meta in zip(raw_texts, metadatas):\n",
    "    for chunk in splitter.split_text(text):\n",
    "        docs.append(Document(page_content=chunk, metadata=meta))\n",
    "\n",
    "print(f\"▶ Created {len(docs)} chunks from {len(raw_texts)} documents.\")\n"
   ],
   "id": "ca4d714962ad40d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Created 8121 chunks from 7871 documents.\n"
     ]
    }
   ],
   "execution_count": 265
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Embed & Build a FAISS Vector Index\n",
    "Use a Sentence-Transformer to embed each chunk, then store in FAISS for fast nearest-neighbour lookup."
   ],
   "id": "c7b19aa8fa25874a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:17.003305Z",
     "start_time": "2025-05-25T13:11:57.521892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 5.1 Initialize your embedding model\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# 5.2 Create FAISS index from Document objects\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "# 5.3 (Optional) persist to disk for later reuse\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "vectorstore.save_local(INDEX_PATH)\n",
    "print(f\"✔ FAISS index saved to '{INDEX_PATH}'.\")\n"
   ],
   "id": "37ae62208b9e7c92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ FAISS index saved to 'results/faiss_index'.\n"
     ]
    }
   ],
   "execution_count": 266
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Wire Up a LangChain RetrievalQA Pipeline\n",
    "We now plug your FAISS store and the Meta-Llama-3.1-8b generator into a single retrieval-augmented chain."
   ],
   "id": "9adb9f015e48c8e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:17.049274Z",
     "start_time": "2025-05-25T13:14:17.035269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Option B: set it directly in your environment\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n"
   ],
   "id": "45baecdb609ecdcb",
   "outputs": [],
   "execution_count": 267
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:19.436331Z",
     "start_time": "2025-05-25T13:14:17.057282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── Cell: Load FAISS index & build retriever ────────────────────────────────────\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Recreate your embedder exactly as when you built the index\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load your on-disk FAISS index (you trust its provenance)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Wrap as a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ],
   "id": "3b992f350d616aa6",
   "outputs": [],
   "execution_count": 268
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.157927Z",
     "start_time": "2025-05-25T13:14:19.453965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ── 0) Grab your token from env ────────────────────────────────────────────────\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set HUGGINGFACE_HUB_TOKEN in your environment before running this cell.\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ── 1) Reload FAISS index ──\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"results/faiss_index\",\n",
    "    embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ── 2) Connect to local LM Studio API ──────────────────────────────────────────\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama-3.1-8b-instruct\",  # Just for tracking, not actually used to load model\n",
    "    openai_api_key=\"lm-studio\",               # Dummy API key as used in your chat() function\n",
    "    openai_api_base=\"http://localhost:1234/v1\", # Your LM Studio API endpoint\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# ── 3) Build & run RetrievalQA ─────────────────────────────────────────────────\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",       # or \"map_reduce\" / \"refine\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# # ── 4) Test query ──────────────────────────────────────────────────────────────\n",
    "# query = \"How would you implement binary search in Python?\"\n",
    "# result = qa_chain(query)\n",
    "# print(\"Answer:\\n\", result[\"result\"])\n",
    "# print(\"\\nSources:\")\n",
    "# for doc in result[\"source_documents\"]:\n",
    "#     print(\"-\", doc.metadata[\"source\"])\n",
    "\n",
    "\n"
   ],
   "id": "526fbcb07a89502f",
   "outputs": [],
   "execution_count": 269
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.189603Z",
     "start_time": "2025-05-25T13:14:22.176954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell: normalize_history.py\n",
    "\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "H = \"chat-history.json\"\n",
    "\n",
    "if os.path.exists(H):\n",
    "    data = json.load(open(H, \"r\", encoding=\"utf-8\"))\n",
    "    # if flat list of messages, wrap it\n",
    "    if isinstance(data, list) and data and isinstance(data[0], dict) and \"role\" in data[0]:\n",
    "        wrapped = [{\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"conversation\": data\n",
    "        }]\n",
    "        json.dump(wrapped, open(H, \"w\", encoding=\"utf-8\"), indent=2, ensure_ascii=False)\n",
    "        print(\"Normalized existing chat-history.json\")\n"
   ],
   "id": "ec8cac48faa08170",
   "outputs": [],
   "execution_count": 270
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.221601Z",
     "start_time": "2025-05-25T13:14:22.207602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from os.path import exists\n",
    "\n",
    "import requests\n",
    "\n",
    "#API endpoint exposed in Lm studio\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "#model ID\n",
    "model_id = \"meta-llama-3.1-8b-instruct\"\n",
    "\n",
    "headers={\n",
    "    \"Content-Type\" : \"application/json\",\n",
    "    \"Authorization\" :\"Bearer lm-studio\" #Dummy API key\n",
    "}\n",
    "\n",
    "# messages: [ #keep conversation history\n",
    "#                 {\"role\":\"user\", #what you type, only sends current prompt\n",
    "#                  \"content\":user_input}\n",
    "#             ]\n",
    "\n",
    "#Keep the message history\n",
    "ensure_correct_format()\n",
    "#History file path, to keep conversation\n",
    "# Cell 3: save_history()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "history_file = \"chat-history.json\"\n",
    "\n",
    "# Cell: save_history.py\n",
    "\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "H = \"chat-history.json\"\n",
    "\n",
    "\n",
    "\n",
    "#Prompt loop\n",
    "def chat():\n",
    "    print(\" Talk to LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "    {\"role\": \"system\", #Sets the intial behavior, the text below\n",
    "     \"content\": \"You are a helpful programming tutor.\"}\n",
    "] #Messages reset each time\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            #save chat history\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            break\n",
    "\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\",\n",
    "                         \"content\": user_input})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_id,#id of model\n",
    "            \"messages\": messages,#chat history to preserve context\n",
    "            \"temperature\": 0.7 #control creativiy\n",
    "        }\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:#send request to lm api\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=1000)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "                # Add assistant message\n",
    "                messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "                print(\"\\n LLaMA:\", flush=True)\n",
    "                print(answer, flush=True)\n",
    "                print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\" Error {response.status_code}: {response.text}\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\" Connection error:\", e)\n",
    "            break\n",
    "        save_history(messages)\n"
   ],
   "id": "34443ddc8c9e41c9",
   "outputs": [],
   "execution_count": 271
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.252663Z",
     "start_time": "2025-05-25T13:14:22.238667Z"
    }
   },
   "cell_type": "code",
   "source": "# chat()",
   "id": "e519a2c24c2734ac",
   "outputs": [],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.283955Z",
     "start_time": "2025-05-25T13:14:22.270947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Search for Pythagorean theorem related questions in GSM8K\n",
    "# pythagorean_questions = []\n",
    "#\n",
    "# # Load the dataset if not already loaded\n",
    "# gsm = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "# #\n",
    "# # Search for relevant keywords\n",
    "# keywords = [\"pythagora\", \"pythagorean\", \"right triangle\", \"hypotenuse\", \"a^2 + b^2\"]\n",
    "#\n",
    "# for i, example in enumerate(gsm):\n",
    "#     question = example[\"question\"].lower()\n",
    "#     for keyword in keywords:\n",
    "#         if keyword.lower() in question:\n",
    "#             pythagorean_questions.append({\n",
    "#                 \"index\": i,\n",
    "#                 \"question\": example[\"question\"],\n",
    "#                 \"answer\": example[\"answer\"]\n",
    "#             })\n",
    "#             break\n",
    "#\n",
    "# # Print the number of matching questions\n",
    "# print(f\"Found {len(pythagorean_questions)} questions related to the Pythagorean theorem\")\n",
    "#\n",
    "# # Display the first few matches if any exist\n",
    "# for i, q in enumerate(pythagorean_questions[:3]):\n",
    "#     print(f\"\\nQuestion {i+1}:\")\n",
    "#     print(q[\"question\"])\n",
    "#     print(\"\\nAnswer:\")\n",
    "#     print(q[\"answer\"])"
   ],
   "id": "3ae90eb18febee3e",
   "outputs": [],
   "execution_count": 273
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.315166Z",
     "start_time": "2025-05-25T13:14:22.300486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "#\n",
    "# # Load the FAISS index file\n",
    "# index = faiss.read_index(\"results/faiss_index/index.faiss\")\n",
    "#\n",
    "# # Print basic info\n",
    "# print(\"Index type:\", type(index).__name__)\n",
    "# print(\"Dimension:\", index.d)\n",
    "# # print(\"Is trained:\", index.is_trained)\n",
    "# # print(\"Total vectors stored:\", index.ntotal)\n",
    "#\n",
    "# # Example: retrieve all stored vectors (if they fit in memory)\n",
    "# try:\n",
    "#     xb = index.reconstruct_n(0, index.ntotal)  # returns all vectors\n",
    "#     print(\"Sample vector (first one):\", xb[0])\n",
    "# except Exception as e:\n",
    "#     print(\"Cannot reconstruct vectors:\", e)\n"
   ],
   "id": "e73fbafadbe7a90c",
   "outputs": [],
   "execution_count": 274
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.346177Z",
     "start_time": "2025-05-25T13:14:22.331174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 4: update_knowledge_base()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "HISTORY_FILE = \"chat-history.json\"\n",
    "LAST_UPDATE_FILE = \"results/last_update.txt\"\n",
    "INDEX_PATH = \"results/faiss_index\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def get_new_conversations() -> List[Dict]:\n",
    "    \"\"\"Get all timestamped conversations since the last update.\"\"\"\n",
    "    last_update = None\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            last_update = f.read().strip()\n",
    "\n",
    "    if not os.path.exists(HISTORY_FILE):\n",
    "        return []\n",
    "\n",
    "    with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        history = json.load(f)\n",
    "        if not isinstance(history, list):\n",
    "            history = [history]\n",
    "\n",
    "    if last_update:\n",
    "        return [c for c in history if c.get(\"timestamp\", \"\") > last_update]\n",
    "    return history\n",
    "\n",
    "def is_correction(conv: Dict) -> bool:\n",
    "    \"\"\"Detect if a user correction follows an assistant message.\"\"\"\n",
    "    msgs = conv.get(\"conversation\", [])\n",
    "    for i in range(1, len(msgs)):\n",
    "        if msgs[i][\"role\"] == \"user\" and msgs[i-1][\"role\"] == \"assistant\":\n",
    "            txt = msgs[i][\"content\"].lower()\n",
    "            if any(kw in txt for kw in [\"wrong\",\"incorrect\",\"mistake\",\"no,\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def conversations_to_docs(conversations: List[Dict]) -> List[Document]:\n",
    "    \"\"\"Convert timestamped conversations to Document objects for FAISS.\"\"\"\n",
    "    docs = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for idx, conv in enumerate(conversations):\n",
    "        text = \"\"\n",
    "        for msg in conv[\"conversation\"]:\n",
    "            if msg[\"role\"] != \"system\":\n",
    "                prefix = \"Question: \" if msg[\"role\"]==\"user\" else \"Answer: \"\n",
    "                text += f\"{prefix}{msg['content']}\\n\\n\"\n",
    "        metadata = {\"source\": f\"conversation-{idx}\", \"timestamp\": conv.get(\"timestamp\",\"\")}\n",
    "        for chunk in splitter.split_text(text):\n",
    "            docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def update_knowledge_base():\n",
    "    \"\"\"Update the FAISS index with any new conversations.\"\"\"\n",
    "    print(\"Updating knowledge base...\")\n",
    "    new_convs = get_new_conversations()\n",
    "    if not new_convs:\n",
    "        print(\"No new conversations found.\")\n",
    "        return\n",
    "\n",
    "    corrections = [c for c in new_convs if is_correction(c)]\n",
    "    print(f\"Found {len(new_convs)} new convs, {len(corrections)} with corrections\")\n",
    "\n",
    "    docs = conversations_to_docs(new_convs)\n",
    "    if not docs:\n",
    "        print(\"No documents to add.\")\n",
    "        return\n",
    "\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    try:\n",
    "        vs = FAISS.load_local(INDEX_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "    except Exception:\n",
    "        vs = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "    vs.add_documents(docs)\n",
    "    vs.save_local(INDEX_PATH)\n",
    "\n",
    "    os.makedirs(os.path.dirname(LAST_UPDATE_FILE), exist_ok=True)\n",
    "    with open(LAST_UPDATE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "\n",
    "    print(\"Knowledge base updated successfully\")\n",
    "\n",
    "update_knowledge_base()\n"
   ],
   "id": "5f5ec0b6114dda7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating knowledge base...\n",
      "No new conversations found.\n"
     ]
    }
   ],
   "execution_count": 275
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.377176Z",
     "start_time": "2025-05-25T13:14:22.363174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rag_chat():\n",
    "    \"\"\"Interactive chat function that uses the RAG-enhanced model instead of direct API calls\"\"\"\n",
    "    print(\" Talk to RAG-enhanced LLaMA 3.1 (type 'exit' to quit)\\n\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a helpful programming tutor.\"}\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\" You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            save_history(messages)\n",
    "            print(f\"\\n Conversation saved to {history_file}\")\n",
    "            update_knowledge_base()\n",
    "            break\n",
    "\n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        print(\"Your question is: \")\n",
    "        print(user_input)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try:\n",
    "            # Use RAG chain instead of direct LM Studio API\n",
    "            result = qa_chain(user_input)\n",
    "            answer = result[\"result\"]\n",
    "\n",
    "            # Add assistant message to conversation history\n",
    "            messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "            print(\"\\n RAG-LLaMA:\", flush=True)\n",
    "            print(answer, flush=True)\n",
    "            print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {str(e)}\\n\")\n",
    "            break\n",
    "\n",
    "# Start the RAG-enhanced chat\n"
   ],
   "id": "1c8cd6f3d054927f",
   "outputs": [],
   "execution_count": 276
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.408377Z",
     "start_time": "2025-05-25T13:14:22.394370Z"
    }
   },
   "cell_type": "code",
   "source": "# rag_chat()",
   "id": "b9e7a2543ab14725",
   "outputs": [],
   "execution_count": 277
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:14:22.439441Z",
     "start_time": "2025-05-25T13:14:22.424466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Literal\n",
    "import uvicorn\n",
    "\n",
    "# assume you’ve already done:\n",
    "#   from your_langchain_setup import qa_chain, save_history, history_file\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],\n",
    "    allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\",\"assistant\",\"system\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    history: List[ChatMessage]\n",
    "\n",
    "@app.post(\"/rag_chat\", response_model=ChatResponse)\n",
    "async def rag_chat_endpoint(req: ChatRequest):\n",
    "    # 1) Rebuild the turn list\n",
    "    messages = req.history.copy() if req.history else [\n",
    "        {\"role\":\"system\",\"content\":\"You are a helpful programming tutor.\"}\n",
    "    ]\n",
    "    messages.append({\"role\":\"user\",\"content\":req.message})\n",
    "\n",
    "    # 2) Run RAG chain\n",
    "    try:\n",
    "        result: Dict[str, Any] = qa_chain({\"query\": req.message})\n",
    "        answer = result[\"result\"]\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    messages.append({\"role\":\"assistant\",\"content\":answer})\n",
    "\n",
    "    # 3) Persist using your helper (will wrap & append correctly)\n",
    "    save_history(messages)\n",
    "\n",
    "    # 4) Return to frontend\n",
    "    return ChatResponse(answer=answer, history=messages)\n",
    "\n",
    "# Optional: to run standalone\n",
    "# if __name__ == \"__main__\":\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ],
   "id": "ef0db09bc6e3b7f",
   "outputs": [],
   "execution_count": 278
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:15:04.517201Z",
     "start_time": "2025-05-25T13:15:04.506202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run_server, daemon=True)\n",
    "thread.start()\n",
    "print(\"RAG server is now running on http://localhost:8000\")\n"
   ],
   "id": "3cc563116853fdfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG server is now running on http://localhost:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3984]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    }
   ],
   "execution_count": 282
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
